{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ca79ab",
   "metadata": {},
   "source": [
    "# UK Biobank Confounds Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a06740",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190478fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lib.script_01_00 import generate_initial_variables\n",
    "from lib.script_01_01 import generate_raw_confounds\n",
    "from lib.script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal \n",
    "from src.nets.nets_normalise import nets_normalise \n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.nets.nets_deconfound import nets_deconfound\n",
    "\n",
    "from src.duplicate.duplicate_categorical import duplicate_categorical\n",
    "from src.duplicate.duplicate_demedian_norm_by_site import duplicate_demedian_norm_by_site\n",
    "\n",
    "from src.preproc.datenum import datenum\n",
    "from src.preproc.days_in_year import days_in_year\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "from src.memmap.addBlockToMmap import addBlockToMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba155a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c571a",
   "metadata": {},
   "source": [
    "## Script 01_00: gen_init_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set skip to True, we will skip script 01_00 and load in a presaved output\n",
    "skip = True\n",
    "\n",
    "# Run notebook 00\n",
    "if not skip:\n",
    "\n",
    "    # Time the notebook\n",
    "    t1 = time.time()\n",
    "    IDPs, nonIDPs, misc = generate_initial_variables(data_dir, out_dir)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Print the time\n",
    "    print(t2-t1)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Read in precomputed memmaps\n",
    "    IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "    nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "    misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747524c8",
   "metadata": {},
   "source": [
    "*The previous run of notebook zero took 234.92218828201294 seconds ≈ 4 minutes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38458efa",
   "metadata": {},
   "source": [
    "## Script 01_01: gen_raw_conf_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subject IDs\n",
    "sub_ids = IDPs.index\n",
    "\n",
    "# If you set skip to True, we will skip script 01_01 and load in a presaved output\n",
    "skip = True\n",
    "\n",
    "# Run notebook 00\n",
    "if not skip:\n",
    "\n",
    "    # Generate raw confounds\n",
    "    t1 = time.time()\n",
    "    confounds = generate_raw_confounds(data_dir, sub_ids)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Print the time\n",
    "    print(t2-t1)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Read in precomputed confounds\n",
    "    confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c69b9",
   "metadata": {},
   "source": [
    "*Previous run took 9.763055801391602 seconds.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c87bdf",
   "metadata": {},
   "source": [
    "## script_01_02: gen_nonlin_conf_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set skip to True, we will skip script 01_02 and load in a presaved output\n",
    "skip = True\n",
    "\n",
    "# Run notebook 00\n",
    "if not skip:\n",
    "\n",
    "    # Generate non linear confounds and deconfound IDPs\n",
    "    t1 = time.time()\n",
    "    nonlinear_confounds, IDPs_deconf = generate_nonlin_confounds(data_dir, confounds, IDPs)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Time the notebook\n",
    "    print(t2-t1)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Read in precomputed confounds\n",
    "    nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz'))\n",
    "    IDPs_deconf = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2cf93",
   "metadata": {},
   "source": [
    "*Last run took 13329.225360631943 seconds ≈ 3.7 hours.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6eda6",
   "metadata": {},
   "source": [
    "## script_01_03-4: gen_jobs/gen_nonlin_conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "num_IDP = 55\n",
    "\n",
    "# Get the subject ids\n",
    "sub_ids = IDPs_deconf.index\n",
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site = []\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in unique_site_ids:\n",
    "\n",
    "    # Find the non-nan indices for this site\n",
    "    indices = np.where(~np.isnan(IDPs_deconf[:, num_IDP].values.flatten()) & (site_ids == site_id).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site.append(indices)\n",
    "\n",
    "# Delete the indices\n",
    "del indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the site of the non-linear confound\n",
    "site_no = int(nonlinear_confounds.columns[i].split('Site_')[1][0])\n",
    "\n",
    "# Get the nonlinear confound for this site\n",
    "nonlinear_confound = nonlinear_confounds[:,i].values\n",
    "\n",
    "# Subset to just this site (remembering zero indexing)\n",
    "nonlinear_confound = nonlinear_confound[inds_per_site[site_no-1]]\n",
    "\n",
    "# Get X\n",
    "X = nets_demean(pd.DataFrame(nonlinear_confound)).values\n",
    "\n",
    "# Get Y\n",
    "Y = IDPs_deconf[:, num_IDP].values\n",
    "Y = Y[inds_per_site[site_no-1]]\n",
    "\n",
    "# Get predicted Y = Xbeta\n",
    "pred_Y = np.nansum(X*Y)/np.nansum(X**2)*X\n",
    "\n",
    "# Get variance explained by pred_Y\n",
    "ve1 = 100*((np.nanstd(pred_Y)/np.std(Y[~np.isnan(X)]))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c24118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import f  \n",
    "from scipy.stats import t \n",
    "from scipy.linalg import pinv, lstsq  \n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.nets.nets_pearson import nets_pearson\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "# Demean the confound data for the current site and nonlinear confound\n",
    "X = nets_demean(pd.DataFrame(nonlinear_confound)).values\n",
    "\n",
    "# Get Y\n",
    "Y = IDPs_deconf[:, num_IDP].values\n",
    "Y = Y[inds_per_site[site_no-1]]\n",
    "\n",
    "# Remove potential nans from X\n",
    "Y = Y[~np.isnan(X)]\n",
    "X = X[~np.isnan(X)]\n",
    "\n",
    "# Get predicted Y = Xbeta\n",
    "pred_Y = np.nansum(X*Y)/np.nansum(X**2)*X\n",
    "\n",
    "# Compute the residuals\n",
    "resids = Y - pred_Y\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Variance explained version 1\n",
    "# --------------------------------------------------------\n",
    "# Get variance explained by pred_Y\n",
    "ve1 = 100*((np.nanstd(pred_Y)/np.std(Y[~np.isnan(X)]))**2)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# P Version 1\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Compute the sum of squares for the effect\n",
    "SSeffect = np.linalg.norm(pred_Y - np.mean(pred_Y))**2  \n",
    "\n",
    "# Compute the sum of squares for the error\n",
    "SSerror = np.linalg.norm(resids - np.mean(resids))**2  \n",
    "\n",
    "# Compute the degrees of freedom for the effect\n",
    "df = np.linalg.matrix_rank(X) \n",
    "\n",
    "# Compute the degrees of freedom for the error\n",
    "dferror = len(Y) - df  \n",
    "\n",
    "# Compute the F-statistic\n",
    "F = (SSeffect / df) / (SSerror / dferror)  \n",
    "\n",
    "# Compute p[i] using the F-distribution\n",
    "p = 1 - f.cdf(F, df, dferror)  \n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Variance explained version 2\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Construct new design matrix\n",
    "XplusIntercept = np.ones((X.shape[0],2))\n",
    "XplusIntercept[:,1] = X[:]\n",
    "\n",
    "# Perform OLS regression\n",
    "U, D, Vt = np.linalg.svd(XplusIntercept, full_matrices=False)\n",
    "\n",
    "# Get the rank of the matrix\n",
    "rank = np.sum(D > 1e-10)\n",
    "\n",
    "# Rank reduce U, D and Vt\n",
    "U = U[:, :rank] \n",
    "Vt = Vt[:rank,:]\n",
    "D = D[:rank]\n",
    "\n",
    "# Get betahat\n",
    "beta = (Vt.T/D) @ (U.T @ Y)\n",
    "\n",
    "# Get residuals\n",
    "resids = Y - XplusIntercept @ beta\n",
    "\n",
    "# Get sigma^2 estimator\n",
    "sigma2 = np.sum(resids**2)/Y.shape[0]\n",
    "\n",
    "# Contrast for beta2\n",
    "L = np.array([[0],[1]])\n",
    "\n",
    "# Contrast variance\n",
    "invDVtL = Vt/D @ L \n",
    "varLtBeta = np.sqrt(sigma2*invDVtL.T @ invDVtL)\n",
    "\n",
    "# T statistic for contrast\n",
    "T = L.T @ beta / varLtBeta\n",
    "\n",
    "# Second version of variance explained\n",
    "ve2 = 100*(1-(np.std(resids)**2/np.std(Y)**2))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# P-value version 2\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# P value\n",
    "p2 = 1 - t.cdf(T, dferror)[0,0]  \n",
    "\n",
    "# --------------------------------------------------------\n",
    "# P-value version 3\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Compute pearson coefficient\n",
    "R, p3 = nets_pearson(X,Y)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Variance explained version 3\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Compute version 3 of variance explained\n",
    "ve3 = 100*R**2\n",
    "\n",
    "print(p, p2, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe21bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from lib.func_01_05 import func_01_05_gen_nonlin_conf\n",
    "\n",
    "t1 = time.time()\n",
    "func_01_05_gen_nonlin_conf(data_dir, out_dir, num_IDP, nonlinear_confounds, IDPs_deconf)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb509e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the number of nonlinear confounds\n",
    "num_conf_nonlin = nonlinear_confounds.shape[1]\n",
    "\n",
    "# Get the number of IDPs\n",
    "num_IDPs = IDPs_deconf.shape[1]\n",
    "\n",
    "tmp = np.memmap(os.path.join(out_dir, 'p1.npy'), mode='r+', shape=(num_IDPs, num_conf_nonlin), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a25ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[55,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba62f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_IDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ccb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename=os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz')\n",
    "print(filename)\n",
    "# Read in self_copy and create a new instance\n",
    "with open(filename, 'rb') as f:\n",
    "    self_copy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bfd1f",
   "metadata": {},
   "source": [
    "## Garbage Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this won't execute in Jupyter until the code is restarted.\n",
    "#del IDPs, nonIDPs, misc, categorical_IDPs, continuous_IDPs, other_IDPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
