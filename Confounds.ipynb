{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ca79ab",
   "metadata": {},
   "source": [
    "# UK Biobank Confounds Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a06740",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190478fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lib.script_01_00 import generate_initial_variables\n",
    "from lib.script_01_01 import generate_raw_confounds\n",
    "from lib.script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal \n",
    "from src.nets.nets_normalise import nets_normalise \n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.nets.nets_deconfound import nets_deconfound\n",
    "\n",
    "from src.duplicate.duplicate_categorical import duplicate_categorical\n",
    "from src.duplicate.duplicate_demedian_norm_by_site import duplicate_demedian_norm_by_site\n",
    "\n",
    "from src.preproc.datenum import datenum\n",
    "from src.preproc.days_in_year import days_in_year\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "from src.memmap.addBlockToMmap import addBlockToMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba155a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c571a",
   "metadata": {},
   "source": [
    "## Script 01_00: gen_init_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set skip to True, we will skip script 01_00 and load in a presaved output\n",
    "skip = True\n",
    "\n",
    "# Run notebook 00\n",
    "if not skip:\n",
    "\n",
    "    # Time the notebook\n",
    "    t1 = time.time()\n",
    "    IDPs, nonIDPs, misc = generate_initial_variables(data_dir, out_dir)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Print the time\n",
    "    print(t2-t1)\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Read in precomputed memmaps\n",
    "    IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "    nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "    misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747524c8",
   "metadata": {},
   "source": [
    "*The previous run of notebook zero took 246.62735080718994 seconds ≈ 4.1 minutes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38458efa",
   "metadata": {},
   "source": [
    "## Script 01_01: gen_raw_conf_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subject IDs\n",
    "sub_ids = IDPs.index\n",
    "\n",
    "# If you set skip to True, we will skip script 01_01 and load in a presaved output\n",
    "skip = True\n",
    "\n",
    "# Run notebook 01\n",
    "if not skip:\n",
    "\n",
    "    # Generate raw confounds\n",
    "    t1 = time.time()\n",
    "    confounds = generate_raw_confounds(data_dir, sub_ids)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Print the time\n",
    "    print(t2-t1)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Read in precomputed confounds\n",
    "    confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c69b9",
   "metadata": {},
   "source": [
    "*Previous run took 12.58459210395813 seconds.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c87bdf",
   "metadata": {},
   "source": [
    "## Script 01_02: gen_nonlin_conf_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set skip to True, we will skip script 01_02 and load in a presaved output\n",
    "skip = False\n",
    "\n",
    "# Run notebook 02\n",
    "if not skip:\n",
    "\n",
    "    # Set cluster configuration\n",
    "    local_cluster = {'cluster_type':'local','num_nodes':12}\n",
    "\n",
    "    # Generate non linear confounds and deconfound IDPs\n",
    "    t1 = time.time()\n",
    "    nonlinear_confounds, IDPs_deconf = generate_nonlin_confounds(data_dir, confounds, IDPs, local_cluster)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Time the notebook\n",
    "    print(t2-t1)\n",
    "\n",
    "    # Save the results as files we can reconstruct memory mapped dataframes from\n",
    "    nonlinear_confounds_fname = os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz')\n",
    "    nonlinear_confounds.save(nonlinear_confounds_fname)\n",
    "    \n",
    "    # Save the results as files we can reconstruct memory mapped dataframes from\n",
    "    IDPs_deconf_fname = os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz')\n",
    "    IDPs_deconf.save(IDPs_deconf_fname)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Read in precomputed confounds\n",
    "    nonlinear_confounds_fname = os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz')\n",
    "    nonlinear_confounds = read_memmap_df(nonlinear_confounds_fname)\n",
    "\n",
    "    # Read in precomputed IDPs\n",
    "    IDPs_deconf_fname = os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz')\n",
    "    IDPs_deconf = read_memmap_df(IDPs_deconf_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2cf93",
   "metadata": {},
   "source": [
    "*Last local cluster run (12 nodes) took 1225.3047969341278 seconds ≈ 20.4 minutes.*\n",
    "\n",
    "*Comparison on the same machine; MatLab local cluster run (12 nodes) took 2305.303473 seconds ≈ 38.4 minutes.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77814e-4013-4292-8ef9-f1384b306b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_confounds[1:20,1:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e54c54-6b77-4f8f-83bd-84665c4a1678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "B = IDPs_deconf[:,:]\n",
    "\n",
    "common_cols = list(set(IDPs_deconf.columns) & set(IDPs.columns))\n",
    "B.loc[:, common_cols] = IDPs.loc[:, common_cols].mask(IDPs_deconf.loc[:, common_cols].isna(), np.nan)\n",
    "\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b89fd6-3f39-4ba9-a8ec-2491004740b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDPs_deconf.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53934d9-e9aa-4d2f-9ab6-92e8f7135a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(100,100)\n",
    "b = np.random.randn(100,100)\n",
    "\n",
    "a[a>0] = np.NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19905e69-7748-4b94-98a2-2dec1db9faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = nonlinear_confounds[1:10,300:310]\n",
    "y_current = tmp[['HeadMotion_mean_dMRI_rel_Site_1_squared']]\n",
    "non_nan = ~np.array((y_current==0).astype(int).values,dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c391b-ed50-4eb5-af25-867f9541b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[~non_nan.flatten()]=np.NaN\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6eda6",
   "metadata": {},
   "source": [
    "## Script 01_03-4: gen_jobs/gen_nonlin_conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d35859-12af-47bc-9a1e-3a1c761d1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you set skip to True, we will skip script 01_03 and load in a presaved output\n",
    "skip = True\n",
    "\n",
    "# Run notebook 03\n",
    "if not skip:\n",
    "\n",
    "    # Set cluster configuration\n",
    "    dask_cluster = {'cluster_type':'local','num_nodes':12}\n",
    "\n",
    "    # Generate non linear confounds and deconfound IDPs\n",
    "    t1 = time.time()\n",
    "    get_p_vals_and_ve(data_dir, out_dir, nonlinear_confounds, IDPs_deconf, cluster_cfg=None)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Time the notebook\n",
    "    print(t2-t1)\n",
    "\n",
    "    # # Save the results as files we can reconstruct memory mapped dataframes from\n",
    "    # nonlinear_confounds_fname = os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz')\n",
    "    # nonlinear_confounds.save(nonlinear_confounds_fname)\n",
    "    \n",
    "    # # Save the results as files we can reconstruct memory mapped dataframes from\n",
    "    # IDPs_deconf_fname = os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz')\n",
    "    # IDPs_deconf.save(IDPs_deconf_fname)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # # Read in precomputed confounds\n",
    "    # nonlinear_confounds_fname = os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz')\n",
    "    # nonlinear_confounds = read_memmap_df(nonlinear_confounds_fname)\n",
    "\n",
    "    # # Read in precomputed IDPs\n",
    "    # IDPs_deconf_fname = os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz')\n",
    "    # IDPs_deconf = read_memmap_df(IDPs_deconf_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f032a-8b79-4998-9f34-a62ba23b6b7b",
   "metadata": {},
   "source": [
    "## Script 01_05: gen_nonlin_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d957ea-bf6f-4286-be38-07d34f5aba18",
   "metadata": {},
   "source": [
    "This code is called to by `script_01_03-04.py` and does not need to be run at this level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbfb1c-1ac9-4ef3-9d4c-4af0d6229ccd",
   "metadata": {},
   "source": [
    "## Script 01_06: gen_nonlin_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b87259-416f-41e2-9e14-9a2c471aa648",
   "metadata": {},
   "source": [
    "This script has been passed over, as it is only for generating plots in matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "num_IDP = 55\n",
    "\n",
    "# Get the subject ids\n",
    "sub_ids = IDPs_deconf.index\n",
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site = []\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in unique_site_ids:\n",
    "\n",
    "    # Find the non-nan indices for this site\n",
    "    indices = np.where(~np.isnan(IDPs_deconf[:, num_IDP].values.flatten()) & (site_ids == site_id).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site.append(indices)\n",
    "\n",
    "# Delete the indices\n",
    "del indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the site of the non-linear confound\n",
    "site_no = int(nonlinear_confounds.columns[i].split('Site_')[1][0])\n",
    "\n",
    "# Get the nonlinear confound for this site\n",
    "nonlinear_confound = nonlinear_confounds[:,i].values\n",
    "\n",
    "# Subset to just this site (remembering zero indexing)\n",
    "nonlinear_confound = nonlinear_confound[inds_per_site[site_no-1]]\n",
    "\n",
    "# Get X\n",
    "X = nets_demean(pd.DataFrame(nonlinear_confound)).values\n",
    "\n",
    "# Get Y\n",
    "Y = IDPs_deconf[:, num_IDP].values\n",
    "Y = Y[inds_per_site[site_no-1]]\n",
    "\n",
    "# Get predicted Y = Xbeta\n",
    "pred_Y = np.nansum(X*Y)/np.nansum(X**2)*X\n",
    "\n",
    "# Get variance explained by pred_Y\n",
    "ve1 = 100*((np.nanstd(pred_Y)/np.std(Y[~np.isnan(X)]))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c24118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import f  \n",
    "from scipy.stats import t \n",
    "from scipy.linalg import pinv, lstsq  \n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.nets.nets_pearson import nets_pearson\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "# Demean the confound data for the current site and nonlinear confound\n",
    "X = nets_demean(pd.DataFrame(nonlinear_confound)).values\n",
    "\n",
    "# Get Y\n",
    "Y = IDPs_deconf[:, num_IDP].values\n",
    "Y = Y[inds_per_site[site_no-1]]\n",
    "\n",
    "# Remove potential nans from X\n",
    "Y = Y[~np.isnan(X)]\n",
    "X = X[~np.isnan(X)]\n",
    "\n",
    "# Get predicted Y = Xbeta\n",
    "pred_Y = np.nansum(X*Y)/np.nansum(X**2)*X\n",
    "\n",
    "# Compute the residuals\n",
    "resids = Y - pred_Y\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Variance explained version 1\n",
    "# --------------------------------------------------------\n",
    "# Get variance explained by pred_Y\n",
    "ve1 = 100*((np.nanstd(pred_Y)/np.std(Y[~np.isnan(X)]))**2)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# P Version 1\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Compute the sum of squares for the effect\n",
    "SSeffect = np.linalg.norm(pred_Y - np.mean(pred_Y))**2  \n",
    "\n",
    "# Compute the sum of squares for the error\n",
    "SSerror = np.linalg.norm(resids - np.mean(resids))**2  \n",
    "\n",
    "# Compute the degrees of freedom for the effect\n",
    "df = np.linalg.matrix_rank(X) \n",
    "\n",
    "# Compute the degrees of freedom for the error\n",
    "dferror = len(Y) - df  \n",
    "\n",
    "# Compute the F-statistic\n",
    "F = (SSeffect / df) / (SSerror / dferror)  \n",
    "\n",
    "# Compute p[i] using the F-distribution\n",
    "p = 1 - f.cdf(F, df, dferror)  \n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Variance explained version 2\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Construct new design matrix\n",
    "XplusIntercept = np.ones((X.shape[0],2))\n",
    "XplusIntercept[:,1] = X[:]\n",
    "\n",
    "# Perform OLS regression\n",
    "U, D, Vt = np.linalg.svd(XplusIntercept, full_matrices=False)\n",
    "\n",
    "# Get the rank of the matrix\n",
    "rank = np.sum(D > 1e-10)\n",
    "\n",
    "# Rank reduce U, D and Vt\n",
    "U = U[:, :rank] \n",
    "Vt = Vt[:rank,:]\n",
    "D = D[:rank]\n",
    "\n",
    "# Get betahat\n",
    "beta = (Vt.T/D) @ (U.T @ Y)\n",
    "\n",
    "# Get residuals\n",
    "resids = Y - XplusIntercept @ beta\n",
    "\n",
    "# Get sigma^2 estimator\n",
    "sigma2 = np.sum(resids**2)/Y.shape[0]\n",
    "\n",
    "# Contrast for beta2\n",
    "L = np.array([[0],[1]])\n",
    "\n",
    "# Contrast variance\n",
    "invDVtL = Vt/D @ L \n",
    "varLtBeta = np.sqrt(sigma2*invDVtL.T @ invDVtL)\n",
    "\n",
    "# T statistic for contrast\n",
    "T = L.T @ beta / varLtBeta\n",
    "\n",
    "# Second version of variance explained\n",
    "ve2 = 100*(1-(np.std(resids)**2/np.std(Y)**2))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# P-value version 2\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# P value\n",
    "p2 = 1 - t.cdf(T, dferror)[0,0]  \n",
    "\n",
    "# --------------------------------------------------------\n",
    "# P-value version 3\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Compute pearson coefficient\n",
    "R, p3 = nets_pearson(X,Y)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Variance explained version 3\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Compute version 3 of variance explained\n",
    "ve3 = 100*R**2\n",
    "\n",
    "print(p, p2, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe21bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from lib.func_01_05 import func_01_05_gen_nonlin_conf\n",
    "\n",
    "t1 = time.time()\n",
    "func_01_05_gen_nonlin_conf(data_dir, out_dir, num_IDP, nonlinear_confounds, IDPs_deconf)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba62f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_IDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, as_completed\n",
    "from lib.script_01_05 import func_01_05_gen_nonlin_conf\n",
    "\n",
    "cluster_cfg = {'cluster_type':'slurm','num_nodes':100}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Handle empty configuration\n",
    "# --------------------------------------------------------------------------------\n",
    "if cluster_cfg is None:\n",
    "\n",
    "    # Set new local configuration\n",
    "    cluster_cfg = {'cluster_type':'local','num_nodes':1}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Set up cluster\n",
    "# --------------------------------------------------------------------------------\n",
    "if 'cluster_type' in cluster_cfg:\n",
    "\n",
    "    # Check if we are using a HTCondor cluster\n",
    "    if cluster_cfg['cluster_type'].lower() == 'htcondor':\n",
    "\n",
    "        # Load the HTCondor Cluster\n",
    "        from dask_jobqueue import HTCondorCluster\n",
    "        cluster = HTCondorCluster()\n",
    "\n",
    "    # Check if we are using an LSF cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'lsf':\n",
    "\n",
    "        # Load the LSF Cluster\n",
    "        from dask_jobqueue import LSFCluster\n",
    "        cluster = LSFCluster()\n",
    "\n",
    "    # Check if we are using a Moab cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'moab':\n",
    "\n",
    "        # Load the Moab Cluster\n",
    "        from dask_jobqueue import MoabCluster\n",
    "        cluster = MoabCluster()\n",
    "\n",
    "    # Check if we are using a OAR cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'oar':\n",
    "\n",
    "        # Load the OAR Cluster\n",
    "        from dask_jobqueue import OARCluster\n",
    "        cluster = OARCluster()\n",
    "\n",
    "    # Check if we are using a PBS cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'pbs':\n",
    "\n",
    "        # Load the PBS Cluster\n",
    "        from dask_jobqueue import PBSCluster\n",
    "        cluster = PBSCluster()\n",
    "\n",
    "    # Check if we are using an SGE cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'sge':\n",
    "\n",
    "        # Load the SGE Cluster\n",
    "        from dask_jobqueue import SGECluster\n",
    "        cluster = SGECluster()\n",
    "\n",
    "    # Check if we are using a SLURM cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'slurm':\n",
    "\n",
    "        # Load the SLURM Cluster\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster()\n",
    "\n",
    "    # Check if we are using a local cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'local':\n",
    "\n",
    "        # Load the Local Cluster\n",
    "        from dask.distributed import LocalCluster\n",
    "        cluster = LocalCluster()\n",
    "\n",
    "    # Raise a value error if none of the above\n",
    "    else:\n",
    "        raise ValueError('The cluster type, ' + cluster_cfg['cluster_type'] + ', is not recognized.')\n",
    "\n",
    "else:\n",
    "    # Raise a value error if the cluster type was not specified\n",
    "    raise ValueError('Please specify \"cluster_type\" in the cluster configuration.')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Connect to client\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Connect to cluster\n",
    "client = Client(cluster)   \n",
    "\n",
    "# Read in number of nodes we need\n",
    "num_nodes = int(cluster_cfg['num_nodes'])\n",
    "\n",
    "# Scale the cluster\n",
    "cluster.scale(num_nodes)\n",
    "\n",
    "# Get the dashboard link\n",
    "dashboard_link = client.cluster.dashboard_link\n",
    "dashboard_port = dashboard_link.split(':')[-1].split('/')[0]\n",
    "\n",
    "print(\"Dask distributed is now running at the following address on your cluster: \" + \n",
    "      dashboard_link + \". If you wish to run locally, port \" + str(dashboard_port) +\n",
    "      \" to your machine. e.g. run something like: \\n \\n ssh -L \"+ str(dashboard_port) + ':localhost:' +\n",
    "      str(dashboard_port) + \" username@cluster_address \\n \\nand then navigate to http://localhost:\" + \n",
    "      str(dashboard_port) +\"/status to view the console.\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Run cluster jobs\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Get the number of nonlinear confounds\n",
    "num_conf_nonlin = nonlinear_confounds.shape[1]\n",
    "\n",
    "# Get the number of IDPs\n",
    "num_IDPs = IDPs_deconf.shape[1]\n",
    "\n",
    "# Empty futures list\n",
    "futures = []\n",
    "\n",
    "# Submit jobs\n",
    "for i in np.arange(num_IDPs):\n",
    "\n",
    "    # Run the i^{th} job.\n",
    "    future_i = client.submit(func_01_05_gen_nonlin_conf, data_dir, out_dir, i, \n",
    "                             nonlinear_confounds_fname, IDPs_deconf_fname, \n",
    "                             pure=False)\n",
    "\n",
    "    # Append to list \n",
    "    futures.append(future_i)\n",
    "\n",
    "# Completed jobs\n",
    "completed = as_completed(futures)\n",
    "\n",
    "# Wait for results\n",
    "for i in completed:\n",
    "    i.result()\n",
    "\n",
    "# Delete the future objects (NOTE: This is important! If you don't delete the \n",
    "# futures dask tries to rerun them every time you call the result function).\n",
    "del i, completed, futures, future_i\n",
    "\n",
    "# Work out columns and index for dataframes\n",
    "indices = IDPs_deconf.columns\n",
    "columns = nonlinear_confounds.columns\n",
    "\n",
    "# Create p1 memory mapped df\n",
    "p1 = np.memmap(os.path.join(out_dir, 'p1.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "p1 = pd.DataFrame(p1,index=indices,columns=columns)\n",
    "p1 = MemoryMappedDF(p1)\n",
    "\n",
    "# Create p2 memory mapped df\n",
    "p2 = np.memmap(os.path.join(out_dir, 'p2.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "p2 = pd.DataFrame(p2,index=indices,columns=columns)\n",
    "p2 = MemoryMappedDF(p2)\n",
    "\n",
    "# Create p3 memory mapped df\n",
    "p3 = np.memmap(os.path.join(out_dir, 'p3.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "p3 = pd.DataFrame(p3,index=indices,columns=columns)\n",
    "p3 = MemoryMappedDF(p3)\n",
    "\n",
    "# Create ve1 memory mapped df\n",
    "ve1 = np.memmap(os.path.join(out_dir, 've1.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "ve1 = pd.DataFrame(ve1,index=indices,columns=columns)\n",
    "ve1 = MemoryMappedDF(ve1)\n",
    "\n",
    "# Create ve2 memory mapped df\n",
    "ve2 = np.memmap(os.path.join(out_dir, 've2.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "ve2 = pd.DataFrame(ve2,index=indices,columns=columns)\n",
    "ve2 = MemoryMappedDF(ve2)\n",
    "\n",
    "# Create ve3 memory mapped df\n",
    "ve3 = np.memmap(os.path.join(out_dir, 've3.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "ve3 = pd.DataFrame(ve3,index=indices,columns=columns)\n",
    "ve3 = MemoryMappedDF(ve3)\n",
    "\n",
    "# Remove original files\n",
    "fnames = [os.path.join(out_dir, 'p1.npy'), os.path.join(out_dir, 'p2.npy'),\n",
    "          os.path.join(out_dir, 'p3.npy'), os.path.join(out_dir, 've1.npy'),\n",
    "          os.path.join(out_dir, 've2.npy'), os.path.join(out_dir, 've3.npy')]\n",
    "\n",
    "# Loop through files removing each\n",
    "for fname in fnames:\n",
    "    os.remove(fname)\n",
    "\n",
    "#return(p1,p2,p3,ve1,ve2,ve3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b1a3c-f478-4588-801a-365af6480f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the number of nonlinear confounds\n",
    "num_conf_nonlin = nonlinear_confounds.shape[1]\n",
    "\n",
    "# Get the number of IDPs\n",
    "num_IDPs = IDPs_deconf.shape[1]\n",
    "\n",
    "tmp = np.memmap(os.path.join(out_dir, 'p1.npy'),dtype=np.float32,shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdd324-9f82-45de-8bb7-830a2250a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Work out columns and index for dataframes\n",
    "indices = IDPs_deconf.columns\n",
    "columns = nonlinear_confounds.columns\n",
    "\n",
    "# Create p1 memory mapped df\n",
    "p1 = np.memmap(os.path.join(out_dir, 'p1.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "p1 = pd.DataFrame(p1,index=indices,columns=columns)\n",
    "p1 = MemoryMappedDF(p1)\n",
    "\n",
    "# Create p2 memory mapped df\n",
    "p2 = np.memmap(os.path.join(out_dir, 'p2.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "p2 = pd.DataFrame(p2,index=indices,columns=columns)\n",
    "p2 = MemoryMappedDF(p2)\n",
    "\n",
    "# Create p3 memory mapped df\n",
    "p3 = np.memmap(os.path.join(out_dir, 'p3.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "p3 = pd.DataFrame(p3,index=indices,columns=columns)\n",
    "p3 = MemoryMappedDF(p3)\n",
    "\n",
    "# Create ve1 memory mapped df\n",
    "ve1 = np.memmap(os.path.join(out_dir, 've1.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "ve1 = pd.DataFrame(ve1,index=indices,columns=columns)\n",
    "ve1 = MemoryMappedDF(ve1)\n",
    "\n",
    "# Create ve2 memory mapped df\n",
    "ve2 = np.memmap(os.path.join(out_dir, 've2.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "ve2 = pd.DataFrame(ve2,index=indices,columns=columns)\n",
    "ve2 = MemoryMappedDF(ve2)\n",
    "\n",
    "# Create ve3 memory mapped df\n",
    "ve3 = np.memmap(os.path.join(out_dir, 've3.npy'),dtype=np.float32,\n",
    "               shape=(num_IDPs, num_conf_nonlin),mode='r')[:,:]\n",
    "ve3 = pd.DataFrame(ve3,index=indices,columns=columns)\n",
    "ve3 = MemoryMappedDF(ve3)\n",
    "\n",
    "# # Remove original files\n",
    "# fnames = [os.path.join(out_dir, 'p1.npy'), os.path.join(out_dir, 'p2.npy'),\n",
    "#           os.path.join(out_dir, 'p3.npy'), os.path.join(out_dir, 've1.npy'),\n",
    "#           os.path.join(out_dir, 've2.npy'), os.path.join(out_dir, 've3.npy')]\n",
    "\n",
    "# # Loop through files removing each\n",
    "# for fname in fnames:\n",
    "#     os.remove(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc944f14-8fac-4aff-bbcf-39627fab7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDPs_deconf.shape[0]-IDPs_deconf.search_cols('ASL_region*').isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bbe47-0258-474a-b168-7e58923f5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the results as files we can reconstruct memory mapped dataframes from\n",
    "p1_fname = os.path.join(os.getcwd(),'saved_memmaps','p1.npz')\n",
    "p1.save(p1_fname)\n",
    "\n",
    "# Save the results as files we can reconstruct memory mapped dataframes from\n",
    "p2_fname = os.path.join(os.getcwd(),'saved_memmaps','p2.npz')\n",
    "p2.save(p2_fname)\n",
    "\n",
    "# Save the results as files we can reconstruct memory mapped dataframes from\n",
    "p3_fname = os.path.join(os.getcwd(),'saved_memmaps','p3.npz')\n",
    "p3.save(p3_fname)\n",
    "\n",
    "# Save the results as files we can reconstruct memory mapped dataframes from\n",
    "ve1_fname = os.path.join(os.getcwd(),'saved_memmaps','ve1.npz')\n",
    "ve1.save(ve1_fname)\n",
    "\n",
    "# Save the results as files we can reconstruct memory mapped dataframes from\n",
    "ve2_fname = os.path.join(os.getcwd(),'saved_memmaps','ve2.npz')\n",
    "ve2.save(ve2_fname)\n",
    "\n",
    "# Save the results as files we can reconstruct memory mapped dataframes from\n",
    "ve3_fname = os.path.join(os.getcwd(),'saved_memmaps','ve3.npz')\n",
    "ve3.save(ve3_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bfd1f",
   "metadata": {},
   "source": [
    "## Garbage Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this won't execute in Jupyter until the code is restarted.\n",
    "#del IDPs, nonIDPs, misc, categorical_IDPs, continuous_IDPs, other_IDPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
