{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import shutil  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "\n",
    "from script_01_00 import generate_initial_variables\n",
    "from script_01_01 import generate_raw_confounds\n",
    "from script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from nets.nets_load_match import nets_load_match\n",
    "from nets.nets_inverse_normal import nets_inverse_normal \n",
    "from nets.nets_normalise import nets_normalise \n",
    "from nets.nets_demean import nets_demean\n",
    "from nets.nets_deconfound_multiple import nets_deconfound_multiple\n",
    "\n",
    "from duplicate.duplicate_categorical import duplicate_categorical\n",
    "from duplicate.duplicate_demedian_norm_by_site import duplicate_demedian_norm_by_site\n",
    "\n",
    "from preproc.datenum import datenum\n",
    "from preproc.switch_type import switch_type\n",
    "from preproc.days_in_year import days_in_year\n",
    "from preproc.filter_columns_by_site import filter_columns_by_site\n",
    "\n",
    "from memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from memmap.read_memmap_df import read_memmap_df\n",
    "from memmap.addBlockToMmap import addBlockToMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82731372-cb80-4dca-aaec-ce4671396009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2982046-cd27-46f4-a701-72855e94cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in precomputed memmaps\n",
    "IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "IDPs_deconf = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz'))\n",
    "nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))\n",
    "confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))\n",
    "nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz'))\n",
    "# p1 = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','p.npz'))\n",
    "# nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds_reduced.npz'))\n",
    "# IDPs_deconf_ct = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf_ct.npz'))\n",
    "# confounds_with_ct = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds_with_ct.npz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb35a9-c90e-459c-bc48-b5a804976d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precomputed filenames\n",
    "IDPs_deconf_smooth_fname = os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf_smooth.npz')\n",
    "confounds_with_smooth_fname = os.path.join(os.getcwd(),'saved_memmaps','confounds_with_smooth.npz')\n",
    "\n",
    "# Read in precomputed\n",
    "IDPs_deconf_smooth = read_memmap_df(IDPs_deconf_smooth_fname)\n",
    "confounds_with_smooth = read_memmap_df(confounds_with_smooth_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e74b51-612d-4ab8-bc74-59dd21fbd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(confounds_with_smooth.get_groups(['acq date']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5393523-df05-4450-95cc-f1ba24f8d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_with_smooth.get_groups(['acq date']).iloc[-10:, -9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29cbfb-bb73-4d8f-a99d-fa69db24e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_with_smooth.get_groups(['acq time']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4120900-b7c7-467e-8005-7fb28aa49631",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_with_smooth.get_groups(['acq time']).iloc[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a56bd9-2c3f-4948-bcc8-026c8d39aed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e00bb3-4370-4204-9392-33edc9ef88c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4cd5e-025a-4735-9c5c-173afa5c2017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584e2d8-2b40-4d99-8043-26a90fd9c0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608397b5-5d17-4748-9fe5-a4877fe3e7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee4845-8ff6-4022-8c56-5b381a90a4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73599f99-d102-4171-bc7c-b94250949833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f07d7-2428-4936-9784-bdb94a797bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af04f8-6f0c-4ed9-abd6-9a140587c61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61142d-4aa0-4837-ae80-eab96c9e4bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c4305-1181-4eb8-8494-6b5823c87826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749188f-324a-4e8d-9f89-8acda39694ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562cc35-fc28-49d7-9be1-5086410f9e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4ac76-8b9b-4579-81f1-4af52b643b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c00c2-decf-48f3-886d-a8803bd44449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4d454-c04c-42f9-a5b1-77466e202e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309f030-8fb4-416c-a307-bff5ee35e327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af47658-27d6-4582-87b0-2c6fa70af3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9efc0d-a72d-489d-ab4d-3db6cce9ab70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e9bad-bca3-4996-b103-20519c344b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5429294-ada2-4c09-bb58-39ca993d716d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e79d4-9b1b-44dd-8fc6-f67a51ead5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6112ba-458f-459e-8bd7-989a3111629f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3398115-5713-416f-b954-326848c4ffc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5c371-fdbe-4b1c-972e-563ad4e369ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4bbf0-0580-414d-aa1d-9bacc3351f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a2ea6-9485-487e-b537-239b9e2cd675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa498f44-fffd-4edf-8087-6c31494d9fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7860a-d821-4ad5-940b-13ff2ce4d469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cdc5d5-4a4d-4fde-9095-da35553bcc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9735250f-76f7-44b2-9ee1-0624bd833413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a92346-dc4a-40b1-808d-f0d1608fc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from nets.nets_load_match import nets_load_match\n",
    "from nets.nets_inverse_normal import nets_inverse_normal\n",
    "\n",
    "from preproc.datenum import datenum\n",
    "from preproc.days_in_year import days_in_year\n",
    "\n",
    "from memmap.MemoryMappedDF import MemoryMappedDF\n",
    "\n",
    "from logio.my_log import my_log\n",
    "from logio.loading import ascii_loading_bar\n",
    "\n",
    "# Log file\n",
    "logfile = os.path.join(os.getcwd(),'log.html')\n",
    "\n",
    "# Remove previous log file\n",
    "if os.path.exists(logfile):\n",
    "    os.remove(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bddf320-ff5a-42e4-abcd-cfa348740bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update log\n",
    "my_log(str(datetime.now()) +': Stage 1: Generating Initial Variables.', mode='a', filename=logfile)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read subject IDs in and exclude those that have been dropped\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Load the subject ids\n",
    "sub_ids = np.loadtxt(os.path.join(data_dir, 'subj.txt'), dtype=int)\n",
    "\n",
    "# Load the subject ids to be excluded\n",
    "sub_ids_to_exclude = np.loadtxt(os.path.join(data_dir, 'excluded_subjects.txt'), dtype=int)\n",
    "\n",
    "# Find the common elements between the two arrays\n",
    "common_ids = np.intersect1d(sub_ids, sub_ids_to_exclude)\n",
    "\n",
    "# Remove the common elements from sub_ids\n",
    "sub_ids = np.setdiff1d(sub_ids, common_ids)\n",
    "\n",
    "# Clean workspace\n",
    "del sub_ids_to_exclude, common_ids\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read all IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# dtypes for IDPs\n",
    "dtypes = {i: 'float64' for i in range(893)}\n",
    "dtypes[0] = 'int32'\n",
    "dtypes[892] = 'int32'\n",
    "\n",
    "# Load the IDPs\n",
    "all_IDPs = nets_load_match(os.path.join(data_dir, 'IDPs.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# Find subjects with T1\n",
    "t1_subs = ~all_IDPs.iloc[:, 16].isna()\n",
    "\n",
    "# Remove non-T1 subs from IDPs and subject ids\n",
    "all_IDPs = all_IDPs.loc[t1_subs]\n",
    "sub_ids = sub_ids[t1_subs]\n",
    "\n",
    "# Get the number of rows in ALL_IDs\n",
    "n = len(sub_ids)\n",
    "\n",
    "# Clean workspace\n",
    "del t1_subs\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read fmrib info and compute time stamps\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# dtypes for fmrib info\n",
    "dtypes = {i: 'float64' for i in range(6)}\n",
    "dtypes[0] = 'int32'\n",
    "dtypes[1] = 'int32'\n",
    "dtypes[4] = 'int32'\n",
    "\n",
    "# Read in info \n",
    "fmrib_info = nets_load_match(os.path.join(data_dir, 'ID_initial_workspace.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# Get the acquisition time\n",
    "time_stamp_h = np.floor(fmrib_info.iloc[:, 1] / 10000)\n",
    "time_stamp_m = np.floor((fmrib_info.iloc[:, 1] - time_stamp_h * 10000) / 100)\n",
    "time_stamp_s = fmrib_info.iloc[:, 1] - time_stamp_h * 10000 - time_stamp_m * 100\n",
    "\n",
    "# Convert time of day to \"decimal\" hours\n",
    "fmrib_info.iloc[:, 1] = time_stamp_h + time_stamp_m / 60 + time_stamp_s / 3600\n",
    "\n",
    "# Get the fraction of the day when the subject was acquired\n",
    "day_fraction = (fmrib_info.iloc[:, 1] - 7) / 13\n",
    "\n",
    "# Get acquisition date\n",
    "time_stamp_y = np.floor(fmrib_info.iloc[:, 0] / 10000)\n",
    "time_stamp_m = np.floor((fmrib_info.iloc[:, 0] - time_stamp_y * 10000) / 100)\n",
    "time_stamp_d = fmrib_info.iloc[:, 0] - time_stamp_y * 10000 - time_stamp_m * 100\n",
    "\n",
    "# Convert scan date to \"decimal\" years\n",
    "dates = [datenum(int(y), int(m), int(d)) for y, m, d in zip(time_stamp_y, time_stamp_m, time_stamp_d)]\n",
    "days_since_year_start = np.array([(date - datenum(int(y), 1, 1)) for date, y in zip(dates, time_stamp_y)])\n",
    "\n",
    "# Output decimal years (note we need change datatype for compatibility\n",
    "fmrib_info = fmrib_info.astype({1: 'float64'})\n",
    "fmrib_info.iloc[:, 0] = time_stamp_y + days_since_year_start / days_in_year(time_stamp_y)\n",
    "\n",
    "# Calculate the discrete and continuous scan date (that is scan date given to the nearest day vs\n",
    "# to the nearest second)\n",
    "scan_date = fmrib_info.iloc[:, 0]\n",
    "scan_date_cont = time_stamp_y + (days_since_year_start + day_fraction) / days_in_year(time_stamp_y)\n",
    "\n",
    "# Clean workspace\n",
    "del time_stamp_h, time_stamp_m, time_stamp_s, time_stamp_d, time_stamp_y\n",
    "del dates, days_since_year_start\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read resting state IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# dtypes for noise 25\n",
    "dtypes = {i: 'float64' for i in range(22)}\n",
    "dtypes[0] = 'int32'\n",
    "\n",
    "# dtypes for noise 100\n",
    "dtypes = {i: 'float64' for i in range(56)}\n",
    "dtypes[0] = 'int32'\n",
    "\n",
    "# Read in node amplitudes\n",
    "node_amps_25 = nets_load_match(os.path.join(data_dir, 'rfMRI_d25_NodeAmplitudes_v1.txt'), sub_ids, dtypes=dtypes)\n",
    "node_amps_100 = nets_load_match(os.path.join(data_dir, 'rfMRI_d100_NodeAmplitudes_v1.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# dtypes for noise 100\n",
    "dtypes = {i: 'float64' for i in range(211)}\n",
    "dtypes[0] = 'int32'\n",
    "\n",
    "# Read in partial correlation network IDPs\n",
    "net_25 = nets_load_match(os.path.join(data_dir, 'rfMRI_d25_partialcorr_v1.txt'), sub_ids, dtypes=dtypes)\n",
    "net_100 = nets_load_match(os.path.join(data_dir, 'rfMRI_d100_partialcorr_v1.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in FS IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "# dtypes for FS\n",
    "dtypes = {i: 'float64' for i in range(1274)}\n",
    "dtypes[0] = 'int32'\n",
    "\n",
    "FS = nets_load_match(os.path.join(data_dir, 'FS_IDPs.txt'), sub_ids, dtypes=dtypes)\n",
    "FS_use_T2 = FS.iloc[:, 0]  # Get the first column\n",
    "FS = FS.iloc[:, 1:]  # Get the rest of the columns except the first\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in ASL IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "# dtypes for ASL\n",
    "dtypes = {i: 'float64' for i in range(51)}\n",
    "dtypes[0] = 'int32'\n",
    "\n",
    "ASL = nets_load_match(os.path.join(data_dir, 'ASL_IDPs.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in QSM IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "# dtypes for QSM\n",
    "dtypes = {i: 'float64' for i in range(19)}\n",
    "dtypes[0] = 'int32'\n",
    "\n",
    "QSM = nets_load_match(os.path.join(data_dir, 'QSM_IDPs.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in WMH \n",
    "# ----------------------------------------------------------------------------------\n",
    "# dtypes for QSM\n",
    "dtypes = {i: 'float64' for i in range(3)}\n",
    "dtypes[0] = 'int32'\n",
    "\n",
    "WMH = nets_load_match(os.path.join(data_dir, 'ID_WMH.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in IDP names\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Assuming the file is space or tab delimited. \n",
    "df = pd.read_csv(os.path.join(data_dir, \"IDPinfo.txt\"), sep='\\t', usecols=[0], header=0)\n",
    "\n",
    "# Remove the first name as this is the subject index column, which we will delete\n",
    "IDP_names = df.values\n",
    "IDP_names = list(IDP_names.reshape(np.prod(IDP_names.shape)))\n",
    "\n",
    "# Clean up\n",
    "del df \n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add IDP names for resting state ICA\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Append rfMRI amplitudes (ICA25 nodes)\n",
    "IDP_names.extend([f'rfMRI amplitudes (ICA25 node {i})' for i in range(1, node_amps_25.shape[1] + 1)])\n",
    "\n",
    "# Append rfMRI amplitudes (ICA100 nodes)\n",
    "IDP_names.extend([f'rfMRI amplitudes (ICA100 node {i})' for i in range(1, node_amps_100.shape[1] + 1)])\n",
    "\n",
    "# Append rfMRI connectivity (ICA25 edges)\n",
    "IDP_names.extend([f'rfMRI connectivity (ICA25 edge {i})' for i in range(1, net_25.shape[1] + 1)])\n",
    "\n",
    "# Append rfMRI connectivity (ICA100 edges)\n",
    "IDP_names.extend([f'rfMRI connectivity (ICA100 edge {i})' for i in range(1, net_100.shape[1] + 1)])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add IDP names for for FS, ASL, QSM and QC\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Read FS_names.txt\n",
    "with open(f\"{data_dir}/FS_names.txt\", 'r') as file:\n",
    "    FS_names = file.read().splitlines()\n",
    "\n",
    "# Exclude the first name\n",
    "FS_names = FS_names[1:]  \n",
    "\n",
    "# Read ASL_names.txt\n",
    "with open(f\"{data_dir}/ASL_names.txt\", 'r') as file:\n",
    "    ASL_names = file.read().splitlines()\n",
    "\n",
    "# Read QSM_names.txt\n",
    "with open(f\"{data_dir}/QSM_names.txt\", 'r') as file:\n",
    "    QSM_names = file.read().splitlines()\n",
    "\n",
    "# Add the 2 IDP names from WMH\n",
    "WMH_names =['IDP_T2_FLAIR_BIANCA_periventWMH_volume',\n",
    "            'IDP_T2_FLAIR_BIANCA_deepWMH_volume']\n",
    "\n",
    "# Add to running IDP names\n",
    "IDP_names = IDP_names + FS_names + ASL_names + QSM_names + WMH_names\n",
    "\n",
    "# QC IDPs and names\n",
    "QC_IDPs = all_IDPs.iloc[:, 0:16]\n",
    "QC_IDPs_names = IDP_names[0:16]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# IDPs to exclude vs include\n",
    "# ----------------------------------------------------------------------------------\n",
    "ind_IDPs_to_exclude = list(range(1, 18)) + list(range(888, 893))\n",
    "ind_IDPs_to_include = np.setdiff1d(np.arange(all_IDPs.shape[1]), np.array(ind_IDPs_to_exclude) - 1)\n",
    "\n",
    "# Select the specified columns from all_IDPs\n",
    "subset_all_IDPs = all_IDPs.iloc[:, ind_IDPs_to_include]\n",
    "\n",
    "# Concatenate all the DataFrames/Series horizontally\n",
    "subset_IDPs = pd.concat([\n",
    "    subset_all_IDPs.reset_index(drop=True), \n",
    "    node_amps_25.reset_index(drop=True),\n",
    "    node_amps_100.reset_index(drop=True),\n",
    "    net_25.reset_index(drop=True),\n",
    "    net_100.reset_index(drop=True),\n",
    "    FS.reset_index(drop=True),\n",
    "    ASL.reset_index(drop=True),\n",
    "    QSM.reset_index(drop=True),\n",
    "    WMH.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Update the included indices to include the node_amps_25, node_amps_100, etc.\n",
    "ind_names_to_include = np.hstack((ind_IDPs_to_include + 1, range(893, len(IDP_names) + 1))) - 1  \n",
    "\n",
    "# Construct IDP_names by subsetting\n",
    "IDP_names = [IDP_names[i] for i in ind_names_to_include]\n",
    "\n",
    "# Add names to all_IDPs\n",
    "subset_IDPs.columns = IDP_names\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Outlier detection. \n",
    "# ----------------------------------------------------------------------------------\n",
    "# Below is a description of what is happening here, taken from \"Confound modelling \n",
    "# in UK Biobank brain imaging\":\n",
    "#\n",
    "# For any given confound, we define outliers thus: First we subtract the median\n",
    "# value from all subjectsâ€™ values. We then compute the median-absolutedeviation \n",
    "# (across all subjects) and multiply this MAD by 1.48 (so that it is equal to \n",
    "# the standard deviation if the data had been Gaussian). We then normalise all \n",
    "# values by dividing them by this scaled MAD. Finally, we define values as \n",
    "# outliers if their magnitude is greater than 8.\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Subtract the median, ignoring NaNs\n",
    "subset_IDPs_m = subset_IDPs - np.nanmedian(subset_IDPs, axis=0)\n",
    "\n",
    "# Calculate the median absolute deviation, again ignoring NaNs\n",
    "medabs = np.nanmedian(np.abs(subset_IDPs_m), axis=0)\n",
    "\n",
    "# np.finfo(float).eps is machine epsilon for float64\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "# Get a mask for the absolute median\n",
    "low_medabs_mask = medabs < eps\n",
    "\n",
    "# Standardise the non-zero medians\n",
    "if np.any(low_medabs_mask):\n",
    "    medabs[low_medabs_mask] = np.nanstd(subset_IDPs_m.iloc[:, low_medabs_mask], axis=0) / 1.48\n",
    "\n",
    "# Divide by medabs\n",
    "subset_IDPs_m = subset_IDPs_m / medabs\n",
    "\n",
    "# Set values with absolute value greater than 5 to NaN\n",
    "subset_IDPs_m[np.abs(subset_IDPs_m) > 5] = np.nan\n",
    "\n",
    "# Update log\n",
    "my_log(str(datetime.now()) +': Loaded initial variables.', mode='a', filename=logfile)\n",
    "my_log(str(datetime.now()) +': Performing quartile normalisation of IDPs...', mode='a', filename=logfile)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Quartile Normalisation of IDPS\n",
    "# ----------------------------------------------------------------------------------\n",
    "IDPs = nets_inverse_normal(subset_IDPs_m)\n",
    "\n",
    "# Update log\n",
    "my_log(str(datetime.now()) +': Quartile normalisation complete.', mode='r', filename=logfile)\n",
    "my_log(str(datetime.now()) +': Loading miscellaneous variables...', mode='a', filename=logfile)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Non-IDPS\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read general names\n",
    "with open(os.path.join(data_dir, 'OTHER_GENERAL_names.txt'), 'r') as file:\n",
    "    gen_names = [line.strip() for line in file]\n",
    "\n",
    "# Datatypes for this file (general variables have a mix so best specify)\n",
    "dtypes = {0: 'int32', 1: 'float64', 2: 'float64', 3: 'float64', 4: 'object', 5: 'object', 6: 'object', 7: 'object'}\n",
    "\n",
    "# General variables\n",
    "gen_vars = nets_load_match(os.path.join(data_dir, 'ID_OTHER_GENERAL.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Work out scan dates\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Replace NaN in scan_date with the maximum value plus 0.1\n",
    "scan_date = scan_date.values\n",
    "scan_date[np.isnan(scan_date)] = np.nanmax(scan_date) + 0.1\n",
    "scan_date = pd.Series(scan_date)\n",
    "\n",
    "# Extract sex, year of birth (yob) and month of birth (mob) from GEN_vars\n",
    "sex = gen_vars.iloc[:, 0]\n",
    "yob = gen_vars.iloc[:, 1]\n",
    "mob = gen_vars.iloc[:, 2]\n",
    "\n",
    "# Calculate birth_date as year plus the adjusted month value divided by 12\n",
    "birth_date = yob + (mob - 0.5) / 12\n",
    "\n",
    "# Calculate age by subtracting birth_date from scan_date\n",
    "age = scan_date - birth_date\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Write out age, sex and head-size\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Create a Pandas DataFrame to organize the data\n",
    "nonIDPs = pd.DataFrame({\n",
    "    'ID': sub_ids,\n",
    "    'AGE': age,\n",
    "    'SEX': sex,\n",
    "    'HEADSIZE': np.where(np.isnan(all_IDPs.iloc[:, 17]), np.nan, all_IDPs.iloc[:, 16]),\n",
    "    'TOD': day_fraction,\n",
    "    'FST2': FS_use_T2\n",
    "})\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Create a mapping between column names and file names\n",
    "column_file_mapping = {'AGE': 'ID_AGE.txt',\n",
    "                       'SEX': 'ID_SEX.txt',\n",
    "                       'HEADSIZE': 'ID_HEADSIZE.txt',\n",
    "                       'TOD': 'ID_TOD.txt',\n",
    "                       'FST2': 'ID_FST2.txt'}\n",
    "\n",
    "# Save each dataframe column to a separate text file\n",
    "for col_name in column_file_mapping.keys():\n",
    "\n",
    "    # Get the filepath\n",
    "    file_path = os.path.join(out_dir, column_file_mapping[col_name])\n",
    "\n",
    "    # Remove previous file if needed\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "    # Save column\n",
    "    nonIDPs[['ID', col_name]].to_csv(file_path, sep=' ', index=False, header=False, na_rep='NaN')\n",
    "    \n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in Eddy currents and tablepos (currently unused)\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Define the directory paths\n",
    "workspaces_dir = os.path.join(out_dir, 'workspaces', 'ws_00')\n",
    "figs_dir = os.path.join(out_dir, 'figs', 'EDDYQC')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(workspaces_dir, exist_ok=True)\n",
    "\n",
    "# Datatypes for ed\n",
    "dtypes = {0: 'int32', 1: 'float64', 2: 'float64', 3: 'float64', 4: 'object'}\n",
    "ed = nets_load_match(os.path.join(data_dir, 'ID_EDDYQC.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# Datatypes for ta\n",
    "dtypes = {0: 'int32', 1: 'float64', 2: 'float64', 3: 'float64'}\n",
    "ta = nets_load_match(os.path.join(data_dir, 'ID_TABLEPOS.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Sort data\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Sort scan_date and get the sorted indices (we use a lexsort to consistently handle\n",
    "# tied elements)\n",
    "index_sorted_date = pd.Series(np.lexsort((np.arange(len(scan_date)),scan_date.values)))\n",
    "sorted_date = scan_date.iloc[index_sorted_date]\n",
    "\n",
    "# Sort IDPs\n",
    "IDPs = IDPs.iloc[index_sorted_date,:]\n",
    "nonIDPs = nonIDPs.iloc[index_sorted_date,:]\n",
    "\n",
    "# Reorder subject ids based on sorted indices\n",
    "sub_ids = sub_ids[index_sorted_date]\n",
    "\n",
    "# Set row indices on dataframes\n",
    "IDPs.index = sub_ids\n",
    "nonIDPs.index = sub_ids\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Miscellaneous variables (this houses any variables that I'm unsure are used)\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Get the general variables that we haven't saved elsewhere\n",
    "misc = gen_vars.iloc[index_sorted_date, 1:]\n",
    "\n",
    "# Read general names\n",
    "with open(f\"{data_dir}/OTHER_GENERAL_names.txt\", 'r') as file:\n",
    "    gen_names = file.read().splitlines()\n",
    "    \n",
    "# Set column names\n",
    "misc.columns = gen_names[1:]\n",
    "misc.index = sub_ids\n",
    "\n",
    "# Convert string columns to appropriate type\n",
    "object_cols = misc.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert the object columns to string\n",
    "misc[object_cols] = misc[object_cols].astype(\"string\")\n",
    "\n",
    "# Update log\n",
    "my_log(str(datetime.now()) +': Loaded miscellaneous variables and sorted.', mode='r', filename=logfile)\n",
    "my_log(str(datetime.now()) +': Saving results...', mode='a', filename=logfile)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Output memmaps\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Return IDPs dataframe\n",
    "IDPs = MemoryMappedDF(IDPs)\n",
    "\n",
    "# Return non-IDPs dataframe\n",
    "nonIDPs = MemoryMappedDF(nonIDPs)\n",
    "\n",
    "# Return miscellaneous dataframe\n",
    "misc = MemoryMappedDF(misc)\n",
    "\n",
    "# Update\n",
    "my_log(str(datetime.now()) +': Results saved.', mode='r', filename=logfile)\n",
    "my_log(str(datetime.now()) +': Stage 1: Complete.', mode='a', filename=logfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5d065-373d-4674-b5f7-afe7c4d7292d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
