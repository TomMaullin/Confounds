{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import shutil  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "\n",
    "from script_01_00 import generate_initial_variables\n",
    "from script_01_01 import generate_raw_confounds\n",
    "from script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from nets.nets_load_match import nets_load_match\n",
    "from nets.nets_inverse_normal import nets_inverse_normal \n",
    "from nets.nets_normalise import nets_normalise \n",
    "from nets.nets_demean import nets_demean\n",
    "from nets.nets_deconfound_multiple import nets_deconfound_multiple\n",
    "\n",
    "from duplicate.duplicate_categorical import duplicate_categorical\n",
    "from duplicate.duplicate_demedian_norm_by_site import duplicate_demedian_norm_by_site\n",
    "\n",
    "from preproc.datenum import datenum\n",
    "from preproc.switch_type import switch_type\n",
    "from preproc.days_in_year import days_in_year\n",
    "from preproc.filter_columns_by_site import filter_columns_by_site\n",
    "\n",
    "from memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from memmap.read_memmap_df import read_memmap_df\n",
    "from memmap.addBlockToMmap import addBlockToMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82731372-cb80-4dca-aaec-ce4671396009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b673d-d350-4ff6-856b-34006f2111b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precomputed filenames\n",
    "p_fname = os.path.join(os.getcwd(),'saved_memmaps','p.npz')\n",
    "ve_fname = os.path.join(os.getcwd(),'saved_memmaps','ve.npz')\n",
    "\n",
    "# Read in precomputed p and ve\n",
    "p = read_memmap_df(p_fname)\n",
    "ve = read_memmap_df(ve_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2533124-d3fc-4845-bb65-d436d6d26686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import scoreatpercentile\n",
    "# Get the average and maximum variance explained\n",
    "avg_ve = ve[:,:].mean()\n",
    "max_ve = ve[:,:].max()\n",
    "\n",
    "# Get percentage thresholds\n",
    "thr_for_avg = scoreatpercentile(avg_ve, 95)\n",
    "thr_for_ve = max(0.75, scoreatpercentile(ve[:,:].dropna().values.flatten(), 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdabda6-9476-49bf-972d-175fa4f31a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_for_ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610a9bf-69f4-4c5a-8bda-33d36de026a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_ve=ve[:,:].values.flatten()\n",
    "flattened_ve = flattened_ve[~np.isnan(flattened_ve)]\n",
    "thr_for_ve = max(0.75, scoreatpercentile(flattened_ve, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a2f60-e1d4-425c-9e70-773644c3c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_for_ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2982046-cd27-46f4-a701-72855e94cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in precomputed memmaps\n",
    "IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "IDPs_deconf = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz'))\n",
    "nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))\n",
    "confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))\n",
    "nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz'))\n",
    "# p1 = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','p.npz'))\n",
    "# nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds_reduced.npz'))\n",
    "# IDPs_deconf_ct = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf_ct.npz'))\n",
    "# confounds_with_ct = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds_with_ct.npz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1adc2-17d7-454e-9c4a-5be4cb9ae72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_confounds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa2c04-8f8c-4661-b479-62cb16ff1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'<iframe src=\"' + + '\" width=\"100%\" height=\"500\"></iframe>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699765a-95ca-4c24-a9fb-1bffd32d162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_loading_bar(percentage):\n",
    "    \"\"\"\n",
    "    Generate an ASCII loading bar based on the given percentage.\n",
    "\n",
    "    Args:\n",
    "        percentage (float): The percentage value (between 0 and 100).\n",
    "\n",
    "    Returns:\n",
    "        str: The ASCII loading bar string.\n",
    "    \"\"\"\n",
    "    # Ensure the percentage is within the valid range\n",
    "    percentage = max(0, min(100, percentage))\n",
    "\n",
    "    # Calculate the number of filled and empty bars\n",
    "    bar_length = 50  # Adjust this value to change the length of the loading bar\n",
    "    filled_bars = int(bar_length * (percentage / 100))\n",
    "    empty_bars = bar_length - filled_bars\n",
    "\n",
    "    # Create the loading bar string\n",
    "    bar = '[' + '█' * filled_bars + '░' * empty_bars + ']'\n",
    "\n",
    "    # Format the percentage string\n",
    "    percentage_str = f\"{percentage:.1f}%\"\n",
    "\n",
    "    # Combine the loading bar and percentage\n",
    "    loading_bar = f\"{bar} {percentage_str}\"\n",
    "\n",
    "    return loading_bar\n",
    "\n",
    "print(ascii_loading_bar(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89f012-495e-49ed-9fdd-e594b1f1047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_log(message, mode='a', filename='file.html'):\n",
    "    \"\"\"\n",
    "    Write a message to an HTML file with a header, basic formatting, and styling.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to be written.\n",
    "        mode (str, optional): The mode to open the file with. 'a' for append, 'r' for replace current line. Defaults to 'a'.\n",
    "        filename (str, optional): The name of the HTML file. Defaults to 'file.html'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r+', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            if not lines:  # If the file is empty, create the HTML structure\n",
    "                file.write('<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Confounds Log</title>\\n')\n",
    "                file.write('<style>\\nbody { font-family: Arial, sans-serif; margin: 20px; background-color: #e6f0ff; }\\n')\n",
    "                file.write('h1 { color: #333; position: sticky; top: 0; background-color: #e6f0ff; padding: 10px; }\\n')\n",
    "                file.write('hr { border: none; border-top: 1px solid #ccc; margin: 10px 0; }\\n</style>\\n</head>\\n<body>\\n<h1>Confounds Log</h1>\\n<hr>\\n')\n",
    "                if mode == 'a':\n",
    "                    file.write(f'<p>{message}</p>\\n')\n",
    "                elif mode == 'r':\n",
    "                    file.write(f'<p>{message}</p>\\n')\n",
    "                file.write('</body>\\n</html>')\n",
    "            else:\n",
    "                if mode == 'a':\n",
    "                    lines.append(f'<p>{message}</p>\\n')\n",
    "                elif mode == 'r':\n",
    "                    print('lines 2: ', lines[-1])\n",
    "                    print(f'<p>{message}</p>\\n')\n",
    "                    if lines:\n",
    "                        lines[-3] = f'<p>{message}</p>\\n'\n",
    "                        lines[-2] = '</body>\\n'\n",
    "                        lines[-1] = '</html>'\n",
    "                    else:\n",
    "                        lines = lines[:-2]\n",
    "                        lines.append(f'<p>{message}</p>\\n')\n",
    "                        lines.append('</body>\\n')\n",
    "                        lines.append('</html>')\n",
    "                file.seek(0)\n",
    "                file.writelines(lines)\n",
    "    except FileNotFoundError:\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write('<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Confounds Log</title>\\n')\n",
    "            file.write('<style>\\nbody { font-family: Arial, sans-serif; margin: 20px; background-color: #e6f0ff; }\\n')\n",
    "            file.write('h1 { color: #333; position: sticky; top: 0; background-color: #e6f0ff; padding: 10px; }\\n')\n",
    "            file.write('hr { border: none; border-top: 1px solid #ccc; margin: 10px 0; }\\n</style>\\n</head>\\n<body>\\n<h1>Confounds Log</h1>\\n<hr>\\n')\n",
    "            if mode == 'a':\n",
    "                file.write(f'<p>{message}</p>\\n')\n",
    "            elif mode == 'r':\n",
    "                file.write(f'<p>{message}</p>\\n')\n",
    "            file.write('</body>\\n</html>')\n",
    "            \n",
    "for i in range(100):\n",
    "    my_log('test' + str(i), mode='a', filename=os.path.join(os.getcwd(),'tmp3.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b20c2-1c40-491c-89b9-ddf747317a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf241c-27c4-42cf-99e0-a6348652b157",
   "metadata": {},
   "source": [
    "## draft 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd41b4-6b21-4d4c-b5de-c9479571a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert input to memory mapped dataframes if it isn't already\n",
    "all_conf = switch_type(all_conf, out_type='MemoryMappedDF')\n",
    "IDPs = switch_type(IDPs, out_type='MemoryMappedDF')\n",
    "\n",
    "# Confound groups we are interested in.\n",
    "conf_name = ['AGE', 'AGE_SEX', 'HEAD_SIZE',  'TE', 'STRUCT_MOTION', \n",
    "             'DVARS', 'HEAD_MOTION', 'HEAD_MOTION_ST', 'TABLE_POS', \n",
    "             'EDDY_QC']\n",
    "\n",
    "# Get all the confounds in the group\n",
    "conf_group = all_conf.get_groups(conf_name)\n",
    "\n",
    "# Get the subject ids\n",
    "sub_ids = IDPs.index\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Estimate the block size (number of subjects we want to allow in memory at\n",
    "# a given time).\n",
    "# -------------------------------------------------------------------------\n",
    "# Developer note: The below is only a rough estimate, but is fairly robust\n",
    "# as it is a little conservative. The rule of thumb is to take the maximum\n",
    "# amount of memory (MAXMEM) divide it by the number of subjects we have,\n",
    "# divide by 64 (as each observation is float64 at most) and then divide by\n",
    "# 8 (as we may want to make several copies of whatever we load in, but we\n",
    "# rarely make more than 8). The resulting number should be roughly the\n",
    "# number of columns of a dataframe we are able to load in at a time. This\n",
    "# doesn't need to be perfect as often python can handle more - it is just\n",
    "# a precaution, and does improve efficiency substantially.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Rough estimate of maximum memory (bytes)\n",
    "MAXMEM = 2**32\n",
    "\n",
    "# Get the number of subjects\n",
    "n_sub = len(sub_ids)\n",
    "\n",
    "# Block size computation\n",
    "blksize = int(MAXMEM/n_sub/8/64)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site = []\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in unique_site_ids:\n",
    "\n",
    "    # Find the indices where all elements in a row of siteDATA match the current valueSite\n",
    "    # Note: This assumes siteDATA and siteValues have compatible shapes or values for comparison\n",
    "    indices = np.where((site_ids == site_id).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site.append(indices)\n",
    "\n",
    "# Delete the indices\n",
    "del indices\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Initialise empty array to store results\n",
    "conf_nonlin = pd.DataFrame(index=conf_group.index)\n",
    "\n",
    "# Load in confounds\n",
    "conf = all_conf[:,:]\n",
    "\n",
    "# Site number\n",
    "for site_index in (unique_site_ids + 1):\n",
    "    \n",
    "    # Subset the confounds to a specific site\n",
    "    conf_group_site = filter_columns_by_site(conf_group, site_index)\n",
    "\n",
    "    # Get indices for the current site\n",
    "    site_indices = inds_per_site[site_index-1] \n",
    "\n",
    "    # Reduce to just the indices we're interested in\n",
    "    conf_group_site = conf_group_site.iloc[site_indices, :]\n",
    "\n",
    "    # Get all the confounds at the site\n",
    "    all_conf_site = conf.iloc[site_indices, :]\n",
    "\n",
    "    # Get index\n",
    "    site_index = all_conf_site.index\n",
    "\n",
    "    # Get conf_group_site squared\n",
    "    conf_group_site_squared = nets_normalise(conf_group_site**2)\n",
    "    conf_group_site_squared.columns = [f\"{col}_squared\" for col in conf_group_site_squared.columns]\n",
    "\n",
    "    # Get conf_group_site inverse normalised\n",
    "    conf_group_site_inormal = nets_inverse_normal(conf_group_site);\n",
    "    conf_group_site_inormal.columns = [f\"{col}_inormal\" for col in conf_group_site_inormal.columns]\n",
    "\n",
    "    # Get conf_group_site squared inverse normalised\n",
    "    conf_group_site_squared_inormal = nets_inverse_normal(conf_group_site_squared);\n",
    "    conf_group_site_squared_inormal.columns = [f\"{col}_inormal\" for col in conf_group_site_squared_inormal.columns]\n",
    "\n",
    "    # Concatenate them side by side\n",
    "    conf_group_site_nonlin = pd.concat([conf_group_site_squared,\n",
    "                                        conf_group_site_inormal,\n",
    "                                        conf_group_site_squared_inormal], axis=1)\n",
    "    conf_group_site_nonlin.index = site_index\n",
    "\n",
    "    # Catch any nans from fully empty columns (we'll drop these later)\n",
    "    conf_group_site_nonlin = conf_group_site_nonlin.fillna(0)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Deconfound for this site\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # Perform deconfounding\n",
    "    conf_nonlin_deconf = nets_deconfound_multiple(conf_group_site_nonlin,\n",
    "                                                  all_conf_site,\n",
    "                                                  mode='svd',\n",
    "                                                  blksize=blksize)\n",
    "    \n",
    "    # Reindex the dataframe to fill off-site values with zeros\n",
    "    conf_nonlin_deconf = conf_nonlin_deconf.reindex(conf_group.index).fillna(0)\n",
    "\n",
    "    # Drop any columns with only 5 values or less\n",
    "    # -------------------------------------------\n",
    "    na_columns = ((~conf_group_site.isna()).sum(axis=0) >= 5)\n",
    "\n",
    "    # Columns for squared\n",
    "    na_columns_squared = na_columns.copy()\n",
    "    na_columns_squared.index = [column + '_squared' for column in na_columns_squared.index]\n",
    "    \n",
    "    # Columns for inormal\n",
    "    na_columns_inormal = na_columns.copy()\n",
    "    na_columns_inormal.index = [column + '_inormal' for column in na_columns_inormal.index]\n",
    "    \n",
    "    # Columns for squared inormal\n",
    "    na_columns_squared_inormal = na_columns.copy()\n",
    "    na_columns_squared_inormal.index = [column + '_squared_inormal' for column in na_columns_squared_inormal.index]\n",
    "    \n",
    "    # Combine\n",
    "    na_columns = pd.concat((na_columns_squared,na_columns_inormal,na_columns_squared_inormal))\n",
    "    \n",
    "    # MARKER - replicating nets_unconfound_par threshold\n",
    "    #conf_nonlin_deconf = conf_nonlin_deconf.loc[:, (conf_nonlin_deconf.abs() >1e-8).sum(axis=0) >= 5]\n",
    "    conf_nonlin_deconf = conf_nonlin_deconf.loc[:, na_columns]\n",
    "    print('executed newer step')\n",
    "    \n",
    "    # Concatenate results\n",
    "    conf_nonlin = conf_nonlin.join(conf_nonlin_deconf, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78af12c-2c16-464f-9a5d-97ace87f1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_nonlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74176425-ef01-4b91-bf5b-1ef182431540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc44b51-fe48-4c0a-a39e-6127beff020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names directory\n",
    "names_dir = os.path.join(data_dir, '..', 'NAMES_confounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f650e-1b0f-45d3-b650-c9a48da9d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the IDPs and confounds we need\n",
    "IDPs = IDPs\n",
    "confounds = confounds_with_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145e978-bc82-463d-9528-475fbd5c8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the subject IDs\n",
    "sub_ids = IDPs.index\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Estimate the block size (number of subjects we want to allow in memory at\n",
    "# a given time).\n",
    "# -------------------------------------------------------------------------\n",
    "# Developer note: The below is only a rough estimate, but is fairly robust\n",
    "# as it is a little conservative. The rule of thumb is to take the maximum\n",
    "# amount of memory (MAXMEM) divide it by the number of subjects we have,\n",
    "# divide by 64 (as each observation is float64 at most) and then divide by\n",
    "# 8 (as we may want to make several copies of whatever we load in, but we\n",
    "# rarely make more than 8). The resulting number should be roughly the\n",
    "# number of columns of a dataframe we are able to load in at a time. This\n",
    "# doesn't need to be perfect as often python can handle more - it is just\n",
    "# a precaution, and does improve efficiency substantially.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Rough estimate of maximum memory (bytes)\n",
    "MAXMEM = 2**32\n",
    "\n",
    "# Get the number of subjects\n",
    "n_sub = len(sub_ids)\n",
    "\n",
    "# Block size computation\n",
    "blksize = int(MAXMEM/n_sub/8/64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2741bed-e16a-42b3-bc39-ddfdb740fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Deconfound IDPs\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Switch type to reduce transfer costs\n",
    "confounds = switch_type(confounds, out_type='filename')\n",
    "IDPs = switch_type(IDPs, out_type='filename')\n",
    "\n",
    "# Deconfound IDPs\n",
    "IDPs_deconf = nets_deconfound_multiple(IDPs, confounds, 'nets_svd', \n",
    "                                       blksize=blksize, coincident=False,\n",
    "                                       cluster_cfg=cluster_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5319f-0217-46f3-84d2-ee315f651491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Switch IDPs back\n",
    "IDPs = switch_type(IDPs, out_type='pandas') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eed04e-9400-45f0-94dc-f18bf14e9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get day fraction (time of day)\n",
    "day_fraction = nonIDPs[:,'TOD']\n",
    "\n",
    "# Normalise day fraction\n",
    "conf_acq_time_linear = nets_normalise(day_fraction)\n",
    "conf_acq_time_linear = conf_acq_time_linear.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e62736-52db-406f-9cd2-28147736e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_acq_time_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025e140-d578-4a6e-8a0d-506fba7d7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    conf_acq_time_linear = conf_acq_time_linear.sort_values(by='TOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c2178-ebc1-494f-b6c7-7d4c2d2b65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Get sorted indices\n",
    "    sub_ids_sorted = conf_acq_time_linear.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c0bef-9c8c-4f1f-b4f0-e9e9ca01730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort IDPs and IDPs_deconf based on sorted sub_ids\n",
    "IDPs_sorted = IDPs.loc[sub_ids_sorted,:]\n",
    "IDPs_deconf_sorted = IDPs_deconf.loc[sub_ids_sorted,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f796596-00c5-4f64-a4d8-bd6e78183bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "site_ids.index = sub_ids\n",
    "site_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d28c6-0cad-4e8e-884a-1bb4b374a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sort site ids\n",
    "site_ids_sorted = site_ids.loc[sub_ids_sorted,:]\n",
    "site_ids_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8ad16-abe2-4772-a042-70f833d1ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site_sorted = {}\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in (unique_site_ids + 1):\n",
    "\n",
    "    # Find the indices where all elements in a row of siteDATA match the current valueSite\n",
    "    # Note: This assumes siteDATA and siteValues have compatible shapes or values for comparison\n",
    "    indices = np.where((site_ids_sorted == site_id-1).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site_sorted[site_id] = indices\n",
    "\n",
    "# Delete the indices\n",
    "del indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6758a4-cbe7-44cc-bd1e-a16783da3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sites = len(inds_per_site_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b3ef4-7419-4243-b401-9f53a1aae0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigma value\n",
    "sigma = 0.1\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f8935-da6b-4852-9e77-9e2c509ddfd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loop through sites\n",
    "# for site_id in inds_per_site_sorted:\n",
    "\n",
    "#     print('Running site ', str(site_id))\n",
    "#     t1_total = time.time()\n",
    "\n",
    "#     # Get subjects for this site\n",
    "#     inds_site = inds_per_site_sorted[site_id]\n",
    "\n",
    "#     # Get the IDPs for this site\n",
    "#     IDPs_for_site = IDPs_deconf_sorted.iloc[inds_site,:]\n",
    "\n",
    "#     # Initialise smoothed IDPs for this site\n",
    "#     smoothed_IDPs_for_site = pd.DataFrame(np.zeros(IDPs_for_site.shape),\n",
    "#                                           index=IDPs_for_site.index,\n",
    "#                                           columns=IDPs_for_site.columns)\n",
    "\n",
    "#     # Get IDPs for site as numpy array\n",
    "#     IDPs_for_site = IDPs_for_site.values\n",
    "\n",
    "#     # Loop through subjects within site\n",
    "#     for j, sub_id in enumerate(inds_site):\n",
    "\n",
    "#         print('Iteration ', j, '/', len(inds_site))\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Get time delta\n",
    "#         timedelta = conf_acq_time_linear.iloc[inds_site[j],:]-conf_acq_time_linear.iloc[inds_site,:]\n",
    "#         t2 = time.time()\n",
    "#         print('marker1: ', t2-t1)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Get the gaussian kernel\n",
    "#         gauss_kernel = np.exp(-0.5*((timedelta/sigma)**2))\n",
    "#         t2 = time.time()\n",
    "#         print('marker2: ', t2-t1)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Handle any potential overflow\n",
    "#         gauss_kernel[gauss_kernel.abs()<1e-10]=0\n",
    "#         t2 = time.time()\n",
    "#         print('marker3: ', t2-t1)\n",
    "\n",
    "        \n",
    "#         t1 = time.time()\n",
    "#         # Get the numerator and denominator for smoothing\n",
    "#         numerator = np.nansum(IDPs_for_site*gauss_kernel.values,axis=0)\n",
    "#         t2 = time.time()\n",
    "#         print('marker3.5: ', t2-t1)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         denominator = np.sum((1*~np.isnan(IDPs_for_site))*gauss_kernel.values,axis=0)\n",
    "#         t2 = time.time()\n",
    "#         print('marker4: ', t2-t1)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Smoothed IDPs for site\n",
    "#         smoothed_IDPs_for_site.iloc[j, :] = numerator/denominator\n",
    "#         t2 = time.time()\n",
    "#         print('marker5: ', t2-t1)\n",
    "\n",
    "#         print(numerator/denominator)\n",
    "\n",
    "    \n",
    "#     t2_total = time.time()\n",
    "#     print('Done site ', str(site_id))\n",
    "#     print('Time elapsed: ', t2_total-t1_total)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa5aaf-75c3-41bc-b57e-38dc40217e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets.nets_smooth_multiple import nets_smooth_multiple\n",
    "from nets.nets_svd import nets_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa861d4b-b29b-48c0-b81f-577cee631a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time points per block, no 8 is included here as\n",
    "# we only ever construct the relevant matrix once in \n",
    "# nets_smooth_single (this is controlling the size of\n",
    "# the xeval*xdata matrix)\n",
    "blksize_time = int(MAXMEM/IDPs.shape[0]/64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135eed0b-c306-4b6c-97aa-32ed3e8961c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# Dict to store smoothed IDPs and pca results\n",
    "smoothed_IDPs_sorted_dict = {}\n",
    "principal_components_sorted_dict = {}\n",
    "esm_sorted_dict = {}\n",
    "\n",
    "# Loop through sites\n",
    "for site_id in inds_per_site_sorted:\n",
    "    \n",
    "    print('Smoothing confounds for site ', str(site_id))\n",
    "    t1_total = time.time()\n",
    "    \n",
    "    # Get subjects for this site\n",
    "    inds_site = inds_per_site_sorted[site_id]\n",
    "    \n",
    "    # Get the IDPs for this site\n",
    "    IDPs_for_site = IDPs_deconf_sorted.iloc[inds_site,:]\n",
    "    \n",
    "    # Get the acquisition times for this site\n",
    "    times_for_site = conf_acq_time_linear.iloc[inds_site,:]\n",
    "    \n",
    "    # Smooth the IDPs\n",
    "    smoothed_IDPs_for_site = nets_smooth_multiple(times_for_site, IDPs_for_site, sigma,\n",
    "                                                  blksize=blksize, blksize_time=blksize_time,\n",
    "                                                  cluster_cfg=cluster_cfg)\n",
    "\n",
    "    print('Confounds smoothed for site ', str(site_id))\n",
    "\n",
    "    # Compute svd of IDPs\n",
    "    principal_components_sorted, esm,_ = nets_svd(smoothed_IDPs_for_site.values)\n",
    "\n",
    "    # Save results\n",
    "    smoothed_IDPs_sorted_dict[site_id] = smoothed_IDPs_for_site\n",
    "    principal_components_sorted_dict[site_id] = principal_components_sorted\n",
    "    esm_sorted_dict[site_id] = esm\n",
    "\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6e112-d78a-4a6a-bf35-6a9818eae96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(IDPs_deconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5274b-be01-49b0-b557-e78b46b17fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets.nets_normalise import nets_normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed99ef5-e1a8-41e3-8f5d-c364a5648629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Estimating the number of temporal components by choosing a number\n",
    "# that explains at least 99% of the variance in the smoothed IDPs.\n",
    "num_temp_comp_sorted = {}\n",
    "conf_acq_time_dict = {}\n",
    " \n",
    "# Loop through sites\n",
    "for site_id in principal_components_sorted_dict:\n",
    "\n",
    "    # Maximum variance explained\n",
    "    max_ve = 0\n",
    "\n",
    "    # Get the principal components for this site\n",
    "    principal_components_site = principal_components_sorted_dict[site_id]\n",
    "\n",
    "    # Get the smoothed IDPs for this site\n",
    "    smoothed_IDPs_site = smoothed_IDPs_sorted_dict[site_id]\n",
    "\n",
    "    # Record index\n",
    "    site_index = smoothed_IDPs_site.index\n",
    "    \n",
    "    # Current number of principal components that we have considered\n",
    "    n_current = 1\n",
    "\n",
    "    # Get columns of rows with all non-nan values\n",
    "    non_nan_rows = ~smoothed_IDPs_site.isna().any(axis=1)\n",
    "\n",
    "    # Filter principal components and smoothed IDPs row wise\n",
    "    principal_components_site = principal_components_site[non_nan_rows.values,:]\n",
    "    smoothed_IDPs_site = smoothed_IDPs_site[non_nan_rows].values\n",
    "    \n",
    "    # Loop through principal components until we have 99% variance explained\n",
    "    while max_ve < 99:\n",
    "\n",
    "        # Get n_current principal components\n",
    "        current_pcs = principal_components_site[:,:n_current]\n",
    "            \n",
    "        # Compute variance explained in smoothed_IDPs_site by current_pcs\n",
    "        current_pcs_pinv = np.linalg.pinv(current_pcs)\n",
    "    \n",
    "        # Compute projection\n",
    "        proj = current_pcs @ (current_pcs_pinv @ smoothed_IDPs_site)\n",
    "        \n",
    "        # Compute variance explained\n",
    "        numerator = 100 * np.sum(proj.flatten() ** 2)\n",
    "        denominator = np.sum(smoothed_IDPs_site.flatten() ** 2)\n",
    "        max_ve = numerator / denominator\n",
    "\n",
    "        print(n_current, max_ve)\n",
    "        \n",
    "        # Check if max_ve is greater than 99\n",
    "        if max_ve < 99:\n",
    "\n",
    "            # Increment counter\n",
    "            n_current = n_current + 1\n",
    "\n",
    "    # Save number of components\n",
    "    num_temp_comp_sorted[site_id] = n_current\n",
    "\n",
    "    # Save new array\n",
    "    principal_components_sorted_dict[site_id] = pd.DataFrame(principal_components_site[:,:n_current],\n",
    "                                                             index=site_index)\n",
    "    conf_acq_time_dict[site_id] = nets_normalise(principal_components_sorted_dict[site_id]).fillna(0)\n",
    "\n",
    "    print('Estimated number of components for site ', site_id, ': ', n_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112baa6-eb0d-4148-8812-1ef6dc135eda",
   "metadata": {},
   "source": [
    "*Matlab had 19 for site 1, 20 for site 2, 22 for site 3 and 21 for site 4 (82 total)*\n",
    "\n",
    "*Python gives 18 for site 1, 19 for site 2, 21 for site 3 and 20 for site 4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c1d14-bb13-4c56-87bc-90f9ceae8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_id in smoothed_IDPs_sorted_dict:\n",
    "\n",
    "    print(principal_components_sorted_dict[site_id].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fcd0a-b8e8-4bd1-8384-291bbbde1a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d6ce0-a154-43cf-bdeb-e46c3271c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct column names for temporal components\n",
    "tc_colnames = []\n",
    "\n",
    "# Loop through sites constructing colnames and converting principal components\n",
    "for site_id in principal_components_sorted_dict:\n",
    "\n",
    "    # Site columnnames \n",
    "    tc_colnames_site = ['ACQT_Site_' + str(site_id) + '__' + str(pc_id) for pc_id in range(1,num_temp_comp_sorted[site_id]+1)]\n",
    "\n",
    "    # Replace header on site specific dataframes\n",
    "    principal_components_sorted_dict[site_id].columns = tc_colnames_site\n",
    "\n",
    "    # Update running column names\n",
    "    tc_colnames = tc_colnames + tc_colnames_site\n",
    "    \n",
    "# Number of estimated components in total\n",
    "num_temp_comp_sorted_total = 0\n",
    "\n",
    "# Sum values over sites\n",
    "for site_id in num_temp_comp_sorted:\n",
    "    num_temp_comp_sorted_total = num_temp_comp_sorted_total + num_temp_comp_sorted[site_id]\n",
    "    \n",
    "# Reconstruct principal components confound dataframe\n",
    "conf_acq_time = pd.DataFrame(np.zeros((n_sub,num_temp_comp_sorted_total)),\n",
    "                             index = sub_ids_sorted,\n",
    "                             columns = tc_colnames)\n",
    "\n",
    "# Loop through sites constructing colnames and converting principal components\n",
    "for site_id in principal_components_sorted_dict:\n",
    "\n",
    "    # Add in temporal components sorted\n",
    "    conf_acq_time.update(principal_components_sorted_dict[site_id])\n",
    "\n",
    "# Convert the indexing back to the original order\n",
    "conf_acq_time = conf_acq_time.loc[sub_ids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f98de-6f8d-4cb0-bf4a-cc440965137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faba78e-d43a-40dd-bba9-971a770717d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta = conf_acq_time_linear.iloc[inds_site[j],:]-conf_acq_time_linear.iloc[inds_site,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0886a6e-027f-4876-897b-9a293f09d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_kernel = np.exp(-0.5*((timedelta/sigma)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1b76b-7ebf-49db-87d2-014b4a1e5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a pandas DataFrame 'df' with a single column\n",
    "data = gauss_kernel.iloc[:, 0]  # Get the single column as a pandas Series\n",
    "\n",
    "# Create a range of indices from 1 to the length of the data\n",
    "indices = range(1, len(data) + 1)\n",
    "\n",
    "# Plot the data against the indices\n",
    "# plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "# plt.plot(indices, data)\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Dataframe Column Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d9f8b-d792-4f12-b381-67e9ca13f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle any potential overflow\n",
    "gauss_kernel[gauss_kernel.abs()<1e-10]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cac34b-7ed4-406e-863d-8c3364bca122",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDPs_for_site = IDPs.iloc[inds_site,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6545958-bace-4fa5-97d7-5298ebbcb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(IDPs_for_site.values*gauss_kernel.values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e69d5-8cf2-47d8-bdee-9d6e1d1fb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerator and denominator for smoothing\n",
    "numerator = np.nansum(IDPs_for_site.values*gauss_kernel.values,axis=0)\n",
    "denominator = np.sum((1*~np.isnan(IDPs_for_site.values))*gauss_kernel.values,axis=0)\n",
    "\n",
    "# Get the smoothed IDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3535d-f918-490b-aec9-f63f9dd5ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf95f5-a92a-422f-b636-a1dad49d90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099d17c-a9df-4f4d-b865-db344b23f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was adapted from the below answer on stack overflow\n",
    "# https://stackoverflow.com/questions/24143320/gaussian-sum-filter-for-irregular-spaced-points\n",
    "\n",
    "def gaussian_sum_smooth(xdata, ydata, xeval, sigma, null_thresh=0.6):\n",
    "    \"\"\"Apply gaussian sum filter to data.\n",
    "    \n",
    "    xdata, ydata : array\n",
    "        Arrays of x- and y-coordinates of data. \n",
    "        Must be 1d and have the same length.\n",
    "    \n",
    "    xeval : array\n",
    "        Array of x-coordinates at which to evaluate the smoothed result\n",
    "    \n",
    "    sigma : float\n",
    "        Standard deviation of the Gaussian to apply to each data point\n",
    "        Larger values yield a smoother curve.\n",
    "    \n",
    "    null_thresh : float\n",
    "        For evaluation points far from data points, the estimate will be\n",
    "        based on very little data. If the total weight is below this threshold,\n",
    "        return np.nan at this location. Zero means always return an estimate.\n",
    "        The default of 0.6 corresponds to approximately one sigma away \n",
    "        from the nearest datapoint.\n",
    "    \"\"\"\n",
    "    # Distance between every combination of xdata and xeval\n",
    "    # each row corresponds to a value in xeval\n",
    "    # each col corresponds to a value in xdata\n",
    "    delta_x = xeval[:, None] - xdata\n",
    "\n",
    "    # Calculate weight of every value in delta_x using Gaussian\n",
    "    # Maximum weight is 1.0 where delta_x is 0\n",
    "    weights = np.exp(-0.5 * ((delta_x / sigma) ** 2))\n",
    "\n",
    "    # Multiply each weight by every data point, and sum over data points\n",
    "    smoothed = np.dot(weights, ydata)\n",
    "\n",
    "    # Nullify the result when the total weight is below threshold\n",
    "    # This happens at evaluation points far from any data\n",
    "    # 1-sigma away from a data point has a weight of ~0.6\n",
    "    nan_mask = weights.sum(1) < null_thresh\n",
    "    smoothed[nan_mask] = np.nan\n",
    "    \n",
    "    # Normalize by dividing by the total weight at each evaluation point\n",
    "    # Nullification above avoids divide by zero warning shere\n",
    "    smoothed = smoothed / weights.sum(1)\n",
    "\n",
    "    # Return result    \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578c4e2-2a84-495f-94c3-50ffe7c49321",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = conf_acq_time_linear.iloc[inds_site,:].values\n",
    "\n",
    "ydata = IDPs_for_site[:,1:10]\n",
    "\n",
    "# mask = ~np.isnan(ydata)\n",
    "\n",
    "# xdata = xdata[mask]\n",
    "xeval = np.array(xdata)\n",
    "# ydata = ydata[mask]\n",
    "\n",
    "null_thresh = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b024b-39ef-491e-b200-bf102bd49640",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.shape, xeval.shape, ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfda9f-b979-420c-b6f0-3ae03f4d0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "smoothed_IDPs=gaussian_sum_smooth(xdata, ydata, xeval, sigma, null_thresh=0.6)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74a986-7e2e-42fe-834e-dfb658181670",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_IDPs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a2ba0-56ed-47bb-b031-82bf720ee461",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.flatten().shape, ydata[:,1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955eef4-7eee-414b-9c8e-031579c0e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(smoothed_IDPs).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6c2c9-a338-4afa-a8a4-8b2a6301a234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de9098-144f-4ac4-a1e6-f0fd5017add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was adapted from the below answer on stack overflow\n",
    "# https://stackoverflow.com/questions/24143320/gaussian-sum-filter-for-irregular-spaced-points\n",
    "\n",
    "def nets_smooth(xdata, ydata, xeval, sigma, null_thresh=0.6):\n",
    "    \"\"\"Apply gaussian sum filter to data.\n",
    "    \n",
    "    xdata, ydata : array\n",
    "        Arrays of x- and y-coordinates of data. \n",
    "        xdata be have only one dimension > 1 and have the same height as ydata.\n",
    "        ydata must be two dimensional (n_obs x n_variables)\n",
    "    \n",
    "    xeval : array\n",
    "        Array of x-coordinates at which to evaluate the smoothed result\n",
    "    \n",
    "    sigma : float\n",
    "        Standard deviation of the Gaussian to apply to each data point\n",
    "        Larger values yield a smoother curve.\n",
    "    \n",
    "    null_thresh : float\n",
    "        For evaluation points far from data points, the estimate will be\n",
    "        based on very little data. If the total weight is below this threshold,\n",
    "        return np.nan at this location. Zero means always return an estimate.\n",
    "        The default of 0.6 corresponds to approximately one sigma away \n",
    "        from the nearest datapoint.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten xdata and xeval\n",
    "    xdata = xdata.flatten()\n",
    "    xeval = xeval.flatten()\n",
    "    \n",
    "    # Distance between every combination of xdata and xeval\n",
    "    # each row corresponds to a value in xeval\n",
    "    # each col corresponds to a value in xdata\n",
    "    delta_x = xeval[:, None] - xdata\n",
    "    \n",
    "    # Calculate weight of every value in delta_x using Gaussian\n",
    "    # Maximum weight is 1.0 where delta_x is 0\n",
    "    weights = np.exp(-0.5 * ((delta_x / sigma) ** 2))\n",
    "    \n",
    "    # Temporarily remove zeros from ydata\n",
    "    ydata_wo_nans = np.array(ydata)\n",
    "    ydata_wo_nans[np.isnan(ydata)]=0\n",
    "    \n",
    "    # Multiply each weight by every data point, and sum over data points\n",
    "    smoothed = weights @ ydata_wo_nans\n",
    "    \n",
    "    # Nullify the result when the total weight is below threshold\n",
    "    # This happens at evaluation points far from any data\n",
    "    # 1-sigma away from a data point has a weight of ~0.6\n",
    "    nan_mask = weights.sum(1) < null_thresh\n",
    "    smoothed[nan_mask] = np.nan\n",
    "    \n",
    "    # Normalize by dividing by the total weight at each evaluation point\n",
    "    # Nullification above avoids divide by zero warning shere\n",
    "    for k in np.arange(smoothed.shape[1]):\n",
    "        \n",
    "        # Get nan mask\n",
    "        non_nan_mask = ~np.isnan(ydata[:,k])\n",
    "        \n",
    "        # Get smoothed\n",
    "        smoothed[:,k] = smoothed[:,k] / weights[:,non_nan_mask].sum(1)\n",
    "\n",
    "    # Return result\n",
    "    return(smoothed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185acfdb-4aaa-4747-926e-cbdc0c4a2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308dc3d-f36a-4f9d-80cb-49d3ec6026d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed.shape\n",
    "for k in np.arange(smoothed.shape[1]):\n",
    "    \n",
    "    # Get nan mask\n",
    "    non_nan_mask = ~np.isnan(ydata[:,k])\n",
    "    \n",
    "    # Get smoothed\n",
    "    print(smoothed[:,k] / weights[:,non_nan_mask].sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612eb756-9730-486f-ae85-c9e1265c474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[ 0.23876152  0.30856143 -0.19740716 ... -0.00821319 -0.25510581\n",
    " -0.14734484]\n",
    "marker5:  0.000518798828125\n",
    "[ 0.23761886  0.30677943 -0.19650863 ... -0.00709767 -0.25337046\n",
    " -0.14570244]\n",
    "[ 0.23742288  0.30647581 -0.19635486 ... -0.00690842 -0.25307346\n",
    " -0.14542344]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617df6e-8d48-4c3d-a489-bc48f7d98fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_mask = ~np.isnan(ydata[:,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17333fc-d75b-4002-b03f-e32f6475ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(non_nan_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6fdb8-ed78-4529-921c-bc8fcce0854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape, weights.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755104a-eff8-4093-a668-f1de35fca2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[non_nan_mask,:].shape, weights[non_nan_mask,:].sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035a04c-411b-4676-a72c-f058ffe76152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012db30-7bbd-49db-a3c1-fef14607eb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fa2e7-1871-4ef2-bd5d-f4dd5a554941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def my_log2(message, mode='a',filename=None):\n",
    "    \"\"\"\n",
    "    Write a message to an HTML file with a header, basic formatting, and styling.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to be written.\n",
    "        mode (str, optional): The mode to open the file with. 'a' for append, 'r' for replace current line. Defaults to 'a'.\n",
    "        filename (str, optional): The name of the HTML file. Defaults to None (no output).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if file is in use\n",
    "    fileLocked = True\n",
    "    while fileLocked:\n",
    "        try:\n",
    "            # Create lock file, so other jobs know we are writing to this file\n",
    "            f = os.open(filename + \".lock\", os.O_CREAT|os.O_EXCL|os.O_RDWR)\n",
    "            fileLocked = False\n",
    "        except FileExistsError:\n",
    "            fileLocked = True\n",
    "            \n",
    "    if filename is not None:\n",
    "        try:\n",
    "            # Read file line\n",
    "            with open(filename, 'r+', encoding='utf-8') as file:\n",
    "                lines = [line for line in file]\n",
    "\n",
    "            # Writing filelines\n",
    "            with open(filename, 'w+', encoding='utf-8') as file:\n",
    "                if not lines:  # If the file is empty, create the HTML structure\n",
    "                    print('here1')\n",
    "                    file.write('<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Confounds Log</title>\\n')\n",
    "                    file.write('<style>\\nbody { font-family: Arial, sans-serif; margin: 20px; background-color: #e6f0ff; }\\n')\n",
    "                    file.write('h1 { color: #333; position: sticky; top: 0; background-color: #e6f0ff; padding: 10px; }\\n')\n",
    "                    file.write('hr { border: none; border-top: 1px solid #ccc; margin: 10px 0; }\\n</style>\\n</head>\\n<body>\\n<h1>Confounds Log</h1>\\n<hr>\\n')\n",
    "                    file.write('<p>' + message + '</p>\\n')\n",
    "                    file.write('</body>\\n')\n",
    "                    file.write('</html>')\n",
    "                else:\n",
    "                    if mode == 'a':\n",
    "                        print('here2')\n",
    "                        lines.append('</html>')\n",
    "                        lines[-3] = '<p>' + message + '</p>\\n'\n",
    "                        lines[-2] = '</body>\\n'\n",
    "                    elif mode == 'r':\n",
    "                        print('here5')\n",
    "                        print(lines)\n",
    "                        lines[-3] = '<p>' + message + '</p>\\n'\n",
    "                        lines[-2] = '</body>\\n'\n",
    "                        lines[-1] = '</html>'\n",
    "                        print(lines)\n",
    "                    file.writelines(lines)\n",
    "        except FileNotFoundError:\n",
    "            print('here3')\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                print('here4')\n",
    "                file.write('<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Confounds Log</title>\\n')\n",
    "                file.write('<style>\\nbody { font-family: Arial, sans-serif; margin: 20px; background-color: #e6f0ff; }\\n')\n",
    "                file.write('h1 { color: #333; position: sticky; top: 0; background-color: #e6f0ff; padding: 10px; }\\n')\n",
    "                file.write('hr { border: none; border-top: 1px solid #ccc; margin: 10px 0; }\\n</style>\\n</head>\\n<body>\\n<h1>Confounds Log</h1>\\n<hr>\\n')\n",
    "                file.write('<p>' + message + '</p>\\n')\n",
    "                file.write('</body>\\n')\n",
    "                file.write('</html>')\n",
    "\n",
    "    # Release the file lock\n",
    "    os.remove(filename + \".lock\")\n",
    "    os.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59488a7c-338e-41f2-b037-5b9f8da09658",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['a','b']\n",
    "x.append('c')\n",
    "x[-2]='e'\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4ab3e-8a45-446d-a91c-b7b5244244f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from nets.nets_load_match import nets_load_match\n",
    "from nets.nets_inverse_normal import nets_inverse_normal\n",
    "\n",
    "from preproc.datenum import datenum\n",
    "from preproc.days_in_year import days_in_year\n",
    "\n",
    "from memmap.MemoryMappedDF import MemoryMappedDF\n",
    "\n",
    "from logio.my_log import my_log\n",
    "from logio.loading import ascii_loading_bar\n",
    "\n",
    "logfile = '/well/nichols/users/inf852/confounds/log.html'\n",
    "\n",
    "# Update log\n",
    "my_log2(str(datetime.now()) +': Stage 1: Generating Initial Variables.', mode='a', filename=logfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e2b31-8f79-415f-b9a4-32a3267f2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '/well/nichols/users/inf852/confounds/log.html'\n",
    "# mode='r'\n",
    "# message=str(datetime.now()) +': Quartile normalisation complete.'\n",
    "# with open(filename, 'r+', encoding='utf-8') as file:\n",
    "#     lines = [line.rstrip() for line in file]\n",
    "\n",
    "# with open(filename, 'w+', encoding='utf-8') as file:\n",
    "#     if not lines:  # If the file is empty, create the HTML structure\n",
    "#         print('here')\n",
    "#     else:\n",
    "#         if mode == 'a':\n",
    "#             print('here2')\n",
    "#         elif mode == 'r':\n",
    "#             print(lines)\n",
    "#             lines[-3] = f'<p>{message}</p>\\n'\n",
    "#             lines[-2] = '</body>\\n'\n",
    "#             lines[-1] = '</html>'\n",
    "\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7112b30-3889-4fa9-bcc2-52086efca077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f189e-0c0a-4780-8155-d3b88045514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update log\n",
    "my_log2(str(datetime.now()) +': Loaded initial variables.', mode='a', filename=logfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da590625-e706-4200-a336-fafd7555ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_log2(str(datetime.now()) +': Performing quartile normalisation of IDPs...', mode='a', filename=logfile)\n",
    "# works up to here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5708edd-a591-4641-b373-acbe61b7bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update log\n",
    "my_log2(str(datetime.now()) +': Quartile normalisation complete.', mode='r', filename=logfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf29d60-83b0-4e43-bc72-bb48a20f8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_log2(str(datetime.now()) +': Loading miscellaneous variables...', mode='a', filename=logfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89c29a-2a1d-4e8e-be03-43abe3ef54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update log\n",
    "my_log2(str(datetime.now()) +': Loaded miscellaneous variables and sorted.', mode='r', filename=logfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7175f6-607b-49f8-87b0-7384f45d48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_log2(str(datetime.now()) +': Saving results...', mode='a', filename=logfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fafc2c-dc39-4ba8-9cbf-d956cec1d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update\n",
    "my_log2(str(datetime.now()) +': Results saved.', mode='r', filename=logfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d0347-d239-44b0-967f-92f3d91c2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_log2(str(datetime.now()) +': Stage 1: Complete.', mode='a', filename=logfile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682459d-e412-411e-af6f-adc408bce355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
