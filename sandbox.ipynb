{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from lib.script_01_00 import generate_initial_variables\n",
    "from lib.script_01_01 import generate_raw_confounds\n",
    "from lib.script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from src.nets import nets_load_match, nets_inverse_normal, nets_normalise, nets_demean, nets_deconfound\n",
    "from src.duplicate import duplicate_categorical, duplicate_demedian_norm_by_site\n",
    "from src.preproc import datenum, days_in_year\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "IDPs, nonIDPs, misc = generate_initial_variables(data_dir, out_dir)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)\n",
    "# Get the subject IDs\n",
    "sub_ids = IDPs.index\n",
    "\n",
    "# Generate raw confounds\n",
    "t1 = time.time()\n",
    "confounds = generate_raw_confounds(data_dir, sub_ids)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78901c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import uuid\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Function to read a MemoryMappedDF instance from a file\n",
    "# ------------------------------------------------------------------------------\n",
    "def read_memmap(filename):\n",
    "    \n",
    "    # Read in self_copy and create a new instance\n",
    "    with open(filename, 'rb') as f:\n",
    "        self_copy = pickle.load(f)\n",
    "        \n",
    "    # Create a new hash\n",
    "    self_copy.hash = str(uuid.uuid4())\n",
    "        \n",
    "    print(type(self_copy))\n",
    "    \n",
    "    # Create empty memmap\n",
    "    memmap_df = MemoryMappedDF(pd.DataFrame())\n",
    "    \n",
    "    # We now make a new copy of the memory map files behind the scenes so we \n",
    "    # don't delete the original files on close\n",
    "    for dtype, memmap_file in self_copy.memory_maps.items():\n",
    "        \n",
    "        # Create new memory map filename using new hash\n",
    "        filename = os.path.join(self_copy.directory, f\"{self_copy.hash}_{dtype}.dat\")\n",
    "        \n",
    "        # Create new memory map\n",
    "        memmap_df.memory_maps[dtype]= np.memmap(filename, \n",
    "                                                dtype=self_copy.data_types[dtype], \n",
    "                                                mode='w+', shape=self_copy.shape)\n",
    "        \n",
    "        # Open old memory map\n",
    "        prev_memmap = np.memmap(memmap_file, dtype=dtype, mode='c', shape=self_copy.shape)\n",
    "        \n",
    "        # Copy the existing memory map file over \n",
    "        memmap_df.memory_maps[dtype][:] = prev_memmap[:]\n",
    "        \n",
    "        # Delete the previous memory map\n",
    "        del prev_memmap\n",
    "        \n",
    "        \n",
    "    print(self_copy.memory_maps)\n",
    "    \n",
    "    # Update with new info\n",
    "    memmap_df.__dict__.update(self_copy)\n",
    "    \n",
    "    \n",
    "    # Convert self.memory_maps back to a dictionary of memmaps\n",
    "    memmap_df.memory_maps = {\n",
    "        dtype: np.memmap(filename, dtype=memmap_df.data_types[dtype], \n",
    "                         mode='r', shape=memmap_df.shape)\n",
    "        for dtype, filename in memmap_df.memory_maps.items()\n",
    "    }\n",
    "    \n",
    "    # Return the new instance\n",
    "    return(memmap_df)\n",
    "\n",
    "confounds_tmp = read_memmap(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "confounds_tmp = read_memmap(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_tmp.memory_maps['float32'].filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_tmp.memory_maps['float32'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz')\n",
    "\n",
    "# Read in self_copy and create a new instance\n",
    "with open(filename, 'rb') as f:\n",
    "    self_copy = pickle.load(f)\n",
    "\n",
    "# Create empty memmap\n",
    "memmap_df = MemoryMappedDF(pd.DataFrame())\n",
    "\n",
    "# Update with new info\n",
    "memmap_df.__dict__ = self_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b26f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_copy.memory_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nan_patterns_y)):\n",
    "    \n",
    "    print(y.shape[0]-sum(tmp[i]['pattern']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88127429",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise i\n",
    "i = 0\n",
    "\n",
    "j = 0\n",
    "\n",
    "# Set maximum memory\n",
    "MAXMEM = 2**34 \n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Loop through all unique nan patterns in y\n",
    "while i < len(nan_patterns_y):\n",
    "    \n",
    "    j = j+1\n",
    "    \n",
    "    t1_tmp = time.time()\n",
    "    \n",
    "    # Work out the next largest block we have\n",
    "    num_non_zero = y.shape[0] - sum(tmp[i]['pattern'])\n",
    "    \n",
    "    # Get the memory required for one matrix\n",
    "    mem_vol = sys.getsizeof(np.zeros((num_non_zero, conf2.shape[1]),dtype='uint64'))\n",
    "\n",
    "    # Work out the number of blocks of this size that we can\n",
    "    # perform computation for concurrently\n",
    "    blksize = int(np.floor(MAXMEM/(8*mem_vol)))\n",
    "    \n",
    "    # Check if we've hit the end of the list\n",
    "    if len(nan_patterns_y)-i < blksize:\n",
    "        \n",
    "        # Set blocksize the most we can\n",
    "        blksize = len(nan_patterns_y)-i\n",
    "    \n",
    "    # Create numpy array of zeros to store conf for this block\n",
    "    conf_block = np.zeros((blksize, num_non_zero, conf2.shape[1]))\n",
    "    \n",
    "    # Create numpy array of zeros to store y for this block\n",
    "    y_block = np.zeros((blksize, num_non_zero, 1)) # MARKER 1 needs to be replaced\n",
    "    \n",
    "    print(i, conf_block.shape)\n",
    "    \n",
    "    # Add the confounds to the block\n",
    "    for k in range(blksize):\n",
    "    \n",
    "        # Get the pattern\n",
    "        non_nan = ~np.array(tmp[i+k]['pattern'],dtype=bool)\n",
    "\n",
    "        # Number of non na values\n",
    "        n_non_na = int(np.sum(1*non_nan))\n",
    "        \n",
    "        # Check if we have at least 5 non-nan values\n",
    "        if n_non_na > 5:\n",
    "\n",
    "            # Subset y to the appropriate columns\n",
    "            cols = tmp[i+k]['columns']\n",
    "            \n",
    "            # Get the y's we're interested in\n",
    "            y_current = y[tmp[i+k]['columns']]\n",
    "            \n",
    "            # Get the dimensions of y\n",
    "            y1 = y_current.shape[1]\n",
    "            x1 = conf2.shape[1]\n",
    "\n",
    "            # Subset y and conf to the appropriate rows\n",
    "            y_block[k,:n_non_na,:y1] = y_current[non_nan]\n",
    "            conf_block[k,:n_non_na,:x1] = conf2[non_nan]\n",
    "            \n",
    "\n",
    "    # Multiply the left-singular values which contribute to the rank of conf\n",
    "    # by the corresponding singular values to rank reduce conf\n",
    "    U, D, Vt = np.linalg.svd(conf_block, full_matrices=False)\n",
    "\n",
    "    # Get the rank of the matrix\n",
    "    for k in range(blksize):\n",
    "\n",
    "        rank = np.sum(D[k,:] > 1e-10)\n",
    "\n",
    "        # Rank reduce U\n",
    "        U[k, :, rank:] = 0\n",
    "\n",
    "    # Compute U'Y\n",
    "    UtY = U.transpose((0,2,1)) @ y_block\n",
    "\n",
    "    # Get deconfounding variable predicted values to regress out\n",
    "    deconf_pred = U @ UtY# pd.DataFrame(U @ UtY)\n",
    "#     deconf_pred.index = y_current.index\n",
    "    \n",
    "    # Update i\n",
    "    i = i + blksize\n",
    "    \n",
    "    print(time.time()-t1_tmp)\n",
    "    \n",
    "print(j,i)\n",
    "print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[0] - sum(tmp[i]['pattern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21035550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the pattern\n",
    "non_nan = ~np.array(nan_patterns_y[i]['pattern'],dtype=bool)\n",
    "\n",
    "# Number of non na values\n",
    "n_non_na = int(np.sum(1*non_nan))\n",
    "n_non_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_array = np.random.rand(521, 509)\n",
    "\n",
    "# Wrap this array with Dask, specifying the desired chunk size (100x100 in this case).\n",
    "conf2_dask = da.from_array(conf2, chunks=(100, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ecc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nantools.all_non_nan_inds import all_non_nan_inds\n",
    "from src.nantools.create_nan_patterns import create_nan_patterns\n",
    "\n",
    "# Get the indices for non-nan rows in conf\n",
    "conf_non_nan_inds = all_non_nan_inds(conf)\n",
    "\n",
    "# Reduce conf and y down, ignoring the nan rows for conf\n",
    "conf = conf[conf_non_nan_inds]\n",
    "y = y[conf_non_nan_inds]\n",
    "\n",
    "# We now need to get the nan-patterns for y\n",
    "nan_patterns_y = create_nan_patterns(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values\n",
    "conf = conf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "y_da = da.from_array(y, chunks=(1000, None))\n",
    "conf_da = da.from_array(conf, chunks=(1000, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MARKER\n",
    "t1 = time.time()\n",
    "\n",
    "# Create array to store result\n",
    "pred_y_da = da.zeros(y_da.shape, chunks=(1000, None))\n",
    "    \n",
    "# Loop through all unique nan patterns in y\n",
    "for i in nan_patterns_y:\n",
    "\n",
    "    print('Deconfounding: ', i+1, '/', len(nan_patterns_y))\n",
    "    print('Time elapsed: ', time.time()-t1)\n",
    "    print('Predicted time: ', len(nan_patterns_y)*(time.time()-t1)/(i+1))\n",
    "\n",
    "    # Get the pattern\n",
    "    non_nan = ~np.array(nan_patterns_y[i]['pattern'],dtype=bool)\n",
    "\n",
    "    # Check if we have at least 5 non-nan values\n",
    "    if np.sum(1*non_nan) > 5:\n",
    "        \n",
    "        # Get the y's we're interested in\n",
    "        y_current_da = y_da[:,i]\n",
    "        \n",
    "        # Subset y and conf to the appropriate rows\n",
    "        y_current_da = y_current_da[non_nan]\n",
    "        conf_current_da = conf_da[non_nan]\n",
    "        \n",
    "        # Increase the precision on conf_current (just in case overflow\n",
    "        # becomes a risk)\n",
    "        conf_current_da = conf_current_da.astype(np.float64)\n",
    "        \n",
    "        # Multiply the left-singular values which contribute to the rank of conf\n",
    "        # by the corresponding singular values to rank reduce conf\n",
    "        U_da, S_da, Vt_da = da.linalg.svd(conf_current_da)\n",
    "        \n",
    "        # Rank reduce U\n",
    "        U_da = U_da*(S_da > 1e-10)\n",
    "        \n",
    "        # Get deconfounding variable predicted values to regress out\n",
    "        pred_y_current_da = U_da @ (U_da.T @ y_current_da)\n",
    "        \n",
    "        # Fill in pred_y_da\n",
    "        pred_y_da[non_nan,i] = pred_y_current_da\n",
    "    \n",
    "\n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0297525",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "pred_y_da.compute()\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18adac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([1,1,3,2,1,0,0,0,0])\n",
    "\n",
    "tmp = 1*(s > 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(30, s.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x*(s > 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7524fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t1 = time.time()\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 1: Outside loop\n",
    "# -----------------------------------------------------------------------------\n",
    "# Multiply the left-singular values which contribute to the rank of conf\n",
    "# by the corresponding singular values to rank reduce conf\n",
    "U, D, Vt = np.linalg.svd(conf2, full_matrices=False)\n",
    "\n",
    "# Get the rank of the matrix\n",
    "rank = np.sum(D > 1e-10)\n",
    "\n",
    "# Rank reduce U\n",
    "U = U[:, :rank]\n",
    "\n",
    "# Compute U'Y\n",
    "UtY = U.T @ My\n",
    "\n",
    "# Get deconfounding variable predicted values to regress out\n",
    "deconf_pred2 = U @ UtY\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 2: Outside loop (numba)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Multiply the left-singular values which contribute to the rank of conf\n",
    "# by the corresponding singular values to rank reduce conf\n",
    "deconf_pred = numba_predict(conf2, My)\n",
    "\n",
    "t3 = time.time()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Step 3: Outside loop (numba compiled)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Multiply the left-singular values which contribute to the rank of conf\n",
    "# by the corresponding singular values to rank reduce conf\n",
    "for i in np.arange(20):\n",
    "    \n",
    "    deconf_pred3 = numba_predict(conf2, My)\n",
    "\n",
    "t4 = time.time()\n",
    "\n",
    "print('Step 1 time: ', t2-t1)\n",
    "print('Step 2 time: ', t3-t2)\n",
    "print('Step 3 time: ', (t4-t3)/20)\n",
    "print('Total: ', t4-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4816f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numba import njit\n",
    "import numba as nb\n",
    "\n",
    "def non_numba_predict(X,Y):\n",
    "    \n",
    "    # Multiply the left-singular values which contribute to the rank of conf\n",
    "    # by the corresponding singular values to rank reduce conf\n",
    "    U, D, _ = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "    # Fix ordering of U (needed for fast numba compile)\n",
    "    U=U[::1]\n",
    "    \n",
    "    # Get the rank of the matrix\n",
    "    rank = np.sum(D > 1e-10)\n",
    "\n",
    "    # Rank reduce U\n",
    "    U = U[:, :rank]\n",
    "    \n",
    "    # Compute U'Y\n",
    "    UtY = U.T.copy() @ Y\n",
    "    \n",
    "    # Get deconfounding variable predicted values to regress out\n",
    "    pred = U @ UtY\n",
    "    \n",
    "    return(pred)\n",
    "\n",
    "@njit(nb.float64[:,:](nb.float64[:,:],nb.float64[:,:]),nopython=True)\n",
    "def numba_predict(X,Y):\n",
    "    \n",
    "    # Multiply the left-singular values which contribute to the rank of conf\n",
    "    # by the corresponding singular values to rank reduce conf\n",
    "    U, D, _ = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "    # Fix ordering of U (needed for fast numba compile)\n",
    "    U=U[::1]\n",
    "    \n",
    "    # Get the rank of the matrix\n",
    "    rank = np.sum(D > 1e-10)\n",
    "\n",
    "    # Rank reduce U\n",
    "    U = U[:, :rank]\n",
    "    \n",
    "    # Compute U'Y\n",
    "    UtY = U.T @ Y\n",
    "    \n",
    "    # Get deconfounding variable predicted values to regress out\n",
    "    pred = U @ UtY\n",
    "    \n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27785c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = 0\n",
    "for i in np.arange(5):\n",
    "    \n",
    "    t1_running = time.time()\n",
    "    pred = non_numba_predict(conf)\n",
    "    t2_running = time.time()\n",
    "    \n",
    "    t_total = t_total + t2_running-t1_running\n",
    "\n",
    "print(t_total/5)\n",
    "\n",
    "t1 = time.time()\n",
    "pred = numba_predict(conf)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)\n",
    "\n",
    "t_total = 0\n",
    "for i in np.arange(5):\n",
    "    \n",
    "    t1_running = time.time()\n",
    "    pred = numba_predict(conf)\n",
    "    t2_running = time.time()\n",
    "\n",
    "    t_total = t_total + t2_running-t1_running\n",
    "\n",
    "print(t_total/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X: ',nb.typeof(X))\n",
    "print('Y: ',nb.typeof(Y))\n",
    "print('U1: ',nb.typeof(U1))\n",
    "print('D: ',nb.typeof(D))\n",
    "print('U2: ',nb.typeof(U2))\n",
    "print('rank: ',nb.typeof(rank))\n",
    "print('U: ',nb.typeof(U))\n",
    "print('UtY: ',nb.typeof(UtY))\n",
    "print('pred: ',nb.typeof(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fec8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.typeof(np.transpose(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit, njit\n",
    "import numba as nb\n",
    "from timeit import default_timer as timer\n",
    "import scipy.linalg as spl\n",
    "\n",
    "# Function to generate a random matrix\n",
    "def random_matrix(m, n):\n",
    "    return np.random.rand(m, n)\n",
    "\n",
    "def non_numba_predict(X):\n",
    "    # Multiply the left-singular values which contribute to the rank of conf\n",
    "    # by the corresponding singular values to rank reduce conf\n",
    "    U1, D, _ = np.linalg.svd(X)\n",
    "    return U1\n",
    "\n",
    "# Numba-compiled version of np.linalg.svd\n",
    "@njit(float64[:](float64[:]), cache=True)\n",
    "def numba_predict(X):\n",
    "    # Multiply the left-singular values which contribute to the rank of conf\n",
    "    # by the corresponding singular values to rank reduce conf\n",
    "    U1, D, _ = np.linalg.svd(X)\n",
    "    return U1\n",
    "\n",
    "# SciPy version of SVD\n",
    "def scipy_predict(X):\n",
    "    U1, D, _ = spl.svd(X)\n",
    "    return U1\n",
    "\n",
    "# Matrix size\n",
    "m, n = 67470, 378\n",
    "\n",
    "# Generate random matrices\n",
    "X = random_matrix(m, n)\n",
    "Y = random_matrix(m, n)\n",
    "\n",
    "# Benchmark non_numba_predict\n",
    "start = timer()\n",
    "U1 = non_numba_predict(X, Y)\n",
    "end = timer()\n",
    "numpy_time = end - start\n",
    "print(f\"NumPy SVD time: {numpy_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark numba_predict\n",
    "start = timer()\n",
    "U1 = numba_predict(X, Y)\n",
    "end = timer()\n",
    "numba_time = end - start\n",
    "print(f\"Numba SVD time: {numba_time:.6f} seconds\")\n",
    "\n",
    "# Benchmark scipy_predict\n",
    "start = timer()\n",
    "U1 = scipy_predict(X, Y)\n",
    "end = timer()\n",
    "scipy_time = end - start\n",
    "print(f\"SciPy SVD time: {scipy_time:.6f} seconds\")\n",
    "\n",
    "print(f\"Speedup (Numba over NumPy): {numpy_time / numba_time:.2f}x\")\n",
    "print(f\"Speedup (SciPy over NumPy): {numpy_time / scipy_time:.2f}x\")\n",
    "\n",
    "# Debugging Numba function\n",
    "print(\"\\nDebugging Numba function:\")\n",
    "numba_predict.inspect_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "@njit(nb.float64[:,:](nb.float64[:,:]))\n",
    "def numba_svd(X):\n",
    "    U, _, _ = np.linalg.svd(X, full_matrices=False)\n",
    "    return(U)\n",
    "    \n",
    "    \n",
    "def dask_svd(X,chunks=(1000, 100)):\n",
    "    da.from_array(X, chunks=chunks).persist()\n",
    "    U, _, _ = da.linalg.svd(X, full_matrices=False)\n",
    "    return(U.compute)\n",
    "\n",
    "\n",
    "n_rep = 50\n",
    "t1 = time.time()\n",
    "for i in np.arange(n_rep):\n",
    "    U, _, _ = np.linalg.svd(conf, full_matrices=False)\n",
    "t2 = time.time()\n",
    "\n",
    "print((t2-t1)/n_rep)\n",
    "\n",
    "U = numba_svd(conf)\n",
    "\n",
    "t1 = time.time()\n",
    "for i in np.arange(n_rep):\n",
    "    U = numba_svd(conf)\n",
    "t2 = time.time()\n",
    "    \n",
    "print((t2-t1)/n_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac456bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427644fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "conf_da = da.from_array(conf_reduced, chunks=(1000, None)).persist()\n",
    "U1, D1, V1 = da.linalg.svd(conf_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.visualize(U1, D1, V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "U1 = U1.compute()\n",
    "D1 = D1.compute()\n",
    "V1 = V1.compute()\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbe3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(np.abs((tmp @ (tmp.T @ My))- (tmp2 @ (tmp2.T @ My)))))\n",
    "print(np.amax(np.abs(My- (tmp2 @ (tmp2.T @ My)))))\n",
    "print(np.amax(np.abs(My- (tmp2 @ (tmp2.T @ My)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fabde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "U, D, V = np.linalg.svd(conf_reduced, full_matrices=False)\n",
    "\n",
    "# Get the rank of the matrix\n",
    "# rank = np.sum(D > 1e-10)\n",
    "\n",
    "# Multiply the left-singular values which contribute to the rank of conf\n",
    "# by the corresponding singular values to rank reduce conf\n",
    "# tmp = tmp[:, :rank]\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "U1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(np.abs(U @ np.diag(D) @ V - conf_reduced)))\n",
    "print(np.amax(np.abs(U1 @ np.diag(D1) @ V1 - conf_reduced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_reduced = conf[:,~np.all(conf==0,axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eca70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
