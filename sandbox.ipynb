{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import shutil \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "\n",
    "from lib.script_01_00 import generate_initial_variables\n",
    "from lib.script_01_01 import generate_raw_confounds\n",
    "from lib.script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal \n",
    "from src.nets.nets_normalise import nets_normalise \n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.nets.nets_deconfound import nets_deconfound\n",
    "\n",
    "from src.duplicate.duplicate_categorical import duplicate_categorical\n",
    "from src.duplicate.duplicate_demedian_norm_by_site import duplicate_demedian_norm_by_site\n",
    "\n",
    "from src.preproc.datenum import datenum\n",
    "from src.preproc.days_in_year import days_in_year\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "from src.memmap.addBlockToMmap import addBlockToMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82731372-cb80-4dca-aaec-ce4671396009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e28b18-6668-4814-b992-8f08cd182a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in precomputed memmaps\n",
    "IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))\n",
    "confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f330dc8-2936-4d62-aa1c-aa018154544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.nets.nets_svd import nets_svd\n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.nantools.all_non_nan_inds import all_non_nan_inds\n",
    "from src.nantools.create_nan_patterns import create_nan_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18da49a-f056-4451-8130-79674d7480f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cfg = {'cluster_type':'local','num_nodes':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c113a-7210-4c63-84d5-d991d4908b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Local Cluster\n",
    "from dask.distributed import LocalCluster\n",
    "from dask.distributed import Client, as_completed\n",
    "cluster = LocalCluster()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Connect to client\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Connect to cluster\n",
    "client = Client(cluster)   \n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Scale\n",
    "# --------------------------------------------------------------------------------\n",
    "cluster.scale(cluster_cfg['num_nodes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379723af-78bc-49c3-a540-9737059c56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee455a3-a8f0-4e19-88da-8aceb02d3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "y = IDPs[:,:]\n",
    "conf = confounds[:,:]\n",
    "mode = 'nets_svd'\n",
    "demean = True\n",
    "dtype = 'float64'\n",
    "conf_has_nans = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6df128-4ce4-41c4-9c85-963a221d8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preproc.switch_type import switch_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ddb2a-6d81-4235-966c-5949047a635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save original index\n",
    "original_index = y.index\n",
    "\n",
    "# Check if confounds have NaNs\n",
    "if conf_has_nans is None:\n",
    "    \n",
    "    # If the type is memory mapped\n",
    "    if type(conf)==MemoryMappedDF:\n",
    "\n",
    "        # Work out if the confounds have nans\n",
    "        conf_has_nans = conf[:,:].isna().sum().sum()\n",
    "\n",
    "    else:\n",
    "        # Work out if the confounds have nans\n",
    "        conf_has_nans = conf.isna().sum().sum()\n",
    "\n",
    "# If the confounds have nans\n",
    "if conf_has_nans:\n",
    "    \n",
    "    # If the type is memory mapped\n",
    "    if type(conf)==MemoryMappedDF:\n",
    "        \n",
    "        # We are trying to avoid reading everything in at once\n",
    "        conf_non_nan_inds = all_non_nan_inds(conf, safeMode=True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Otherwise, we can get the indices for non-nan rows in conf directly\n",
    "        conf_non_nan_inds = all_non_nan_inds(conf)\n",
    "\n",
    "    # Reduce conf and y down, ignoring the nan rows for conf\n",
    "    conf = conf[conf_non_nan_inds]\n",
    "    y = y[conf_non_nan_inds]\n",
    "    \n",
    "    # If we have subset the data we need to demean again\n",
    "    if demean:\n",
    "        \n",
    "        # Demean y and conf\n",
    "        y = nets_demean(y)\n",
    "        conf = nets_demean(conf)\n",
    "\n",
    "    \n",
    "# We now need to get the nan-patterns for y (we don't include\n",
    "# columns with 5 or less values).\n",
    "nan_patterns_y = create_nan_patterns(y, thresh=5)\n",
    "\n",
    "# Number of columns which meet our nan-thresholding requirements\n",
    "n_cols = len([j for i in nan_patterns_y for j in nan_patterns_y[i]['columns']])\n",
    "\n",
    "# Initialize empty nan dataframe\n",
    "y_deconf = pd.DataFrame(np.zeros((y.shape[0],n_cols),dtype=dtype),index=y.index)\n",
    "\n",
    "# We're only including column names for the variables that were not removed during nan pattern\n",
    "# identification.\n",
    "y_deconf.columns = [j for i in nan_patterns_y for j in nan_patterns_y[i]['columns']]\n",
    "\n",
    "# Change types to memory mapped dfs\n",
    "MemoryMappedDF(y).save(os.path.join(os.getcwd(),'temp_mmap','y.npz'))\n",
    "MemoryMappedDF(conf).save(os.path.join(os.getcwd(),'temp_mmap','conf.npz'))\n",
    "# conf = switch_type(conf,out_type=\"filename\",fname=os.path.join(os.getcwd(),'temp_mmap','y.conf'))\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Empty futures list\n",
    "futures = []\n",
    "\n",
    "# Loop through all unique nan patterns in y\n",
    "for i in nan_patterns_y:\n",
    "    \n",
    "    print('Deconfounding: ', i+1, '/', len(nan_patterns_y))\n",
    "\n",
    "    # Get the pattern\n",
    "    non_nan = ~np.array(nan_patterns_y[i]['pattern'],dtype=bool)\n",
    "    \n",
    "    # Submit a job to the local cluster\n",
    "    future = client.submit(inside_loop, os.path.join(os.getcwd(),'temp_mmap','y.npz'), \n",
    "                           os.path.join(os.getcwd(),'temp_mmap','conf.npz'), non_nan, mode, pure=False)\n",
    "\n",
    "    # Append to list \n",
    "    futures.append(future)\n",
    "\n",
    "# Completed jobs\n",
    "completed = as_completed(futures)\n",
    "\n",
    "# Wait for results\n",
    "for i in completed:\n",
    "    i.result()\n",
    "\n",
    "t2 = time.time()\n",
    "print('dask time: ', t2-t1)\n",
    "\n",
    "# Delete the future objects (NOTE: see above comment in setup section).\n",
    "del i, completed, futures, future_b\n",
    "\n",
    "\n",
    "# # Get the list of columns in y that are also in y_deconf\n",
    "# common_columns = [col for col in y.columns if col in y_deconf.columns]\n",
    "\n",
    "# # Reorder y_deconf columns to match the order of common columns in y\n",
    "# y_deconf = y_deconf[common_columns]\n",
    "    \n",
    "# # Initialise output dataframe\n",
    "# deconf_out = pd.DataFrame(index=original_index,columns=y_deconf.columns,dtype=dtype)\n",
    "\n",
    "# # Restore the nan rows\n",
    "# if conf_has_nans:\n",
    "#     deconf_out[conf_non_nan_inds] = np.array(y_deconf.values,dtype=dtype)\n",
    "# else:\n",
    "#     deconf_out[:] = np.array(y_deconf.values,dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae321f-ceb6-464b-acf3-401709d301c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee821ee-259f-4ce2-a00c-1420e9e8004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inside_loop(y, conf, non_nan, mode):\n",
    "\n",
    "    # Change types to memory mapped dfs\n",
    "    y = switch_type(y,out_type=\"MemoryMappedDF\")\n",
    "    conf = switch_type(conf,out_type=\"MemoryMappedDF\")\n",
    "    \n",
    "    # Get the y's we're interested in\n",
    "    y_current = y[nan_patterns_y[i]['columns']]\n",
    "\n",
    "    # Subset y and conf to the appropriate rows\n",
    "    y_current = y_current[non_nan]\n",
    "    conf_current = conf[non_nan]\n",
    "    \n",
    "    # Save y index and columns\n",
    "    y_index = y_current.index\n",
    "    y_columns = y_current.columns\n",
    "    \n",
    "    # If we are demeaning\n",
    "    if demean:\n",
    "        \n",
    "        # Demean conf_current\n",
    "        conf_current = nets_demean(conf_current)\n",
    "        \n",
    "    # We don't want to work on views of the data as it will slow the computation\n",
    "    conf_current = np.array(conf_current.values)\n",
    "    y_current = np.array(y_current.values)\n",
    "    \n",
    "    # Check if we are using psuedo inverse\n",
    "    if mode.lower() == 'pinv':\n",
    "\n",
    "        # Regress conf out of y_current - we perform the pseudo inverse on\n",
    "        # conf^T @ conf as we expect the number of columns to be much(!) less\n",
    "        # than the number of rows and thus this ends up being more numerically\n",
    "        # stable than trying to invert, or approximately invert, conf itself.\n",
    "        betahat = np.linalg.pinv(conf_current.T @ conf_current) @ conf_current.T @ y_current\n",
    "\n",
    "        # Set computational zeros to actual zeros\n",
    "        betahat[np.abs(betahat) < 1e-10] = 0\n",
    "\n",
    "        # Get deconfounding variable predicted values to regress out\n",
    "        deconf_pred = pd.DataFrame(conf_current @ betahat)\n",
    "        deconf_pred.index = y_index\n",
    "        deconf_pred.columns = y_columns\n",
    "\n",
    "    # Otherwise use svd\n",
    "    elif mode.lower() == 'nets_svd':\n",
    "        \n",
    "        # Multiply the left-singular values which contribute to the rank of conf\n",
    "        # by the corresponding singular values to rank reduce conf\n",
    "        U, S, _ = nets_svd(conf_current, reorder=False)\n",
    "        \n",
    "        # Rank reduce U and reduce datatype as only need to multiply\n",
    "        # U = U[:, S < 1e-10]\n",
    "        \n",
    "        # Get deconfounding variable predicted values to regress out\n",
    "        deconf_pred = pd.DataFrame(U @ (U.T @ y_current))\n",
    "        deconf_pred.index = y_index\n",
    "        deconf_pred.columns = y_columns\n",
    "\n",
    "    # Otherwise use svd\n",
    "    elif mode.lower() == 'svd':\n",
    "        \n",
    "        # Multiply the left-singular values which contribute to the rank of conf\n",
    "        # by the corresponding singular values to rank reduce conf\n",
    "        U, S, _ = np.linalg.svd(conf_current, full_matrices=False)\n",
    "        \n",
    "        # Get the rank of the matrix\n",
    "        rank = np.sum(S > 1e-10)\n",
    "        \n",
    "        # Rank reduce U and reduce datatype as only need to multiply\n",
    "        U = U[:, :rank]\n",
    "        \n",
    "        # Get deconfounding variable predicted values to regress out\n",
    "        deconf_pred = pd.DataFrame(U @ (U.T @ y_current))\n",
    "        deconf_pred.index = y_index\n",
    "        deconf_pred.columns = y_columns\n",
    "        \n",
    "    else:\n",
    "\n",
    "        # Perform qr decomposition\n",
    "        Q, R = np.linalg.qr(conf_current)\n",
    "        betahat = np.linalg.pinv(R) @ (Q.T @ y_current)\n",
    "\n",
    "        # Set computational zeros to actual zeros\n",
    "        betahat[np.abs(betahat) < 1e-10] = 0\n",
    "\n",
    "        # Get deconfounding variable predicted values to regress out\n",
    "        deconf_pred = pd.DataFrame(conf_current @ betahat)\n",
    "        deconf_pred.index = y_index\n",
    "        deconf_pred.columns = y_columns\n",
    "        \n",
    "    # Get deconfounded y\n",
    "    y_deconf_current = pd.DataFrame(y_current, index=y_index, columns=y_columns) - deconf_pred\n",
    "\n",
    "    # If we are demeaning, demean y\n",
    "    if demean:\n",
    "        y_deconf_current = nets_demean(y_deconf_current)\n",
    "    \n",
    "    # Update deconfounded y \n",
    "    y_deconf_current_with_nans = np.ones((len(y_deconf.index), \n",
    "                                          len(y_deconf_current.columns)))*np.NaN\n",
    "    \n",
    "    # Update with current values\n",
    "    y_deconf_current_with_nans[non_nan,:] = y_deconf_current.values[:,:]\n",
    "    \n",
    "    # Make into a dataframe with correct index and rows\n",
    "    y_deconf_current_with_nans = pd.DataFrame(y_deconf_current_with_nans,\n",
    "                                             index=y_deconf.index,\n",
    "                                             columns=y_deconf_current.columns)\n",
    "    \n",
    "    # Horizontal concatenate\n",
    "    y_deconf.update(y_deconf_current_with_nans)\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    print('iteration time: ', t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c23915-41ff-419c-9717-bce76cd8c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the y's we're interested in\n",
    "y_current = y[nan_patterns_y[i]['columns']]\n",
    "\n",
    "non_nan = ~np.array(nan_patterns_y[i]['pattern'],dtype=bool)\n",
    "\n",
    "len(non_nan),sum(non_nan),y_current.index[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6206681-8d36-452d-ab73-4eae9c3ac684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confounds = mmap\n",
    "confounds_np_copy = np.array(confounds[:,:])\n",
    "confounds_np = np.array(confounds_np_copy)\n",
    "confounds_mask = np.array(np.abs(confounds_np_copy)>1e-10,dtype=bool)\n",
    "confounds_mask_copy = np.array(confounds_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881afebe-4d2c-41b5-99b0-1adfb66bc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_copy.shape\n",
    "\n",
    "t1 = time.time()\n",
    "y_copy = IDPs[:,i].values\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "t1 = time.time()\n",
    "confounds_np_copy[~non_nan,:]=0\n",
    "confounds_mask_copy[~non_nan,:]=0\n",
    "y_copy[~non_nan,:]=0\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "confounds_num_vals = np.sum(confounds_mask_copy,axis=0)\n",
    "confounds_num_vals[confounds_num_vals==0]=1\n",
    "confounds_means = np.sum(confounds_np_copy,axis=0)/confounds_num_vals\n",
    "confounds_np_copy[:,:] = confounds_np_copy - confounds_means\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd47f2-4d6b-4de0-94fc-21f96200fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "u,d,vt = nets_svd(confounds_np_copy)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "t1 = time.time()\n",
    "u,d,vt = nets_svd(confounds_np_copy[:,confounds_num_vals>0])\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae604fef-8863-455a-9330-95eb112053ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_num_vals[confounds_num_vals==0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab691d-ca6c-4843-ae1b-81eb7a06924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf_current = np.array(conf_current.values)\n",
    "y_current = np.array(y_current.values)\n",
    "np.sum(np.isnan(U @ (U.T @ y_current)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921cf48-147e-47c6-a7ba-b4f8c868d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(U), y_current.shape\n",
    "t1 = time.time()\n",
    "U @ (U.T @ y_current)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "y_current2 = np.array(y_current.values)\n",
    "\n",
    "t1 = time.time()\n",
    "U @ (U.T @ y_current2)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48b997-9cf0-4e82-88d4-82b4fc6fff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d1a6d-a8c9-4d42-8548-bccbce9b80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "conf_current = np.array(conf_current)\n",
    "\n",
    "t1 = time.time()\n",
    "u,d,v = np.linalg.svd(conf_current, full_matrices=False)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "u2,d2,v2 = jnp.linalg.svd(conf_current, full_matrices=False)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "print(np.allclose(u,u2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0b8a1-30fa-4dd4-abe8-a138531514ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_current.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e976b1-c056-469e-8024-809ce90d61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf_current.shape, y_current.shape\n",
    "betahat = np.linalg.pinv(conf_current.T @ conf_current) @ conf_current.T @ y_current\n",
    "\n",
    "# Set computational zeros to actual zeros\n",
    "betahat[np.abs(betahat) < 1e-10] = 0\n",
    "\n",
    "print(conf_current.shape, betahat.shape)\n",
    "\n",
    "# Get deconfounding variable predicted values to regress out\n",
    "deconf_pred = pd.DataFrame(conf_current @ betahat)\n",
    "# deconf_pred.index = y_current.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c781494-7a4f-4497-90bd-450bc8378f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tmp2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8556c3-375b-4c93-8a28-4a4f265e5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def tmp_svd(x):\n",
    "    u, d, v = np.linalg.svd(x,full_matrices=False)\n",
    "    return(u,d,v)\n",
    "\n",
    "nrep = 10\n",
    "t_total1 = 0\n",
    "t_total2 = 0\n",
    "for i in range(nrep):\n",
    "    t1 = time.time()\n",
    "    u, d, v = np.linalg.svd(x,full_matrices=False)\n",
    "    tmp1 = u @ (u.T @ y)\n",
    "    t2 = time.time()\n",
    "    t_total1 = t_total1+t2-t1\n",
    "    \n",
    "    t1 = time.time()\n",
    "    q,r = np.linalg.qr(x)\n",
    "    tmp2 = q @ (q.T @ y)\n",
    "    t2 = time.time()\n",
    "    t_total2 = t_total2+t2-t1\n",
    "\n",
    "    print(np.allclose(tmp1,tmp2))\n",
    "\n",
    "print(t_total1/nrep, t_total2/nrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389cce2-d283-4482-8c0e-462365ceddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import spmatrix, cholmod\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Convert to CVXOPT spmatrix\n",
    "\n",
    "# Convert to scipy.sparse CSR matrix\n",
    "scipy_sparse_matrix = csr_matrix(tmp)\n",
    "\n",
    "# Convert to CVXOPT spmatrix\n",
    "A = spmatrix(tmp[tmp!=0][:],np.where(tmp!=0)[0],np.where(tmp!=0)[0])\n",
    "B = matrix(y)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "t1 = time.time()\n",
    "cholmod.linsolve(A,B)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "print(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89b1d8-97b1-4aa0-ab15-18e6c993f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tmp[tmp!=0][:]),list(np.where(tmp!=0)[0]),list(np.where(tmp!=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af00f57-4275-429c-be72-e3131b04b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_normalise import nets_normalise\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal\n",
    "from src.nets.nets_deconfound import nets_deconfound\n",
    "\n",
    "from src.preproc.filter_columns_by_site import filter_columns_by_site\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe965927-8a98-4238-b7e3-38f5acda3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confound groups we are interested in.\n",
    "conf_name = ['AGE', 'AGE_SEX', 'HEAD_SIZE',  'TE', 'STRUCT_MOTION', \n",
    "             'DVARS', 'HEAD_MOTION', 'HEAD_MOTION_ST', 'TABLE_POS', \n",
    "             'EDDY_QC']\n",
    "\n",
    "# Get all the confounds in the group\n",
    "conf_group = all_conf.get_groups(conf_name)\n",
    "\n",
    "# Get the subject ids\n",
    "sub_ids = IDPs.index\n",
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site = []\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in unique_site_ids:\n",
    "\n",
    "    # Find the indices where all elements in a row of siteDATA match the current valueSite\n",
    "    # Note: This assumes siteDATA and siteValues have compatible shapes or values for comparison\n",
    "    indices = np.where((site_ids == site_id).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site.append(indices)\n",
    "\n",
    "# Delete the indices\n",
    "del indices\n",
    "\n",
    "# Initialise empty array to store results\n",
    "conf_nonlin = pd.DataFrame(index=conf_group.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a19b47-5c53-404d-a059-0b8b4e6b61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nets_deconfound(IDPs[:,:], all_conf[:,:], 'svd')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.nantools.create_nan_patterns import create_nan_patterns\n",
    "from src.nantools.all_non_nan_inds import all_non_nan_inds\n",
    "from src.nets.nets_demean import nets_demean\n",
    "\n",
    "import time\n",
    "y = IDPs\n",
    "conf = confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c63a12-cfa2-4df0-a0cd-48e3e78fcac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "# Save original index\n",
    "original_index = y.index\n",
    "\n",
    "# Get the indices for non-nan rows in conf\n",
    "conf_non_nan_inds = all_non_nan_inds(conf, safeMode=True)\n",
    "\n",
    "# Reduce conf and y down, ignoring the nan rows for conf\n",
    "if not safeMode:\n",
    "# conf = conf[conf_non_nan_inds]\n",
    "# y = y[conf_non_nan_inds]\n",
    "\n",
    "# # Initialize empty nan dataframe\n",
    "# y_deconf = pd.DataFrame(index=y.index,dtype='float64')\n",
    "\n",
    "# # If we are demeaning\n",
    "# if demean:\n",
    "    \n",
    "#     # Demean y and conf\n",
    "#     y = nets_demean(y)\n",
    "#     conf = nets_demean(conf)\n",
    "    \n",
    "# # We now need to get the nan-patterns for y\n",
    "# nan_patterns_y = create_nan_patterns(y)\n",
    "\n",
    "# t2 = time.time()\n",
    "# print('init time: ', t2-t1)\n",
    "\n",
    "# # Loop through all unique nan patterns in y\n",
    "# for i in nan_patterns_y:\n",
    "    \n",
    "#     t1 = time.time()\n",
    "#     print('Deconfounding: ', i+1, '/', len(nan_patterns_y))\n",
    "\n",
    "#     # Get the pattern\n",
    "#     non_nan = ~np.array(nan_patterns_y[i]['pattern'],dtype=bool)\n",
    "\n",
    "#     t2 = time.time()\n",
    "#     print('nonnan time: ', t2-t1)\n",
    "    \n",
    "#     # Check if we have at least 5 non-nan values\n",
    "#     if np.sum(1*non_nan) > 5:\n",
    "\n",
    "#         t1 = time.time()\n",
    "        \n",
    "#         # Subset y to the appropriate columns\n",
    "#         cols = nan_patterns_y[i]['columns']\n",
    "\n",
    "#         # Get the y's we're interested in\n",
    "#         y_current = y[nan_patterns_y[i]['columns']]\n",
    "\n",
    "#         # Subset y and conf to the appropriate rows\n",
    "#         y_current = y_current[non_nan]\n",
    "#         conf_current = conf[non_nan]\n",
    "        \n",
    "#         t2 = time.time()\n",
    "#         print('subsetting time: ', t2-t1)\n",
    "    \n",
    "#         t1 = time.time()\n",
    "#         # If we are demeaning\n",
    "#         if demean:\n",
    "            \n",
    "#             # Demean conf_current\n",
    "#             conf_current = nets_demean(conf_current)\n",
    "\n",
    "#         t2 = time.time()\n",
    "#         print('demean time: ', t2-t1)\n",
    "        \n",
    "#         t1 = time.time()\n",
    "#         # Increase the precision on conf_current (just in case overflow\n",
    "#         # becomes a risk)\n",
    "#         conf_current = np.array(conf_current,dtype=np.float64)\n",
    "\n",
    "#         t2 = time.time()\n",
    "#         print('copy time: ', t2-t1)\n",
    "        \n",
    "#         # Check if we are using psuedo inverse\n",
    "#         if mode.lower() == 'pinv':\n",
    "\n",
    "#             # Regress conf out of y_current - we perform the pseudo inverse on\n",
    "#             # conf^T @ conf as we expect the number of columns to be much(!) less\n",
    "#             # than the number of rows and thus this ends up being more numerically\n",
    "#             # stable than trying to invert, or approximately invert, conf itself.\n",
    "#             betahat = np.linalg.pinv(conf_current.T @ conf_current) @ conf_current.T @ y_current\n",
    "\n",
    "#             # Set computational zeros to actual zeros\n",
    "#             betahat[np.abs(betahat) < 1e-10] = 0\n",
    "\n",
    "#             # Get deconfounding variable predicted values to regress out\n",
    "#             deconf_pred = pd.DataFrame(conf_current @ betahat)\n",
    "#             deconf_pred.index = y_current.index\n",
    "\n",
    "#         # Otherwise use svd\n",
    "#         elif mode.lower() == 'svd':\n",
    "\n",
    "#             t1 = time.time()\n",
    "#             # Multiply the left-singular values which contribute to the rank of conf\n",
    "#             # by the corresponding singular values to rank reduce conf\n",
    "#             U, S, Vt = np.linalg.svd(conf_current, full_matrices=False)\n",
    "\n",
    "#             # Get the rank of the matrix\n",
    "#             rank = np.sum(S > 1e-10)\n",
    "\n",
    "#             # Rank reduce U\n",
    "#             U = U[:, :rank] \n",
    "\n",
    "#             # Get deconfounding variable predicted values to regress out\n",
    "#             deconf_pred = pd.DataFrame(U @ (U.T @ y_current))\n",
    "#             deconf_pred.index = y_current.index\n",
    "            \n",
    "#             t2 = time.time()\n",
    "#             print('svd time: ', t2-t1)\n",
    "            \n",
    "#         else:\n",
    "\n",
    "#             # Perform qr decomposition\n",
    "#             Q, R = np.linalg.qr(conf_current)\n",
    "#             betahat = np.linalg.pinv(R) @ (Q.T @ y_current)\n",
    "\n",
    "#             # Set computational zeros to actual zeros\n",
    "#             betahat[np.abs(betahat) < 1e-10] = 0\n",
    "\n",
    "#             # Get deconfounding variable predicted values to regress out\n",
    "#             deconf_pred = pd.DataFrame(conf_current @ betahat)\n",
    "#             deconf_pred.index = y_current.index\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Get deconfounded y\n",
    "#         y_deconf_current = y_current - deconf_pred\n",
    "#         t2 = time.time()\n",
    "#         print('deconf_current time: ', t2-t1)\n",
    "    \n",
    "#         t1 = time.time()\n",
    "#         # If we are demeaning, demean y\n",
    "#         if demean:\n",
    "#             y_deconf_current = nets_demean(y_deconf_current)\n",
    "#         t2 = time.time()\n",
    "#         print('demean time 2: ', t2-t1)\n",
    "        \n",
    "#         t1 = time.time()\n",
    "#         # Update deconfounded y (v2)\n",
    "#         y_deconf_current_with_nans = np.ones((len(y_deconf.index), \n",
    "#                                               len(y_deconf_current.columns)))*np.NaN\n",
    "        \n",
    "#         # Update with current values\n",
    "#         y_deconf_current_with_nans[non_nan,:] = y_deconf_current.values[:,:]\n",
    "        \n",
    "#         # Make into a dataframe with correct index and rows\n",
    "#         y_deconf_current_with_nans = pd.DataFrame(y_deconf_current_with_nans,\n",
    "#                                                  index=y_deconf.index,\n",
    "#                                                  columns=y_deconf_current.columns)\n",
    "        \n",
    "#         # Horizontal concatenate\n",
    "#         y_deconf = pd.concat((y_deconf_current_with_nans, y_deconf), axis=1)\n",
    "        \n",
    "#         t2 = time.time()\n",
    "#         print('update deconfounded: ', t2-t1)\n",
    "\n",
    "# t1 = time.time()\n",
    "# # Get the list of columns in y that are also in y_deconf\n",
    "# common_columns = [col for col in y.columns if col in y_deconf.columns]\n",
    "\n",
    "# # Reorder y_deconf columns to match the order of common columns in y\n",
    "# y_deconf = y_deconf[common_columns]\n",
    "    \n",
    "# # Remove columns where all values are NaN\n",
    "# y_deconf = y_deconf.dropna(axis=1, how='all')\n",
    "\n",
    "# # Restore the nan rows\n",
    "# deconf_out = pd.DataFrame(index=original_index,columns=y_deconf.columns,dtype=dtype)\n",
    "# deconf_out[conf_non_nan_inds] = np.array(y_deconf.values,dtype='float64')\n",
    "# t2 = time.time()\n",
    "# print('deconf out: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3b459-91e3-4a36-b452-b9a69ee49f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "\n",
    "type(conf)==MemoryMappedDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948de57-3e79-437d-bc35-0f6adaef32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "conf_non_nan_inds = all_non_nan_inds(conf, safeMode=True)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d76f4-3867-41b6-a4e3-09327722863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = conf\n",
    "\n",
    "\n",
    "# Create an empty boolean array\n",
    "nan_array = np.zeros(x.shape[1], dtype=bool)\n",
    "\n",
    "# Loop through columns one by one\n",
    "for col in range(x.shape[1]):\n",
    "    nan_array[col] = np.isnan(x[:, col]).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5a4b2-3a6d-4cc9-aebe-5bf57a35d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_array.shape, np.isnan(x[:, col]).any().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad1681-412e-466c-a302-84fb2f2d7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89027c13-0f84-4e7e-9103-131fb695c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(x[:, 0]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425adca-936d-47fe-b481-13a5ab08876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_non_nan_inds(x, safeMode=False):\n",
    "\n",
    "    # If we aren't in safe mode just read everything in.\n",
    "    if not safeMode:\n",
    "        \n",
    "        # If the type is memory mapped\n",
    "        if type(x)==MemoryMappedDF:\n",
    "    \n",
    "            # Get the values\n",
    "            x = x[:,:].values\n",
    "\n",
    "        return(~np.isnan(x).any(axis=1))\n",
    "\n",
    "    # Assume we can't load all data in at once\n",
    "    else:\n",
    "\n",
    "        # Create an empty boolean array\n",
    "        nan_array = np.zeros(x.shape[1], dtype=bool)\n",
    "\n",
    "        # Loop through columns one by one\n",
    "        for col in range(x.shape[1]):\n",
    "            nan_array[col] = np.isnan(x[:, col].values).any()\n",
    "\n",
    "        # Return result\n",
    "        return(nan_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e375a-845e-43a3-8234-c44f1bdfa0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from src.nets.nets_svd import nets_svd\n",
    "\n",
    "tmp = np.random.randn(60000,400)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "u,d,v = np.linalg.svd(tmp, full_matrices=False)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "t1 = time.time()\n",
    "u2,d2,v2 = nets_svd(tmp, reorder=False)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c3cfc-f4f8-4cb8-a564-644c437554a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(np.abs(tmp - (u @ np.diag(d) @ v))), np.amax(np.abs(tmp - (u2 @ np.diag(d2) @ v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05c106-aba1-4220-8370-20c90fcda833",
   "metadata": {},
   "outputs": [],
   "source": [
    "u2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835696b-c6a7-47d2-be9d-64243ae06bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdc83b-eb68-4715-91e2-ca6089e282bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d7c24d-aac8-469d-b9ab-34871f477e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735508c-f204-46de-b2ed-27ffd5c7fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dask.config.paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed1d1f-9415-42ee-a411-2259a3735413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf37ac-310a-426f-a7c1-19e2709aede8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd12d7-dd00-4bd8-9ded-514417d30339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07410473-3627-48aa-9a04-e6bb39cb715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "from dask.distributed import Client, as_completed\n",
    "cluster = LocalCluster()\n",
    "\n",
    "# Connect to cluster\n",
    "client = Client(cluster)   \n",
    "\n",
    "# Read in number of nodes we need\n",
    "num_nodes = 12\n",
    "\n",
    "# Scale the cluster\n",
    "cluster.scale(num_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31309196-41b2-4a00-af24-2b3267e24bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4590f4e-f623-483e-80b2-20de2ad32fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447197a8-3691-4633-a86c-68dcbd9ef4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = IDPs[:,:]\n",
    "conf = confounds[:,:]\n",
    "mode='nets_svd'\n",
    "demean=True, \n",
    "dtype='float64'\n",
    "conf_has_nans=None\n",
    "\n",
    "t1 = time.time()\n",
    "# Save original index\n",
    "original_index = y.index\n",
    "\n",
    "# Check if confounds have NaNs\n",
    "if conf_has_nans is None:\n",
    "    \n",
    "    # If the type is memory mapped\n",
    "    if type(conf)==MemoryMappedDF:\n",
    "\n",
    "        # Work out if the confounds have nans\n",
    "        conf_has_nans = conf[:,:].isna().sum().sum()\n",
    "\n",
    "    else:\n",
    "        # Work out if the confounds have nans\n",
    "        conf_has_nans = conf.isna().sum().sum()\n",
    "\n",
    "# If the confounds have nans\n",
    "if conf_has_nans:\n",
    "    \n",
    "    # If the type is memory mapped\n",
    "    if type(conf)==MemoryMappedDF:\n",
    "        \n",
    "        # We are trying to avoid reading everything in at once\n",
    "        conf_non_nan_inds = all_non_nan_inds(conf, safeMode=True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Otherwise, we can get the indices for non-nan rows in conf directly\n",
    "        conf_non_nan_inds = all_non_nan_inds(conf)\n",
    "\n",
    "    # Reduce conf and y down, ignoring the nan rows for conf\n",
    "    conf = conf[conf_non_nan_inds]\n",
    "    y = y[conf_non_nan_inds]\n",
    "    \n",
    "    # If we have subset the data we need to demean again\n",
    "    if demean:\n",
    "        \n",
    "        # Demean y and conf\n",
    "        y = nets_demean(y)\n",
    "        conf = nets_demean(conf)\n",
    "\n",
    "# Initialize empty nan dataframe\n",
    "y_deconf = pd.DataFrame(index=y.index,dtype=dtype)\n",
    "\n",
    "    \n",
    "# We now need to get the nan-patterns for y\n",
    "nan_patterns_y = create_nan_patterns(y)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print('init time: ', t2-t1)\n",
    "    \n",
    "# Empty futures list\n",
    "futures = []\n",
    "\n",
    "# Submit jobs\n",
    "for i in nan_patterns_y:\n",
    "\n",
    "    print('Deconfounding: ', i+1, '/', len(nan_patterns_y))\n",
    "\n",
    "    # Get the pattern\n",
    "    non_nan = ~np.array(nan_patterns_y[i]['pattern'],dtype=bool)\n",
    "    \n",
    "    # Check if we have at least 5 non-nan values\n",
    "    if np.sum(1*non_nan) > 5:\n",
    "        \n",
    "        # Run the i^{th} job.\n",
    "        future_i = client.submit(inside_loop, \n",
    "                                 y[nan_patterns_y[i]['columns']], \n",
    "                                 conf, non_nan, mode, pure=False)\n",
    "    \n",
    "        # Append to list \n",
    "        futures.append(future_i)\n",
    "\n",
    "# Completed jobs\n",
    "completed = as_completed(futures)\n",
    "\n",
    "    \n",
    "# # Get the list of columns in y that are also in y_deconf\n",
    "# common_columns = [col for col in y.columns if col in y_deconf.columns]\n",
    "\n",
    "# # Reorder y_deconf columns to match the order of common columns in y\n",
    "# y_deconf = y_deconf[common_columns]\n",
    "    \n",
    "# # Remove columns where all values are NaN\n",
    "# y_deconf = y_deconf.dropna(axis=1, how='all')\n",
    "\n",
    "# # Initialise output dataframe\n",
    "# deconf_out = pd.DataFrame(index=original_index,columns=y_deconf.columns,dtype=dtype)\n",
    "\n",
    "# # Restore the nan rows\n",
    "# if conf_has_nans:\n",
    "#     deconf_out[conf_non_nan_inds] = np.array(y_deconf.values,dtype=dtype)\n",
    "# else:\n",
    "#     deconf_out[:] = np.array(y_deconf.values,dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54969e99-1646-4c72-a0fa-d5075141ee75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7272358-6d57-4aa5-bdc2-ec63a128394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preproc.switch_type import switch_type\n",
    "\n",
    "#nonIDPs.save(os.path.join(os.getcwd(),'tmp_file2'))\n",
    "x=os.path.join(os.getcwd(),'tmp_file2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4c04b-7099-44b6-a1a6-66342a1b403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=switch_type(x,'pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061aa92-0e16-4d82-88e1-961c51a16403",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8347f-fe05-4069-8f39-23ae35493018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "y = read_memmap_df(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85e1d9-5bb0-4e0b-bcbb-4334853caadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179524e-2c44-4c9a-b8fb-77e62aedc462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
