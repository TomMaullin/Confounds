{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal\n",
    "\n",
    "from src.preproc.datenum import datenum\n",
    "from src.preproc.days_in_year import days_in_year\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.memmap.read_memmap_df import read_memmap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82731372-cb80-4dca-aaec-ce4671396009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e28b18-6668-4814-b992-8f08cd182a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in precomputed memmaps\n",
    "IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ed42c-1637-4404-a841-858a5887e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "# Unpickler class used to load in pickled object if class definition isn't where\n",
    "# expected.\n",
    "class MyUnpickler(pickle.Unpickler):\n",
    "    \n",
    "    # Look for the memory mapped df class\n",
    "    def find_class(self, module, name):\n",
    "        \n",
    "        # If the name of the object is MemoryMappedDF return the class we loaded\n",
    "        if name == 'MemoryMappedDF':\n",
    "            return MemoryMappedDF\n",
    "        \n",
    "        # Else return the standard class\n",
    "        else:\n",
    "            return super().find_class(module, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19bac4-a138-4416-b6ce-c3255777b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group dataframe columns by their data types\n",
    "dtype_groups = misc.columns.to_series().groupby(misc.dtypes).groups\n",
    "\n",
    "# Iterate over each data type group\n",
    "for dtype, columns in dtype_groups.items():\n",
    "\n",
    "    print(dtype.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd458b-89ef-4e12-98c7-275ce88e34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz')\n",
    "    \n",
    "# Read in self_copy and create a new instance\n",
    "with open(filename, 'rb') as f:\n",
    "    unpickler = MyUnpickler(f)\n",
    "    self_copy2 = unpickler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d84df1-00d2-4377-bd14-8cc02031a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(),'saved_memmaps','misc.npz')\n",
    "    \n",
    "# Read in self_copy and create a new instance\n",
    "with open(filename, 'rb') as f:\n",
    "    unpickler = MyUnpickler(f)\n",
    "    self_copy = unpickler.load()\n",
    "\n",
    "\n",
    "# Create a new hash\n",
    "self_copy.hash = str(uuid.uuid4())\n",
    "\n",
    "# Save the mode\n",
    "self_copy.mode = 'r+'\n",
    "\n",
    "# Check output directory exists\n",
    "if not os.path.exists(self_copy.directory):\n",
    "\n",
    "    # Make the directory\n",
    "    os.makedirs(self_copy.directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ae9f5-52ea-4055-91a3-99a13dea73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # We now make a new copy of the memory map files behind the scenes so we \n",
    "    # don't delete the original files on close\n",
    "    for dtype, memmap_fname in self_copy.memory_maps.items():\n",
    "        \n",
    "        # Create new memory map filename using new hash\n",
    "        filename = os.path.join(self_copy.directory, f\"{self_copy.hash}_{dtype}.dat\")\n",
    "\n",
    "        print(filename, memmap_fname)\n",
    "        \n",
    "        # Copy memmap file to new filename\n",
    "        shutil.copy(memmap_fname, filename)\n",
    "        \n",
    "        # # Get number of elements in original memmap\n",
    "        # memmap_numel = np.memmap(filename, \n",
    "        #                          dtype=self_copy.data_types[dtype], \n",
    "        #                          mode='c').shape[0]\n",
    "        \n",
    "        # # Get number of columns in original memmap\n",
    "        # memmap_ncol = len(self_copy.column_headers[dtype])\n",
    "        \n",
    "        # # Get memmap shape\n",
    "        # memmap_shape = (memmap_numel//memmap_ncol, memmap_ncol)\n",
    "        \n",
    "        # Create new memory map\n",
    "        # self_copy.memory_maps[dtype]= np.memmap(filename, \n",
    "        #                                         dtype=self_copy.data_types[dtype], \n",
    "        #                                         mode='r+', \n",
    "        #                                         shape=memmap_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f8efb-c3ed-4be7-8789-8def1dc53bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# # Get number of elements in original memmap\n",
    "# memmap_numel = np.memmap(filename, \n",
    "#                          dtype=object, \n",
    "#                          mode='c').shape[0]\n",
    "\n",
    "# # Get number of columns in original memmap\n",
    "# memmap_ncol = len(self_copy.column_headers[dtype])\n",
    "\n",
    "# # Get memmap shape\n",
    "# memmap_shape = (memmap_numel//memmap_ncol, memmap_ncol)\n",
    "\n",
    "memmap_shape= (67470, 4)\n",
    "# Create new memory map\n",
    "tmp= np.memmap('/gpfs3/well/nichols/users/inf852/confounds/temp_mmap/7a01178b-0d74-437e-b84e-d636ffcc0217_object.dat',\n",
    "                mode='r+', \n",
    "                shape=memmap_shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6580b-991d-411f-bc60-96ae9cafe394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7e432-a8c8-40e2-a90b-33d3526b2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "\n",
    "memmap_file = os.path.join(f\"/well/nichols/users/inf852/confounds/temp_mmap/8c16a6dd-0eb4-42aa-b7e3-95a155c4b0b2_object.dat\")\n",
    "filename = os.path.join(f\"/well/nichols/users/inf852/confounds/temp_mmap/tmp_object.dat\")\n",
    "\n",
    "# Copy memmap file to new filename\n",
    "shutil.copy(memmap_file, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aba7b3-02ab-43b6-8211-b617e9584485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get number of elements in original memmap\n",
    "memmap_numel = np.memmap(filename, \n",
    "                         dtype=misc.data_types[dtype], \n",
    "                         mode='c').shape[0]\n",
    "\n",
    "# Get number of columns in original memmap\n",
    "memmap_ncol = len(misc.column_headers[dtype])\n",
    "\n",
    "# Get memmap shape\n",
    "memmap_shape = (memmap_numel//memmap_ncol, memmap_ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69faf0-461c-46f4-a740-bbc8f0a36b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.memmap(filename, \n",
    "                         dtype=misc.data_types[dtype], \n",
    "                         mode='c',\n",
    "               shape=memmap_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa06126-7a57-4559-93c3-9b3d8f960322",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0:2,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a6044-f31a-4614-bdf2-0accbb45a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data types for IDs\n",
    "dtypes = {0: 'int32', 1: 'Int16'}\n",
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544812c-a9e9-426b-be07-e8074dfd4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site = []\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in unique_site_ids:\n",
    "\n",
    "    # Find the indices where all elements in a row of siteDATA match the current valueSite\n",
    "    # Note: This assumes siteDATA and siteValues have compatible shapes or values for comparison\n",
    "    indices = np.where((site_ids == site_id).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site.append(indices)\n",
    "\n",
    "# Delete the indices\n",
    "del indices\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Construct confounds for between sites\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Get the number of rows in ALL_IDs\n",
    "n = len(sub_ids)\n",
    "\n",
    "# Initialize names_site list and conf_site matrix\n",
    "names_site = []\n",
    "conf_site = np.zeros((n, len(inds_per_site)-1))\n",
    "\n",
    "# Subjects from Site 1 will have -1 in all site confounds\n",
    "conf_site[inds_per_site[0], :] = -1\n",
    "\n",
    "# Subjects for the other sites will have -1 in their corresponding column\n",
    "# Value by default is 0\n",
    "for i in range(1, len(inds_per_site)):\n",
    "    conf_site[inds_per_site[i], i-1] = 1\n",
    "    names_site.append(f'Site_1_vs_{i+1}')\n",
    "\n",
    "# Normalize conf_site using the nets_normalise function\n",
    "conf_site = nets_normalise(conf_site)\n",
    "conf_site[np.isnan(conf_site)] = 0\n",
    "\n",
    "# Make into dataframe\n",
    "conf_site = pd.DataFrame(conf_site)\n",
    "conf_site.columns = names_site\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Construct dummy confounds for categorical variables\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Construct dummy variables for the following\n",
    "conf_sex          = duplicate_categorical('SEX',         sub_ids, inds_per_site, data_dir)\n",
    "conf_batch        = duplicate_categorical('BATCH',       sub_ids, inds_per_site, data_dir)\n",
    "conf_cmrr         = duplicate_categorical('CMRR',        sub_ids, inds_per_site, data_dir)\n",
    "conf_protocol     = duplicate_categorical('PROTOCOL',    sub_ids, inds_per_site, data_dir)\n",
    "conf_service_pack = duplicate_categorical('SERVICEPACK', sub_ids, inds_per_site, data_dir)\n",
    "conf_scan_events  = duplicate_categorical('SCANEVENTS',  sub_ids, inds_per_site, data_dir)\n",
    "conf_flipped_swi  = duplicate_categorical('FLIPPEDSWI',  sub_ids, inds_per_site, data_dir)\n",
    "conf_fst2         = duplicate_categorical('FST2',        sub_ids, inds_per_site, data_dir)\n",
    "conf_new_eddy     = duplicate_categorical('NEWEDDY',     sub_ids, inds_per_site, data_dir)\n",
    "conf_scaling      = duplicate_categorical('SCALING',     sub_ids, inds_per_site, data_dir)\n",
    "conf_time_points  = duplicate_categorical('TIMEPOINTS',  sub_ids, inds_per_site, data_dir)\n",
    "\n",
    "# Concatenate all the DataFrames/Series horizontally\n",
    "categorical_IDPs = pd.concat([\n",
    "    conf_sex.reset_index(drop=True), \n",
    "    conf_batch.reset_index(drop=True),\n",
    "    conf_cmrr.reset_index(drop=True),\n",
    "    conf_protocol.reset_index(drop=True),\n",
    "    conf_service_pack.reset_index(drop=True),\n",
    "    conf_scan_events.reset_index(drop=True),\n",
    "    conf_flipped_swi.reset_index(drop=True),\n",
    "    conf_fst2.reset_index(drop=True),\n",
    "    conf_new_eddy.reset_index(drop=True),\n",
    "    conf_scaling.reset_index(drop=True),\n",
    "    conf_time_points.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Set row indices on dataframe\n",
    "categorical_IDPs.index = sub_ids\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Construct dummy confounds for continuous variables\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Construct dummy variables for the following\n",
    "conf_head_motion         = duplicate_demedian_norm_by_site('HEADMOTION',       sub_ids, inds_per_site, data_dir)\n",
    "conf_head_motion_st      = duplicate_demedian_norm_by_site('HEADMOTIONST',     sub_ids, inds_per_site, data_dir)\n",
    "conf_head_size           = duplicate_demedian_norm_by_site('HEADSIZE',         sub_ids, inds_per_site, data_dir)\n",
    "conf_table_pos           = duplicate_demedian_norm_by_site('TABLEPOS',         sub_ids, inds_per_site, data_dir)\n",
    "conf_dvars               = duplicate_demedian_norm_by_site('DVARS',            sub_ids, inds_per_site, data_dir)\n",
    "conf_eddy_qc             = duplicate_demedian_norm_by_site('EDDYQC',           sub_ids, inds_per_site, data_dir)\n",
    "conf_struct_head_motion  = duplicate_demedian_norm_by_site('STRUCTHEADMOTION', sub_ids, inds_per_site, data_dir)\n",
    "conf_age                 = duplicate_demedian_norm_by_site('AGE',              sub_ids, inds_per_site, data_dir)\n",
    "conf_te                  = duplicate_demedian_norm_by_site('TE',               sub_ids, inds_per_site, data_dir) \n",
    "\n",
    "# Concatenate all the DataFrames/Series horizontally\n",
    "continuous_IDPs = pd.concat([\n",
    "    conf_age.reset_index(drop=True),\n",
    "    conf_head_size.reset_index(drop=True),\n",
    "    conf_te.reset_index(drop=True),\n",
    "    conf_struct_head_motion.reset_index(drop=True),\n",
    "    conf_dvars.reset_index(drop=True),\n",
    "    conf_head_motion.reset_index(drop=True),\n",
    "    conf_head_motion_st.reset_index(drop=True), \n",
    "    conf_table_pos.reset_index(drop=True),\n",
    "    conf_eddy_qc.reset_index(drop=True),\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Construct dummy confounds for age-sex interaction\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize conf_age_sex with zeros of the same shape as conf_age\n",
    "conf_age_sex = pd.DataFrame(np.zeros_like(conf_age))\n",
    "\n",
    "# Loop over columns in conf_age\n",
    "for i in range(conf_age.shape[1]):\n",
    "\n",
    "    # Find indices where confAge is not zero\n",
    "    ind_non_zero = np.where(conf_age.iloc[:, i] != 0)[0]\n",
    "\n",
    "    # Apply nets_normalise to the product of confAge and confSex for non-zero age indices\n",
    "    conf_age_sex.iloc[ind_non_zero, i] = nets_normalise(conf_age.iloc[ind_non_zero, i] * conf_sex.iloc[ind_non_zero, i])\n",
    "\n",
    "    # Replace NaN values with 0, if any\n",
    "    conf_age_sex[np.isnan(conf_age_sex)] = 0\n",
    "\n",
    "# Generate names for AgeSex per site\n",
    "names_age_sex = [f'AgeSex_Site_{j}' for j in range(1, len(inds_per_site) + 1)]\n",
    "\n",
    "# Set column names for conf_age_sex\n",
    "conf_age_sex.columns = names_age_sex\n",
    "\n",
    "# Create confounds dataframe\n",
    "confounds = pd.concat([\n",
    "    conf_site.reset_index(drop=True),\n",
    "    categorical_IDPs.reset_index(drop=True),\n",
    "    continuous_IDPs.reset_index(drop=True),\n",
    "    conf_age_sex.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Save the index\n",
    "confounds.index = sub_ids\n",
    "\n",
    "# Get confounds memory mapped dataframe\n",
    "confounds = MemoryMappedDF(confounds)\n",
    "\n",
    "# Quick access of site variables\n",
    "confounds.set_group('SITE', conf_site.columns.tolist())\n",
    "\n",
    "# Add groups of categorical variable names for easy access\n",
    "confounds.set_group('SEX', conf_sex.columns.tolist())\n",
    "confounds.set_group('BATCH', conf_batch.columns.tolist())\n",
    "confounds.set_group('CMRR', conf_cmrr.columns.tolist())\n",
    "confounds.set_group('PROTOCOL', conf_protocol.columns.tolist())\n",
    "confounds.set_group('SERVICE_PACK', conf_service_pack.columns.tolist())\n",
    "confounds.set_group('SCAN_EVENTS', conf_scan_events.columns.tolist())\n",
    "confounds.set_group('FLIPPED_SWI', conf_flipped_swi.columns.tolist())\n",
    "confounds.set_group('FS_T2', conf_fst2.columns.tolist())\n",
    "confounds.set_group('NEW_EDDY', conf_new_eddy.columns.tolist())\n",
    "confounds.set_group('SCALING', conf_scaling.columns.tolist())\n",
    "confounds.set_group('TIMEPOINTS', conf_time_points.columns.tolist())\n",
    "\n",
    "# Add groups of continuous variable names for easy access\n",
    "confounds.set_group('AGE', conf_age.columns.tolist())\n",
    "confounds.set_group('HEAD_SIZE', conf_head_size.columns.tolist())\n",
    "confounds.set_group('TE', conf_te.columns.tolist())\n",
    "confounds.set_group('STRUCT_MOTION', conf_struct_head_motion.columns.tolist())\n",
    "confounds.set_group('DVARS', conf_dvars.columns.tolist())\n",
    "confounds.set_group('HEAD_MOTION', conf_head_motion.columns.tolist())\n",
    "confounds.set_group('HEAD_MOTION_ST', conf_head_motion_st.columns.tolist())\n",
    "confounds.set_group('TABLE_POS', conf_table_pos.columns.tolist())\n",
    "confounds.set_group('EDDY_QC', conf_eddy_qc.columns.tolist())\n",
    "\n",
    "# Add groups of age_sex variables for easy access\n",
    "confounds.set_group('AGE_SEX', conf_age_sex.columns.tolist())\n",
    "\n",
    "# Add group of subject level confounds\n",
    "confounds.set_group('SUBJECT', conf_age.columns.tolist() + \\\n",
    "                               conf_sex.columns.tolist() + \\\n",
    "                               conf_age_sex.columns.tolist() + \\\n",
    "                               conf_head_size.columns.tolist())\n",
    "\n",
    "# Add group of acquisition related confounds\n",
    "confounds.set_group('ACQ', conf_site.columns.tolist() + \\\n",
    "                           conf_batch.columns.tolist() + \\\n",
    "                           conf_cmrr.columns.tolist() + \\\n",
    "                           conf_protocol.columns.tolist() + \\\n",
    "                           conf_service_pack.columns.tolist() + \\\n",
    "                           conf_scan_events.columns.tolist() + \\\n",
    "                           conf_flipped_swi.columns.tolist() + \\\n",
    "                           conf_fst2.columns.tolist() + \\\n",
    "                           conf_new_eddy.columns.tolist() + \\\n",
    "                           conf_scaling.columns.tolist() + \\\n",
    "                           conf_te.columns.tolist() + \\\n",
    "                           conf_time_points.columns.tolist())\n",
    "\n",
    "# Add group of motion related confounds\n",
    "confounds.set_group('MOTION', conf_struct_head_motion.columns.tolist() + \\\n",
    "                              conf_dvars.columns.tolist() + \\\n",
    "                              conf_head_motion.columns.tolist() + \\\n",
    "                              conf_head_motion_st.columns.tolist())\n",
    "                    \n",
    "# Add group of table position related confounds\n",
    "confounds.set_group('TABLE', conf_table_pos.columns.tolist() + \\\n",
    "                             conf_eddy_qc.columns.tolist())\n",
    "\n",
    "\n",
    "# Delete previous dataframes\n",
    "del conf_site, categorical_IDPs, continuous_IDPs, conf_age_sex\n",
    "\n",
    "# Return the confounds\n",
    "return(confounds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
