{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from lib.script_01_00 import generate_initial_variables\n",
    "from lib.script_01_01 import generate_raw_confounds\n",
    "from lib.script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from src.nets import nets_load_match, nets_inverse_normal, nets_normalise, nets_demean, nets_deconfound\n",
    "from src.duplicate import duplicate_categorical, duplicate_demedian_norm_by_site\n",
    "from src.preproc import datenum, days_in_year\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eca70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, as_completed\n",
    "from lib.script_01_05 import func_01_05_gen_nonlin_conf\n",
    "\n",
    "cluster_cfg = {'cluster_type':'slurm','num_nodes':3}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Handle empty configuration\n",
    "# --------------------------------------------------------------------------------\n",
    "if cluster_cfg is None:\n",
    "\n",
    "    # Set new local configuration\n",
    "    cluster_cfg = {'cluster_type':'local','num_nodes':1}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Set up cluster\n",
    "# --------------------------------------------------------------------------------\n",
    "if 'cluster_type' in cluster_cfg:\n",
    "\n",
    "    # Check if we are using a HTCondor cluster\n",
    "    if cluster_cfg['cluster_type'].lower() == 'htcondor':\n",
    "\n",
    "        # Load the HTCondor Cluster\n",
    "        from dask_jobqueue import HTCondorCluster\n",
    "        cluster = HTCondorCluster()\n",
    "\n",
    "    # Check if we are using an LSF cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'lsf':\n",
    "\n",
    "        # Load the LSF Cluster\n",
    "        from dask_jobqueue import LSFCluster\n",
    "        cluster = LSFCluster()\n",
    "\n",
    "    # Check if we are using a Moab cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'moab':\n",
    "\n",
    "        # Load the Moab Cluster\n",
    "        from dask_jobqueue import MoabCluster\n",
    "        cluster = MoabCluster()\n",
    "\n",
    "    # Check if we are using a OAR cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'oar':\n",
    "\n",
    "        # Load the OAR Cluster\n",
    "        from dask_jobqueue import OARCluster\n",
    "        cluster = OARCluster()\n",
    "\n",
    "    # Check if we are using a PBS cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'pbs':\n",
    "\n",
    "        # Load the PBS Cluster\n",
    "        from dask_jobqueue import PBSCluster\n",
    "        cluster = PBSCluster()\n",
    "\n",
    "    # Check if we are using an SGE cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'sge':\n",
    "\n",
    "        # Load the SGE Cluster\n",
    "        from dask_jobqueue import SGECluster\n",
    "        cluster = SGECluster()\n",
    "\n",
    "    # Check if we are using a SLURM cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'slurm':\n",
    "\n",
    "        # Load the SLURM Cluster\n",
    "        from dask_jobqueue import SLURMCluster\n",
    "        cluster = SLURMCluster()\n",
    "\n",
    "    # Check if we are using a local cluster\n",
    "    elif cluster_cfg['cluster_type'].lower() == 'local':\n",
    "\n",
    "        # Load the Local Cluster\n",
    "        from dask.distributed import LocalCluster\n",
    "        cluster = LocalCluster()\n",
    "\n",
    "    # Raise a value error if none of the above\n",
    "    else:\n",
    "        raise ValueError('The cluster type, ' + cluster_cfg['cluster_type'] + ', is not recognized.')\n",
    "\n",
    "else:\n",
    "    # Raise a value error if the cluster type was not specified\n",
    "    raise ValueError('Please specify \"cluster_type\" in the cluster configuration.')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Connect to client\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Connect to cluster\n",
    "client = Client(cluster)   \n",
    "\n",
    "# Read in number of nodes we need\n",
    "num_nodes = int(cluster_cfg['num_nodes'])\n",
    "\n",
    "# Scale the cluster\n",
    "cluster.scale(num_nodes)\n",
    "\n",
    "# Get the dashboard link\n",
    "dashboard_link = client.cluster.dashboard_link\n",
    "dashboard_port = dashboard_link.split(':')[-1].split('/')[0]\n",
    "\n",
    "print(\"Dask is now running at the following address on your cluster: \" + \n",
    "      dashboard_link + \". If you wish to run locally, port \" + str(dashboard_port) +\n",
    "      \" to your machine. e.g. run something like: \\n \\n ssh -L \"+ str(dashboard_port) + ':localhost:' +\n",
    "      str(dashboard_port) + \" username@cluster_address \\n \\nand then navigate to http://localhost:\" + \n",
    "      str(dashboard_port) +\"/status to view the console.\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Run cluster jobs\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Get the number of IDPs\n",
    "num_IDPs = IDPs_deconf.shape[1]\n",
    "\n",
    "# Empty futures list\n",
    "futures = []\n",
    "\n",
    "# Submit jobs\n",
    "for i in np.arange(num_IDPs):\n",
    "\n",
    "    # Run the i^{th} job.\n",
    "    future_i = client.submit(func_01_05_gen_nonlin_conf, \n",
    "                             data_dir, out_dir, i, nonlinear_confounds, \n",
    "                             IDPs_deconf, pure=False)\n",
    "\n",
    "    # Append to list \n",
    "    futures.append(future_i)\n",
    "\n",
    "# Completed jobs\n",
    "completed = as_completed(futures)\n",
    "\n",
    "# Wait for results\n",
    "for i in completed:\n",
    "    i.result()\n",
    "\n",
    "# Delete the future objects (NOTE: This is important! If you don't delete the \n",
    "# futures dask tries to rerun them every time you call the result function).\n",
    "del i, completed, futures, future_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577465c-ef34-45ea-9419-decb8129b34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccb8a9-4ac5-4709-863b-316ccc20e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7878c4-f81b-49e9-9a5c-d25b2ca5ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.nets.nets_load_match import nets_load_match \n",
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Load the subject ids\n",
    "sub_ids = np.loadtxt(os.path.join(data_dir, 'subj.txt'), dtype=int)\n",
    "\n",
    "# dtypes for fmrib info\n",
    "dtypes = {i: 'float32' for i in range(6)}\n",
    "dtypes[0] = 'int32'\n",
    "dtypes[1] = 'int32'\n",
    "dtypes[4] = 'int32'\n",
    "\n",
    "# Read in info \n",
    "fmrib_info = nets_load_match(os.path.join(data_dir, 'ID_initial_workspace.txt'), sub_ids, dtypes=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c1b3c-822c-4b9b-a09c-dcc96da6a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmrib_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63309fc3-e915-4ed3-a496-cceafdfe1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10,10)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721798bd-2239-489a-9474-950355d03c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
