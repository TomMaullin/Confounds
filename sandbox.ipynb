{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal\n",
    "\n",
    "from src.preproc.datenum import datenum\n",
    "from src.preproc.days_in_year import days_in_year\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82731372-cb80-4dca-aaec-ce4671396009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e28b18-6668-4814-b992-8f08cd182a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read subject IDs in and exclude those that have been dropped\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Load the subject ids\n",
    "sub_ids = np.loadtxt(os.path.join(data_dir, 'subj.txt'), dtype=int)\n",
    "\n",
    "print(sub_ids.shape)\n",
    "\n",
    "# Load the subject ids to be excluded\n",
    "sub_ids_to_exclude = np.loadtxt(os.path.join(data_dir, 'excluded_subjects.txt'), dtype=int)\n",
    "\n",
    "# Find the common elements between the two arrays\n",
    "common_ids = np.intersect1d(sub_ids, sub_ids_to_exclude)\n",
    "\n",
    "# Remove the common elements from sub_ids\n",
    "sub_ids = np.setdiff1d(sub_ids, common_ids)\n",
    "\n",
    "print(sub_ids.shape)\n",
    "\n",
    "# Clean workspace\n",
    "del sub_ids_to_exclude, common_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b0de2-d136-436f-98e1-0a1a613a6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read all IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# dtypes for IDPs\n",
    "dtypes = {i: 'float64' for i in range(893)}\n",
    "dtypes[0] = 'int64'\n",
    "dtypes[892] = 'int64'\n",
    "\n",
    "# Load the IDPs\n",
    "all_IDPs = nets_load_match(os.path.join(data_dir, 'IDPs.txt'), sub_ids, dtypes=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e98547-2b7d-4a76-9de3-65b6bb8a880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find subjects with T1\n",
    "t1_subs = ~all_IDPs.iloc[:, 16].isna()\n",
    "\n",
    "# Remove non-T1 subs from IDPs and subject ids\n",
    "all_IDPs = all_IDPs.loc[t1_subs]\n",
    "sub_ids = sub_ids[t1_subs]\n",
    "\n",
    "print(all_IDPs.shape)\n",
    "print(sub_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf4d3e-7871-4082-958b-2a21043547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the number of rows in ALL_IDs\n",
    "n = len(sub_ids)\n",
    "\n",
    "# Clean workspace\n",
    "del t1_subs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d137908-7a1e-4d9f-8b25-418acd9b4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read fmrib info and compute time stamps\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# dtypes for fmrib info\n",
    "dtypes = {i: 'float64' for i in range(6)}\n",
    "dtypes[0] = 'int64'\n",
    "dtypes[1] = 'int64'\n",
    "dtypes[4] = 'int64'\n",
    "\n",
    "# Read in info \n",
    "fmrib_info = nets_load_match(os.path.join(data_dir, 'ID_initial_workspace.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# Get the acquisition time\n",
    "time_stamp_h = np.floor(fmrib_info.iloc[:, 1] / 10000)\n",
    "time_stamp_m = np.floor((fmrib_info.iloc[:, 1] - time_stamp_h * 10000) / 100)\n",
    "time_stamp_s = fmrib_info.iloc[:, 1] - time_stamp_h * 10000 - time_stamp_m * 100\n",
    "\n",
    "# Convert time of day to \"decimal\" hours\n",
    "fmrib_info.iloc[:, 1] = time_stamp_h + time_stamp_m / 60 + time_stamp_s / 3600\n",
    "\n",
    "# Get the fraction of the day when the subject was acquired\n",
    "day_fraction = (fmrib_info.iloc[:, 1] - 7) / 13\n",
    "\n",
    "# Get acquisition date\n",
    "time_stamp_y = np.floor(fmrib_info.iloc[:, 0] / 10000)\n",
    "time_stamp_m = np.floor((fmrib_info.iloc[:, 0] - time_stamp_y * 10000) / 100)\n",
    "time_stamp_d = fmrib_info.iloc[:, 0] - time_stamp_y * 10000 - time_stamp_m * 100\n",
    "\n",
    "# Convert scan date to \"decimal\" years\n",
    "dates = [datenum(int(y), int(m), int(d)) for y, m, d in zip(time_stamp_y, time_stamp_m, time_stamp_d)]\n",
    "days_since_year_start = np.array([(date - datenum(int(y), 1, 1)) for date, y in zip(dates, time_stamp_y)])\n",
    "\n",
    "# Output decimal years (note we need change datatype for compatibility\n",
    "fmrib_info = fmrib_info.astype({1: 'float64'})\n",
    "fmrib_info.iloc[:, 0] = time_stamp_y + days_since_year_start / days_in_year(time_stamp_y)\n",
    "\n",
    "# Calculate the discrete and continuous scan date (that is scan date given to the nearest day vs\n",
    "# to the nearest second)\n",
    "scan_date = fmrib_info.iloc[:, 0]\n",
    "scan_date_cont = time_stamp_y + (days_since_year_start + day_fraction) / days_in_year(time_stamp_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb88d5-e964-47c1-873a-9ecbce4c9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean workspace\n",
    "del time_stamp_h, time_stamp_m, time_stamp_s, time_stamp_d, time_stamp_y\n",
    "del dates, days_since_year_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd84bc-1812-444c-9527-82bd5526f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read resting state IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# dtypes for noise 25\n",
    "dtypes = {i: 'float64' for i in range(22)}\n",
    "dtypes[0] = 'int64'\n",
    "\n",
    "# dtypes for noise 100\n",
    "dtypes = {i: 'float64' for i in range(56)}\n",
    "dtypes[0] = 'int64'\n",
    "\n",
    "# Read in node amplitudes\n",
    "node_amps_25 = nets_load_match(os.path.join(data_dir, 'rfMRI_d25_NodeAmplitudes_v1.txt'), sub_ids, dtypes=dtypes)\n",
    "node_amps_100 = nets_load_match(os.path.join(data_dir, 'rfMRI_d100_NodeAmplitudes_v1.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# dtypes for noise 100\n",
    "dtypes = {i: 'float64' for i in range(211)}\n",
    "dtypes[0] = 'int64'\n",
    "\n",
    "# Read in partial correlation network IDPs\n",
    "net_25 = nets_load_match(os.path.join(data_dir, 'rfMRI_d25_partialcorr_v1.txt'), sub_ids, dtypes=dtypes)\n",
    "net_100 = nets_load_match(os.path.join(data_dir, 'rfMRI_d100_partialcorr_v1.txt'), sub_ids, dtypes=dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee7b7d-7814-4278-8af1-4afd200a7761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in FS IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "# dtypes for FS\n",
    "dtypes = {i: 'float64' for i in range(1274)}\n",
    "dtypes[0] = 'int64'\n",
    "\n",
    "FS = nets_load_match(os.path.join(data_dir, 'FS_IDPs.txt'), sub_ids, dtypes=dtypes)\n",
    "FS_use_T2 = FS.iloc[:, 0]  # Get the first column\n",
    "FS = FS.iloc[:, 1:]  # Get the rest of the columns except the first\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e29993-5e71-4a5e-91f9-3dea6e78f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in ASL IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "# dtypes for ASL\n",
    "dtypes = {i: 'float64' for i in range(51)}\n",
    "dtypes[0] = 'int64'\n",
    "\n",
    "ASL = nets_load_match(os.path.join(data_dir, 'ASL_IDPs.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in QSM IDPs\n",
    "# ----------------------------------------------------------------------------------\n",
    "# dtypes for QSM\n",
    "dtypes = {i: 'float64' for i in range(19)}\n",
    "dtypes[0] = 'int64'\n",
    "\n",
    "QSM = nets_load_match(os.path.join(data_dir, 'QSM_IDPs.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in WMH \n",
    "# ----------------------------------------------------------------------------------\n",
    "# dtypes for QSM\n",
    "dtypes = {i: 'float64' for i in range(3)}\n",
    "dtypes[0] = 'int64'\n",
    "\n",
    "WMH = nets_load_match(os.path.join(data_dir, 'ID_WMH.txt'), sub_ids, dtypes=dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18994ee4-4f57-4169-b578-61f04f2d9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in IDP names\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Assuming the file is space or tab delimited. \n",
    "df = pd.read_csv(os.path.join(data_dir, \"IDPinfo.txt\"), sep='\\t', usecols=[0], header=0)\n",
    "\n",
    "# Remove the first name as this is the subject index column, which we will delete\n",
    "IDP_names = df.values\n",
    "IDP_names = list(IDP_names.reshape(np.prod(IDP_names.shape)))\n",
    "\n",
    "# Clean up\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045651d-1344-4f1e-8681-1abba06674f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add IDP names for resting state ICA\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Append rfMRI amplitudes (ICA25 nodes)\n",
    "IDP_names.extend([f'rfMRI amplitudes (ICA25 node {i})' for i in range(1, node_amps_25.shape[1] + 1)])\n",
    "\n",
    "# Append rfMRI amplitudes (ICA100 nodes)\n",
    "IDP_names.extend([f'rfMRI amplitudes (ICA100 node {i})' for i in range(1, node_amps_100.shape[1] + 1)])\n",
    "\n",
    "# Append rfMRI connectivity (ICA25 edges)\n",
    "IDP_names.extend([f'rfMRI connectivity (ICA25 edge {i})' for i in range(1, net_25.shape[1] + 1)])\n",
    "\n",
    "# Append rfMRI connectivity (ICA100 edges)\n",
    "IDP_names.extend([f'rfMRI connectivity (ICA100 edge {i})' for i in range(1, net_100.shape[1] + 1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ffbb0-6a9e-4a2e-a755-6730ad84d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add IDP names for for FS, ASL, QSM and QC\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Read FS_names.txt\n",
    "with open(f\"{data_dir}/FS_names.txt\", 'r') as file:\n",
    "    FS_names = file.read().splitlines()\n",
    "\n",
    "# Exclude the first name\n",
    "FS_names = FS_names[1:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6f262-93b3-4dfc-bc92-29ef4f582edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ASL_names.txt\n",
    "with open(f\"{data_dir}/ASL_names.txt\", 'r') as file:\n",
    "    ASL_names = file.read().splitlines()\n",
    "\n",
    "# Read QSM_names.txt\n",
    "with open(f\"{data_dir}/QSM_names.txt\", 'r') as file:\n",
    "    QSM_names = file.read().splitlines()\n",
    "\n",
    "# Add the 2 IDP names from WMH\n",
    "WMH_names =['IDP_T2_FLAIR_BIANCA_periventWMH_volume',\n",
    "            'IDP_T2_FLAIR_BIANCA_deepWMH_volume']\n",
    "\n",
    "# Add to running IDP names\n",
    "IDP_names = IDP_names + FS_names + ASL_names + QSM_names + WMH_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805155bd-d7e9-4012-8d17-79b86aa8da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# QC IDPs and names\n",
    "QC_IDPs = all_IDPs.iloc[:, 0:16]\n",
    "QC_IDPs_names = IDP_names[0:16]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# IDPs to exclude vs include\n",
    "# ----------------------------------------------------------------------------------\n",
    "ind_IDPs_to_exclude = list(range(1, 18)) + list(range(888, 893))\n",
    "ind_IDPs_to_include = np.setdiff1d(np.arange(all_IDPs.shape[1]), np.array(ind_IDPs_to_exclude) - 1)\n",
    "\n",
    "# Select the specified columns from all_IDPs\n",
    "subset_all_IDPs = all_IDPs.iloc[:, ind_IDPs_to_include]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de870436-cf2b-49a1-896e-d4eb77324962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Concatenate all the DataFrames/Series horizontally\n",
    "subset_IDPs = pd.concat([\n",
    "    subset_all_IDPs.reset_index(drop=True), \n",
    "    node_amps_25.reset_index(drop=True),\n",
    "    node_amps_100.reset_index(drop=True),\n",
    "    net_25.reset_index(drop=True),\n",
    "    net_100.reset_index(drop=True),\n",
    "    FS.reset_index(drop=True),\n",
    "    ASL.reset_index(drop=True),\n",
    "    QSM.reset_index(drop=True),\n",
    "    WMH.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Update the included indices to include the node_amps_25, node_amps_100, etc.\n",
    "ind_names_to_include = np.hstack((ind_IDPs_to_include + 1, range(893, len(IDP_names) + 1))) - 1  \n",
    "\n",
    "# Construct IDP_names by subsetting\n",
    "IDP_names = [IDP_names[i] for i in ind_names_to_include]\n",
    "\n",
    "# Add names to all_IDPs\n",
    "subset_IDPs.columns = IDP_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bdc437-0a85-45c9-9c67-974dbea232f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Outlier detection. \n",
    "# ----------------------------------------------------------------------------------\n",
    "# Below is a description of what is happening here, taken from \"Confound modelling \n",
    "# in UK Biobank brain imaging\":\n",
    "#\n",
    "# For any given confound, we define outliers thus: First we subtract the median\n",
    "# value from all subjects’ values. We then compute the median-absolutedeviation \n",
    "# (across all subjects) and multiply this MAD by 1.48 (so that it is equal to \n",
    "# the standard deviation if the data had been Gaussian). We then normalise all \n",
    "# values by dividing them by this scaled MAD. Finally, we define values as \n",
    "# outliers if their magnitude is greater than 8.\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Subtract the median, ignoring NaNs\n",
    "subset_IDPs_m = subset_IDPs - np.nanmedian(subset_IDPs, axis=0)\n",
    "\n",
    "# Calculate the median absolute deviation, again ignoring NaNs\n",
    "medabs = np.nanmedian(np.abs(subset_IDPs_m), axis=0)\n",
    "\n",
    "# np.finfo(float).eps is machine epsilon for float64\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "# Get a mask for the absolute median\n",
    "low_medabs_mask = medabs < eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc02ead-62e0-45e3-b8aa-33a29b93459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardise the non-zero medians\n",
    "if np.any(low_medabs_mask):\n",
    "    medabs[low_medabs_mask] = np.nanstd(subset_IDPs_m.iloc[:, low_medabs_mask], axis=0) / 1.48\n",
    "\n",
    "\n",
    "# Divide by medabs\n",
    "subset_IDPs_m = subset_IDPs_m / medabs\n",
    "\n",
    "# Set values with absolute value greater than 5 to NaN\n",
    "subset_IDPs_m[np.abs(subset_IDPs_m) > 5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c95a5-e6ab-4050-a0af-a88c2d4039d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Quartile Normalisation of IDPS\n",
    "# ----------------------------------------------------------------------------------\n",
    "IDPs = nets_inverse_normal(subset_IDPs_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc662d7b-2829-4651-9784-b56e3b91a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Non-IDPS\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read general names\n",
    "with open(os.path.join(data_dir, 'OTHER_GENERAL_names.txt'), 'r') as file:\n",
    "    gen_names = [line.strip() for line in file]\n",
    "\n",
    "# Datatypes for this file (general variables have a mix so best specify)\n",
    "dtypes = {0: 'int64', 1: 'float64', 2: 'float64', 3: 'float64', 4: 'object', 5: 'object', 6: 'object', 7: 'object'}\n",
    "\n",
    "# General variables\n",
    "gen_vars = nets_load_match(os.path.join(data_dir, 'ID_OTHER_GENERAL.txt'), sub_ids, dtypes=dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0089085-46e5-4f34-bafd-2d13eb6a85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Work out scan dates\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Replace NaN in scan_date with the maximum value plus 0.1\n",
    "scan_date = scan_date.values\n",
    "scan_date[np.isnan(scan_date)] = np.nanmax(scan_date) + 0.1\n",
    "scan_date = pd.Series(scan_date)\n",
    "\n",
    "# Extract sex, year of birth (yob) and month of birth (mob) from GEN_vars\n",
    "sex = gen_vars.iloc[:, 0]\n",
    "yob = gen_vars.iloc[:, 1]\n",
    "mob = gen_vars.iloc[:, 2]\n",
    "\n",
    "# Calculate birth_date as year plus the adjusted month value divided by 12\n",
    "birth_date = yob + (mob - 0.5) / 12\n",
    "\n",
    "# Calculate age by subtracting birth_date from scan_date\n",
    "age = scan_date - birth_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc75dfe-2347-46f8-93a3-d0b815b15208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Write out age, sex and head-size\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Create a Pandas DataFrame to organize the data\n",
    "nonIDPs = pd.DataFrame({\n",
    "    'ID': sub_ids,\n",
    "    'AGE': age,\n",
    "    'SEX': sex,\n",
    "    'HEADSIZE': np.where(np.isnan(all_IDPs.iloc[:, 17]), np.nan, all_IDPs.iloc[:, 16]),\n",
    "    'TOD': day_fraction,\n",
    "    'FST2': FS_use_T2\n",
    "})\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac27910-1370-49cb-82ce-d9c8512353b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a mapping between column names and file names\n",
    "column_file_mapping = {'AGE': 'ID_AGE.txt',\n",
    "                       'SEX': 'ID_SEX.txt',\n",
    "                       'HEADSIZE': 'ID_HEADSIZE.txt',\n",
    "                       'TOD': 'ID_TOD.txt',\n",
    "                       'FST2': 'ID_FST2.txt'}\n",
    "\n",
    "# Save each dataframe column to a separate text file\n",
    "for col_name in column_file_mapping.keys():\n",
    "\n",
    "    # Get the filepath\n",
    "    file_path = os.path.join(out_dir, column_file_mapping[col_name])\n",
    "\n",
    "    # Remove previous file if needed\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "    # Save column\n",
    "    nonIDPs[['ID', col_name]].to_csv(file_path, sep=' ', index=False, header=False, na_rep='NaN')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854f73e-bdb7-49de-87fc-45fc315a9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Read in Eddy currents and tablepos (currently unused)\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Define the directory paths\n",
    "workspaces_dir = os.path.join(out_dir, 'workspaces', 'ws_00')\n",
    "figs_dir = os.path.join(out_dir, 'figs', 'EDDYQC')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(workspaces_dir, exist_ok=True)\n",
    "\n",
    "# Datatypes for ed\n",
    "dtypes = {0: 'int64', 1: 'float64', 2: 'float64', 3: 'float64', 4: 'object'}\n",
    "ed = nets_load_match(os.path.join(data_dir, 'ID_EDDYQC.txt'), sub_ids, dtypes=dtypes)\n",
    "\n",
    "# Datatypes for ta\n",
    "dtypes = {0: 'int64', 1: 'float64', 2: 'float64', 3: 'float64'}\n",
    "ta = nets_load_match(os.path.join(data_dir, 'ID_TABLEPOS.txt'), sub_ids, dtypes=dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a12b2e-a0c0-45d6-9a29-4c7dd95ee4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.iloc[0:4,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fbaa7-c5fc-460c-8bde-0f57a1e05c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Sort data\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Sort scan_date and get the sorted indices\n",
    "index_sorted_date = np.argsort(scan_date)\n",
    "sorted_date = scan_date.iloc[index_sorted_date]\n",
    "\n",
    "# Sort IDPs\n",
    "IDPs = IDPs.iloc[index_sorted_date,:]\n",
    "nonIDPs = nonIDPs.iloc[index_sorted_date,:]\n",
    "\n",
    "# Reorder subject ids based on sorted indices\n",
    "sub_ids = sub_ids[index_sorted_date]\n",
    "\n",
    "# Set row indices on dataframes\n",
    "IDPs.index = sub_ids\n",
    "nonIDPs.index = sub_ids\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Miscellaneous variables (this houses any variables that I'm unsure are used)\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Get the general variables that we haven't saved elsewhere\n",
    "misc = gen_vars.iloc[index_sorted_date, 1:]\n",
    "\n",
    "# Read general names\n",
    "with open(f\"{data_dir}/OTHER_GENERAL_names.txt\", 'r') as file:\n",
    "    gen_names = file.read().splitlines()\n",
    "    \n",
    "# Set column names\n",
    "misc.columns = gen_names[1:]\n",
    "misc.index = sub_ids\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Output memmaps\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Return IDPs dataframe\n",
    "IDPs = MemoryMappedDF(IDPs)\n",
    "\n",
    "# Return non-IDPs dataframe\n",
    "nonIDPs = MemoryMappedDF(nonIDPs)\n",
    "\n",
    "# Return miscellaneous dataframe\n",
    "misc = MemoryMappedDF(misc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
