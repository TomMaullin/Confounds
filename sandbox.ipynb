{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import shutil  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "\n",
    "from lib.script_01_00 import generate_initial_variables\n",
    "from lib.script_01_01 import generate_raw_confounds\n",
    "from lib.script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal \n",
    "from src.nets.nets_normalise import nets_normalise \n",
    "from src.nets.nets_demean import nets_demean\n",
    "# from src.nets.nets_deconfound import nets_deconfound\n",
    "\n",
    "from src.duplicate.duplicate_categorical import duplicate_categorical\n",
    "from src.duplicate.duplicate_demedian_norm_by_site import duplicate_demedian_norm_by_site\n",
    "\n",
    "from src.preproc.datenum import datenum\n",
    "from src.preproc.switch_type import switch_type\n",
    "from src.preproc.days_in_year import days_in_year\n",
    "from src.preproc.filter_columns_by_site import filter_columns_by_site\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "from src.memmap.addBlockToMmap import addBlockToMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82731372-cb80-4dca-aaec-ce4671396009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2982046-cd27-46f4-a701-72855e94cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in precomputed memmaps\n",
    "IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "IDPs_deconf = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz'))\n",
    "nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))\n",
    "confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))\n",
    "#nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz'))\n",
    "p1 = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','p.npz'))\n",
    "nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds_reduced.npz'))\n",
    "IDPs_deconf_ct = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf_ct.npz'))\n",
    "confounds_with_ct = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds_with_ct.npz'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf241c-27c4-42cf-99e0-a6348652b157",
   "metadata": {},
   "source": [
    "## draft 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd41b4-6b21-4d4c-b5de-c9479571a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.nets.nets_normalise import nets_normalise\n",
    "from src.nets.nets_deconfound_multiple import nets_deconfound_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78af12c-2c16-464f-9a5d-97ace87f1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set cluster configuration\n",
    "cluster_cfg = {'cluster_type':'slurm','num_nodes':100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc44b51-fe48-4c0a-a39e-6127beff020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names directory\n",
    "names_dir = os.path.join(data_dir, '..', 'NAMES_confounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f650e-1b0f-45d3-b650-c9a48da9d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the IDPs and confounds we need\n",
    "IDPs = IDPs\n",
    "confounds = confounds_with_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145e978-bc82-463d-9528-475fbd5c8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the subject IDs\n",
    "sub_ids = IDPs.index\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Estimate the block size (number of subjects we want to allow in memory at\n",
    "# a given time).\n",
    "# -------------------------------------------------------------------------\n",
    "# Developer note: The below is only a rough estimate, but is fairly robust\n",
    "# as it is a little conservative. The rule of thumb is to take the maximum\n",
    "# amount of memory (MAXMEM) divide it by the number of subjects we have,\n",
    "# divide by 64 (as each observation is float64 at most) and then divide by\n",
    "# 8 (as we may want to make several copies of whatever we load in, but we\n",
    "# rarely make more than 8). The resulting number should be roughly the\n",
    "# number of columns of a dataframe we are able to load in at a time. This\n",
    "# doesn't need to be perfect as often python can handle more - it is just\n",
    "# a precaution, and does improve efficiency substantially.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Rough estimate of maximum memory (bytes)\n",
    "MAXMEM = 2**32\n",
    "\n",
    "# Get the number of subjects\n",
    "n_sub = len(sub_ids)\n",
    "\n",
    "# Block size computation\n",
    "blksize = int(MAXMEM/n_sub/8/64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2741bed-e16a-42b3-bc39-ddfdb740fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Deconfound IDPs\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Switch type to reduce transfer costs\n",
    "confounds = switch_type(confounds, out_type='filename')\n",
    "IDPs = switch_type(IDPs, out_type='filename')\n",
    "\n",
    "# Deconfound IDPs\n",
    "IDPs_deconf = nets_deconfound_multiple(IDPs, confounds, 'nets_svd', \n",
    "                                       blksize=blksize, coincident=False,\n",
    "                                       cluster_cfg=cluster_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5319f-0217-46f3-84d2-ee315f651491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Switch IDPs back\n",
    "IDPs = switch_type(IDPs, out_type='pandas') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eed04e-9400-45f0-94dc-f18bf14e9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get day fraction (time of day)\n",
    "day_fraction = nonIDPs[:,'TOD']\n",
    "\n",
    "# Normalise day fraction\n",
    "conf_acq_time_linear = nets_normalise(day_fraction)\n",
    "conf_acq_time_linear = conf_acq_time_linear.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e62736-52db-406f-9cd2-28147736e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_acq_time_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025e140-d578-4a6e-8a0d-506fba7d7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    conf_acq_time_linear = conf_acq_time_linear.sort_values(by='TOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c2178-ebc1-494f-b6c7-7d4c2d2b65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Get sorted indices\n",
    "    sub_ids_sorted = conf_acq_time_linear.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c0bef-9c8c-4f1f-b4f0-e9e9ca01730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort IDPs and IDPs_deconf based on sorted sub_ids\n",
    "IDPs_sorted = IDPs.loc[sub_ids_sorted,:]\n",
    "IDPs_deconf_sorted = IDPs_deconf.loc[sub_ids_sorted,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f796596-00c5-4f64-a4d8-bd6e78183bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "site_ids.index = sub_ids\n",
    "site_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d28c6-0cad-4e8e-884a-1bb4b374a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sort site ids\n",
    "site_ids_sorted = site_ids.loc[sub_ids_sorted,:]\n",
    "site_ids_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8ad16-abe2-4772-a042-70f833d1ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site_sorted = {}\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in (unique_site_ids + 1):\n",
    "\n",
    "    # Find the indices where all elements in a row of siteDATA match the current valueSite\n",
    "    # Note: This assumes siteDATA and siteValues have compatible shapes or values for comparison\n",
    "    indices = np.where((site_ids_sorted == site_id-1).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site_sorted[site_id] = indices\n",
    "\n",
    "# Delete the indices\n",
    "del indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6758a4-cbe7-44cc-bd1e-a16783da3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sites = len(inds_per_site_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b3ef4-7419-4243-b401-9f53a1aae0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigma value\n",
    "sigma = 0.1\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f8935-da6b-4852-9e77-9e2c509ddfd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loop through sites\n",
    "# for site_id in inds_per_site_sorted:\n",
    "\n",
    "#     print('Running site ', str(site_id))\n",
    "#     t1_total = time.time()\n",
    "\n",
    "#     # Get subjects for this site\n",
    "#     inds_site = inds_per_site_sorted[site_id]\n",
    "\n",
    "#     # Get the IDPs for this site\n",
    "#     IDPs_for_site = IDPs_deconf_sorted.iloc[inds_site,:]\n",
    "\n",
    "#     # Initialise smoothed IDPs for this site\n",
    "#     smoothed_IDPs_for_site = pd.DataFrame(np.zeros(IDPs_for_site.shape),\n",
    "#                                           index=IDPs_for_site.index,\n",
    "#                                           columns=IDPs_for_site.columns)\n",
    "\n",
    "#     # Get IDPs for site as numpy array\n",
    "#     IDPs_for_site = IDPs_for_site.values\n",
    "\n",
    "#     # Loop through subjects within site\n",
    "#     for j, sub_id in enumerate(inds_site):\n",
    "\n",
    "#         print('Iteration ', j, '/', len(inds_site))\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Get time delta\n",
    "#         timedelta = conf_acq_time_linear.iloc[inds_site[j],:]-conf_acq_time_linear.iloc[inds_site,:]\n",
    "#         t2 = time.time()\n",
    "#         print('marker1: ', t2-t1)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Get the gaussian kernel\n",
    "#         gauss_kernel = np.exp(-0.5*((timedelta/sigma)**2))\n",
    "#         t2 = time.time()\n",
    "#         print('marker2: ', t2-t1)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Handle any potential overflow\n",
    "#         gauss_kernel[gauss_kernel.abs()<1e-10]=0\n",
    "#         t2 = time.time()\n",
    "#         print('marker3: ', t2-t1)\n",
    "\n",
    "        \n",
    "#         t1 = time.time()\n",
    "#         # Get the numerator and denominator for smoothing\n",
    "#         numerator = np.nansum(IDPs_for_site*gauss_kernel.values,axis=0)\n",
    "#         t2 = time.time()\n",
    "#         print('marker3.5: ', t2-t1)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         denominator = np.sum((1*~np.isnan(IDPs_for_site))*gauss_kernel.values,axis=0)\n",
    "#         t2 = time.time()\n",
    "#         print('marker4: ', t2-t1)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         # Smoothed IDPs for site\n",
    "#         smoothed_IDPs_for_site.iloc[j, :] = numerator/denominator\n",
    "#         t2 = time.time()\n",
    "#         print('marker5: ', t2-t1)\n",
    "\n",
    "#         print(numerator/denominator)\n",
    "\n",
    "    \n",
    "#     t2_total = time.time()\n",
    "#     print('Done site ', str(site_id))\n",
    "#     print('Time elapsed: ', t2_total-t1_total)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa5aaf-75c3-41bc-b57e-38dc40217e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nets.nets_smooth_multiple import nets_smooth_multiple\n",
    "from src.nets.nets_svd import nets_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa861d4b-b29b-48c0-b81f-577cee631a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time points per block, no 8 is included here as\n",
    "# we only ever construct the relevant matrix once in \n",
    "# nets_smooth_single (this is controlling the size of\n",
    "# the xeval*xdata matrix)\n",
    "blksize_time = int(MAXMEM/IDPs.shape[0]/64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135eed0b-c306-4b6c-97aa-32ed3e8961c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# Dict to store smoothed IDPs and pca results\n",
    "smoothed_IDPs_sorted_dict = {}\n",
    "principal_components_sorted_dict = {}\n",
    "esm_sorted_dict = {}\n",
    "\n",
    "# Loop through sites\n",
    "for site_id in inds_per_site_sorted:\n",
    "    \n",
    "    print('Smoothing confounds for site ', str(site_id))\n",
    "    t1_total = time.time()\n",
    "    \n",
    "    # Get subjects for this site\n",
    "    inds_site = inds_per_site_sorted[site_id]\n",
    "    \n",
    "    # Get the IDPs for this site\n",
    "    IDPs_for_site = IDPs_deconf_sorted.iloc[inds_site,:]\n",
    "    \n",
    "    # Get the acquisition times for this site\n",
    "    times_for_site = conf_acq_time_linear.iloc[inds_site,:]\n",
    "    \n",
    "    # Smooth the IDPs\n",
    "    smoothed_IDPs_for_site = nets_smooth_multiple(times_for_site, IDPs_for_site, sigma,\n",
    "                                                  blksize=blksize, blksize_time=blksize_time,\n",
    "                                                  cluster_cfg=cluster_cfg)\n",
    "\n",
    "    print('Confounds smoothed for site ', str(site_id))\n",
    "\n",
    "    # Compute svd of IDPs\n",
    "    principal_components_sorted, esm,_ = nets_svd(smoothed_IDPs_for_site.values)\n",
    "\n",
    "    # Save results\n",
    "    smoothed_IDPs_sorted_dict[site_id] = smoothed_IDPs_for_site\n",
    "    principal_components_sorted_dict[site_id] = principal_components_sorted\n",
    "    esm_sorted_dict[site_id] = esm\n",
    "\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6e112-d78a-4a6a-bf35-6a9818eae96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(IDPs_deconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5274b-be01-49b0-b557-e78b46b17fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nets.nets_normalise import nets_normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed99ef5-e1a8-41e3-8f5d-c364a5648629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Estimating the number of temporal components by choosing a number\n",
    "# that explains at least 99% of the variance in the smoothed IDPs.\n",
    "num_temp_comp_sorted = {}\n",
    "conf_acq_time_dict = {}\n",
    " \n",
    "# Loop through sites\n",
    "for site_id in principal_components_sorted_dict:\n",
    "\n",
    "    # Maximum variance explained\n",
    "    max_ve = 0\n",
    "\n",
    "    # Get the principal components for this site\n",
    "    principal_components_site = principal_components_sorted_dict[site_id]\n",
    "\n",
    "    # Get the smoothed IDPs for this site\n",
    "    smoothed_IDPs_site = smoothed_IDPs_sorted_dict[site_id]\n",
    "\n",
    "    # Record index\n",
    "    site_index = smoothed_IDPs_site.index\n",
    "    \n",
    "    # Current number of principal components that we have considered\n",
    "    n_current = 1\n",
    "\n",
    "    # Get columns of rows with all non-nan values\n",
    "    non_nan_rows = ~smoothed_IDPs_site.isna().any(axis=1)\n",
    "\n",
    "    # Filter principal components and smoothed IDPs row wise\n",
    "    principal_components_site = principal_components_site[non_nan_rows.values,:]\n",
    "    smoothed_IDPs_site = smoothed_IDPs_site[non_nan_rows].values\n",
    "    \n",
    "    # Loop through principal components until we have 99% variance explained\n",
    "    while max_ve < 99:\n",
    "\n",
    "        # Get n_current principal components\n",
    "        current_pcs = principal_components_site[:,:n_current]\n",
    "            \n",
    "        # Compute variance explained in smoothed_IDPs_site by current_pcs\n",
    "        current_pcs_pinv = np.linalg.pinv(current_pcs)\n",
    "    \n",
    "        # Compute projection\n",
    "        proj = current_pcs @ (current_pcs_pinv @ smoothed_IDPs_site)\n",
    "        \n",
    "        # Compute variance explained\n",
    "        numerator = 100 * np.sum(proj.flatten() ** 2)\n",
    "        denominator = np.sum(smoothed_IDPs_site.flatten() ** 2)\n",
    "        max_ve = numerator / denominator\n",
    "\n",
    "        print(n_current, max_ve)\n",
    "        \n",
    "        # Check if max_ve is greater than 99\n",
    "        if max_ve < 99:\n",
    "\n",
    "            # Increment counter\n",
    "            n_current = n_current + 1\n",
    "\n",
    "    # Save number of components\n",
    "    num_temp_comp_sorted[site_id] = n_current\n",
    "\n",
    "    # Save new array\n",
    "    principal_components_sorted_dict[site_id] = pd.DataFrame(principal_components_site[:,:n_current],\n",
    "                                                             index=site_index)\n",
    "    conf_acq_time_dict[site_id] = nets_normalise(principal_components_sorted_dict[site_id]).fillna(0)\n",
    "\n",
    "    print('Estimated number of components for site ', site_id, ': ', n_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112baa6-eb0d-4148-8812-1ef6dc135eda",
   "metadata": {},
   "source": [
    "*Matlab had 19 for site 1, 20 for site 2, 22 for site 3 and 21 for site 4 (82 total)*\n",
    "\n",
    "*Python gives 18 for site 1, 19 for site 2, 21 for site 3 and 20 for site 4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c1d14-bb13-4c56-87bc-90f9ceae8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_id in smoothed_IDPs_sorted_dict:\n",
    "\n",
    "    print(principal_components_sorted_dict[site_id].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fcd0a-b8e8-4bd1-8384-291bbbde1a6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d6ce0-a154-43cf-bdeb-e46c3271c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct column names for temporal components\n",
    "tc_colnames = []\n",
    "\n",
    "# Loop through sites constructing colnames and converting principal components\n",
    "for site_id in principal_components_sorted_dict:\n",
    "\n",
    "    # Site columnnames \n",
    "    tc_colnames_site = ['ACQT_Site_' + str(site_id) + '__' + str(pc_id) for pc_id in range(1,num_temp_comp_sorted[site_id]+1)]\n",
    "\n",
    "    # Replace header on site specific dataframes\n",
    "    principal_components_sorted_dict[site_id].columns = tc_colnames_site\n",
    "\n",
    "    # Update running column names\n",
    "    tc_colnames = tc_colnames + tc_colnames_site\n",
    "    \n",
    "# Number of estimated components in total\n",
    "num_temp_comp_sorted_total = 0\n",
    "\n",
    "# Sum values over sites\n",
    "for site_id in num_temp_comp_sorted:\n",
    "    num_temp_comp_sorted_total = num_temp_comp_sorted_total + num_temp_comp_sorted[site_id]\n",
    "    \n",
    "# Reconstruct principal components confound dataframe\n",
    "conf_acq_time = pd.DataFrame(np.zeros((n_sub,num_temp_comp_sorted_total)),\n",
    "                             index = sub_ids_sorted,\n",
    "                             columns = tc_colnames)\n",
    "\n",
    "# Loop through sites constructing colnames and converting principal components\n",
    "for site_id in principal_components_sorted_dict:\n",
    "\n",
    "    # Add in temporal components sorted\n",
    "    conf_acq_time.update(principal_components_sorted_dict[site_id])\n",
    "\n",
    "# Convert the indexing back to the original order\n",
    "conf_acq_time = conf_acq_time.loc[sub_ids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f98de-6f8d-4cb0-bf4a-cc440965137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faba78e-d43a-40dd-bba9-971a770717d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta = conf_acq_time_linear.iloc[inds_site[j],:]-conf_acq_time_linear.iloc[inds_site,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0886a6e-027f-4876-897b-9a293f09d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_kernel = np.exp(-0.5*((timedelta/sigma)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1b76b-7ebf-49db-87d2-014b4a1e5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a pandas DataFrame 'df' with a single column\n",
    "data = gauss_kernel.iloc[:, 0]  # Get the single column as a pandas Series\n",
    "\n",
    "# Create a range of indices from 1 to the length of the data\n",
    "indices = range(1, len(data) + 1)\n",
    "\n",
    "# Plot the data against the indices\n",
    "# plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "# plt.plot(indices, data)\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Dataframe Column Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d9f8b-d792-4f12-b381-67e9ca13f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle any potential overflow\n",
    "gauss_kernel[gauss_kernel.abs()<1e-10]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cac34b-7ed4-406e-863d-8c3364bca122",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDPs_for_site = IDPs.iloc[inds_site,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6545958-bace-4fa5-97d7-5298ebbcb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(IDPs_for_site.values*gauss_kernel.values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e69d5-8cf2-47d8-bdee-9d6e1d1fb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerator and denominator for smoothing\n",
    "numerator = np.nansum(IDPs_for_site.values*gauss_kernel.values,axis=0)\n",
    "denominator = np.sum((1*~np.isnan(IDPs_for_site.values))*gauss_kernel.values,axis=0)\n",
    "\n",
    "# Get the smoothed IDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3535d-f918-490b-aec9-f63f9dd5ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf95f5-a92a-422f-b636-a1dad49d90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099d17c-a9df-4f4d-b865-db344b23f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was adapted from the below answer on stack overflow\n",
    "# https://stackoverflow.com/questions/24143320/gaussian-sum-filter-for-irregular-spaced-points\n",
    "\n",
    "def gaussian_sum_smooth(xdata, ydata, xeval, sigma, null_thresh=0.6):\n",
    "    \"\"\"Apply gaussian sum filter to data.\n",
    "    \n",
    "    xdata, ydata : array\n",
    "        Arrays of x- and y-coordinates of data. \n",
    "        Must be 1d and have the same length.\n",
    "    \n",
    "    xeval : array\n",
    "        Array of x-coordinates at which to evaluate the smoothed result\n",
    "    \n",
    "    sigma : float\n",
    "        Standard deviation of the Gaussian to apply to each data point\n",
    "        Larger values yield a smoother curve.\n",
    "    \n",
    "    null_thresh : float\n",
    "        For evaluation points far from data points, the estimate will be\n",
    "        based on very little data. If the total weight is below this threshold,\n",
    "        return np.nan at this location. Zero means always return an estimate.\n",
    "        The default of 0.6 corresponds to approximately one sigma away \n",
    "        from the nearest datapoint.\n",
    "    \"\"\"\n",
    "    # Distance between every combination of xdata and xeval\n",
    "    # each row corresponds to a value in xeval\n",
    "    # each col corresponds to a value in xdata\n",
    "    delta_x = xeval[:, None] - xdata\n",
    "\n",
    "    # Calculate weight of every value in delta_x using Gaussian\n",
    "    # Maximum weight is 1.0 where delta_x is 0\n",
    "    weights = np.exp(-0.5 * ((delta_x / sigma) ** 2))\n",
    "\n",
    "    # Multiply each weight by every data point, and sum over data points\n",
    "    smoothed = np.dot(weights, ydata)\n",
    "\n",
    "    # Nullify the result when the total weight is below threshold\n",
    "    # This happens at evaluation points far from any data\n",
    "    # 1-sigma away from a data point has a weight of ~0.6\n",
    "    nan_mask = weights.sum(1) < null_thresh\n",
    "    smoothed[nan_mask] = np.nan\n",
    "    \n",
    "    # Normalize by dividing by the total weight at each evaluation point\n",
    "    # Nullification above avoids divide by zero warning shere\n",
    "    smoothed = smoothed / weights.sum(1)\n",
    "\n",
    "    # Return result    \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578c4e2-2a84-495f-94c3-50ffe7c49321",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = conf_acq_time_linear.iloc[inds_site,:].values\n",
    "\n",
    "ydata = IDPs_for_site[:,1:10]\n",
    "\n",
    "# mask = ~np.isnan(ydata)\n",
    "\n",
    "# xdata = xdata[mask]\n",
    "xeval = np.array(xdata)\n",
    "# ydata = ydata[mask]\n",
    "\n",
    "null_thresh = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b024b-39ef-491e-b200-bf102bd49640",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.shape, xeval.shape, ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfda9f-b979-420c-b6f0-3ae03f4d0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "smoothed_IDPs=gaussian_sum_smooth(xdata, ydata, xeval, sigma, null_thresh=0.6)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74a986-7e2e-42fe-834e-dfb658181670",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_IDPs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a2ba0-56ed-47bb-b031-82bf720ee461",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.flatten().shape, ydata[:,1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955eef4-7eee-414b-9c8e-031579c0e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(smoothed_IDPs).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6c2c9-a338-4afa-a8a4-8b2a6301a234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de9098-144f-4ac4-a1e6-f0fd5017add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was adapted from the below answer on stack overflow\n",
    "# https://stackoverflow.com/questions/24143320/gaussian-sum-filter-for-irregular-spaced-points\n",
    "\n",
    "def nets_smooth(xdata, ydata, xeval, sigma, null_thresh=0.6):\n",
    "    \"\"\"Apply gaussian sum filter to data.\n",
    "    \n",
    "    xdata, ydata : array\n",
    "        Arrays of x- and y-coordinates of data. \n",
    "        xdata be have only one dimension > 1 and have the same height as ydata.\n",
    "        ydata must be two dimensional (n_obs x n_variables)\n",
    "    \n",
    "    xeval : array\n",
    "        Array of x-coordinates at which to evaluate the smoothed result\n",
    "    \n",
    "    sigma : float\n",
    "        Standard deviation of the Gaussian to apply to each data point\n",
    "        Larger values yield a smoother curve.\n",
    "    \n",
    "    null_thresh : float\n",
    "        For evaluation points far from data points, the estimate will be\n",
    "        based on very little data. If the total weight is below this threshold,\n",
    "        return np.nan at this location. Zero means always return an estimate.\n",
    "        The default of 0.6 corresponds to approximately one sigma away \n",
    "        from the nearest datapoint.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten xdata and xeval\n",
    "    xdata = xdata.flatten()\n",
    "    xeval = xeval.flatten()\n",
    "    \n",
    "    # Distance between every combination of xdata and xeval\n",
    "    # each row corresponds to a value in xeval\n",
    "    # each col corresponds to a value in xdata\n",
    "    delta_x = xeval[:, None] - xdata\n",
    "    \n",
    "    # Calculate weight of every value in delta_x using Gaussian\n",
    "    # Maximum weight is 1.0 where delta_x is 0\n",
    "    weights = np.exp(-0.5 * ((delta_x / sigma) ** 2))\n",
    "    \n",
    "    # Temporarily remove zeros from ydata\n",
    "    ydata_wo_nans = np.array(ydata)\n",
    "    ydata_wo_nans[np.isnan(ydata)]=0\n",
    "    \n",
    "    # Multiply each weight by every data point, and sum over data points\n",
    "    smoothed = weights @ ydata_wo_nans\n",
    "    \n",
    "    # Nullify the result when the total weight is below threshold\n",
    "    # This happens at evaluation points far from any data\n",
    "    # 1-sigma away from a data point has a weight of ~0.6\n",
    "    nan_mask = weights.sum(1) < null_thresh\n",
    "    smoothed[nan_mask] = np.nan\n",
    "    \n",
    "    # Normalize by dividing by the total weight at each evaluation point\n",
    "    # Nullification above avoids divide by zero warning shere\n",
    "    for k in np.arange(smoothed.shape[1]):\n",
    "        \n",
    "        # Get nan mask\n",
    "        non_nan_mask = ~np.isnan(ydata[:,k])\n",
    "        \n",
    "        # Get smoothed\n",
    "        smoothed[:,k] = smoothed[:,k] / weights[:,non_nan_mask].sum(1)\n",
    "\n",
    "    # Return result\n",
    "    return(smoothed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185acfdb-4aaa-4747-926e-cbdc0c4a2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308dc3d-f36a-4f9d-80cb-49d3ec6026d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed.shape\n",
    "for k in np.arange(smoothed.shape[1]):\n",
    "    \n",
    "    # Get nan mask\n",
    "    non_nan_mask = ~np.isnan(ydata[:,k])\n",
    "    \n",
    "    # Get smoothed\n",
    "    print(smoothed[:,k] / weights[:,non_nan_mask].sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612eb756-9730-486f-ae85-c9e1265c474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[ 0.23876152  0.30856143 -0.19740716 ... -0.00821319 -0.25510581\n",
    " -0.14734484]\n",
    "marker5:  0.000518798828125\n",
    "[ 0.23761886  0.30677943 -0.19650863 ... -0.00709767 -0.25337046\n",
    " -0.14570244]\n",
    "[ 0.23742288  0.30647581 -0.19635486 ... -0.00690842 -0.25307346\n",
    " -0.14542344]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617df6e-8d48-4c3d-a489-bc48f7d98fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_mask = ~np.isnan(ydata[:,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17333fc-d75b-4002-b03f-e32f6475ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(non_nan_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6fdb8-ed78-4529-921c-bc8fcce0854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape, weights.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755104a-eff8-4093-a668-f1de35fca2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[non_nan_mask,:].shape, weights[non_nan_mask,:].sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035a04c-411b-4676-a72c-f058ffe76152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
