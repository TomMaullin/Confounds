{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import shutil \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "\n",
    "from lib.script_01_00 import generate_initial_variables\n",
    "from lib.script_01_01 import generate_raw_confounds\n",
    "from lib.script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal \n",
    "from src.nets.nets_normalise import nets_normalise \n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.nets.nets_deconfound import nets_deconfound\n",
    "\n",
    "from src.duplicate.duplicate_categorical import duplicate_categorical\n",
    "from src.duplicate.duplicate_demedian_norm_by_site import duplicate_demedian_norm_by_site\n",
    "\n",
    "from src.preproc.datenum import datenum\n",
    "from src.preproc.days_in_year import days_in_year\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "from src.memmap.addBlockToMmap import addBlockToMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82731372-cb80-4dca-aaec-ce4671396009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e28b18-6668-4814-b992-8f08cd182a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in precomputed memmaps\n",
    "IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))\n",
    "confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f330dc8-2936-4d62-aa1c-aa018154544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.nets.nets_svd import nets_svd\n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.nantools.all_non_nan_inds import all_non_nan_inds\n",
    "from src.nantools.create_nan_patterns import create_nan_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b84ab-981b-498b-9822-7708872bcee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nets.nets_deconfound_once import inside_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18da49a-f056-4451-8130-79674d7480f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cfg = {'cluster_type':'local','num_nodes':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee455a3-a8f0-4e19-88da-8aceb02d3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "y = IDPs[:,:]\n",
    "conf = confounds[:,:]\n",
    "mode = 'nets_svd'\n",
    "demean = True\n",
    "dtype = 'float64'\n",
    "conf_has_nans = False\n",
    "check_nan_patterns=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c113a-7210-4c63-84d5-d991d4908b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Local Cluster\n",
    "from dask.distributed import LocalCluster\n",
    "from dask.distributed import Client, as_completed\n",
    "cluster = LocalCluster()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Connect to client\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Connect to cluster\n",
    "client = Client(cluster)   \n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Scale\n",
    "# --------------------------------------------------------------------------------\n",
    "cluster.scale(cluster_cfg['num_nodes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379723af-78bc-49c3-a540-9737059c56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6df128-4ce4-41c4-9c85-963a221d8a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee821ee-259f-4ce2-a00c-1420e9e8004d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ddb2a-6d81-4235-966c-5949047a635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Save original index\n",
    "# original_index = y.index\n",
    "\n",
    "# # Check if confounds have NaNs\n",
    "# if conf_has_nans is None:\n",
    "    \n",
    "#     # If the type is memory mapped\n",
    "#     if type(conf)==MemoryMappedDF:\n",
    "\n",
    "#         # Work out if the confounds have nans\n",
    "#         conf_has_nans = conf[:,:].isna().sum().sum()\n",
    "\n",
    "#     else:\n",
    "#         # Work out if the confounds have nans\n",
    "#         conf_has_nans = conf.isna().sum().sum()\n",
    "\n",
    "# # If the confounds have nans\n",
    "# if conf_has_nans:\n",
    "    \n",
    "#     # If the type is memory mapped\n",
    "#     if type(conf)==MemoryMappedDF:\n",
    "        \n",
    "#         # We are trying to avoid reading everything in at once\n",
    "#         conf_non_nan_inds = all_non_nan_inds(conf, safeMode=True)\n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         # Otherwise, we can get the indices for non-nan rows in conf directly\n",
    "#         conf_non_nan_inds = all_non_nan_inds(conf)\n",
    "\n",
    "#     # Reduce conf and y down, ignoring the nan rows for conf\n",
    "#     conf = conf[conf_non_nan_inds]\n",
    "#     y = y[conf_non_nan_inds]\n",
    "    \n",
    "#     # If we have subset the data we need to demean again\n",
    "#     if demean:\n",
    "        \n",
    "#         # Demean y and conf\n",
    "#         y = nets_demean(y)\n",
    "#         conf = nets_demean(conf)\n",
    "\n",
    "\n",
    "# # If we are checking unique nan patterns record the number of them\n",
    "# if check_nan_patterns:\n",
    "        \n",
    "#     # We now need to get the nan-patterns for y (we don't include\n",
    "#     # columns with 5 or less values).\n",
    "#     nan_patterns_y = create_nan_patterns(y, thresh=5)\n",
    "    \n",
    "#     # Number of columns which meet our nan-thresholding requirements\n",
    "#     n_cols = len([j for i in nan_patterns_y for j in nan_patterns_y[i]['columns']])\n",
    "\n",
    "# # Else, we just set n_cols to the number of columns in y for now and fix at the end\n",
    "# else:\n",
    "\n",
    "#     # Set number of columns\n",
    "#     n_cols = y.shape[1]\n",
    "\n",
    "# # Initialize empty nan dataframe\n",
    "# y_deconf = pd.DataFrame(np.zeros((y.shape[0],n_cols),dtype=dtype),index=y.index)\n",
    "\n",
    "# # Set column headers\n",
    "# if check_nan_patterns:\n",
    "    \n",
    "#     # We're only including column names for the variables that were not removed during nan pattern\n",
    "#     # identification.\n",
    "#     y_deconf.columns = [j for i in nan_patterns_y for j in nan_patterns_y[i]['columns']]\n",
    "\n",
    "# else:\n",
    "    \n",
    "#     # Copy from y\n",
    "#     y_deconf.columns = y.columns\n",
    "    \n",
    "# # Change types to memory mapped dfs\n",
    "# MemoryMappedDF(y).save(os.path.join(os.getcwd(),'temp_mmap','y.npz'))\n",
    "# MemoryMappedDF(conf).save(os.path.join(os.getcwd(),'temp_mmap','conf.npz'))\n",
    "\n",
    "# # Scatter the data across the workers\n",
    "# scattered_y = client.scatter(os.path.join(os.getcwd(),'temp_mmap','y.npz'))\n",
    "# scattered_conf = client.scatter(os.path.join(os.getcwd(),'temp_mmap','conf.npz'))\n",
    "# mode = client.scatter(mode)\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# # Empty futures list\n",
    "# futures = []\n",
    "\n",
    "# # If we are checking unique nan patterns record the number of them\n",
    "# if check_nan_patterns:\n",
    "\n",
    "#     # Number of patterns\n",
    "#     num_patterns = len(nan_patterns_y)\n",
    "\n",
    "# # Otherwise we need to loop through our variables one by one\n",
    "# else:\n",
    "\n",
    "#     # Treating each variable as though it has its own unique pattern\n",
    "#     num_patterns = y.shape[1]\n",
    "    \n",
    "# Loop through all unique nan patterns in y\n",
    "for i in range(num_patterns):\n",
    "\n",
    "    print('Deconfounding: ', i+1, '/', num_patterns)\n",
    "\n",
    "    # If we have a pattern, use it\n",
    "    if check_nan_patterns:\n",
    "        \n",
    "        # Get the pattern and columns\n",
    "        non_nan = ~np.array(nan_patterns_y[i]['pattern'],dtype=bool)\n",
    "        columns = nan_patterns_y[i]['columns']\n",
    "\n",
    "    # Otherwise set to none\n",
    "    else:\n",
    "\n",
    "        # Empty pattern and current column\n",
    "        non_nan = None\n",
    "        columns = [y.columns[i]]\n",
    "    \n",
    "    # Submit a job to the local cluster\n",
    "    future_i = client.submit(inside_loop, scattered_y, scattered_conf, \n",
    "                             columns, mode, non_nan, pure=False)\n",
    "\n",
    "    # Append to list \n",
    "    futures.append(future_i)\n",
    "\n",
    "\n",
    "# Completed jobs\n",
    "completed = as_completed(futures)\n",
    "\n",
    "# Wait for results\n",
    "for i in completed:\n",
    "    i.result()\n",
    "\n",
    "t2 = time.time()\n",
    "print('dask time: ', t2-t1)\n",
    "\n",
    "# Delete the future objects (NOTE: see above comment in setup section).\n",
    "del i, completed, futures, future_i\n",
    "\n",
    "\n",
    "# # Get the list of columns in y that are also in y_deconf\n",
    "# common_columns = [col for col in y.columns if col in y_deconf.columns]\n",
    "\n",
    "# # Reorder y_deconf columns to match the order of common columns in y\n",
    "# y_deconf = y_deconf[common_columns]\n",
    "    \n",
    "# # Initialise output dataframe\n",
    "# deconf_out = pd.DataFrame(index=original_index,columns=y_deconf.columns,dtype=dtype)\n",
    "\n",
    "# # Restore the nan rows\n",
    "# if conf_has_nans:\n",
    "#     deconf_out[conf_non_nan_inds] = np.array(y_deconf.values,dtype=dtype)\n",
    "# else:\n",
    "#     deconf_out[:] = np.array(y_deconf.values,dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06075a5e-8245-4394-878c-d6bc7f52eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae321f-ceb6-464b-acf3-401709d301c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns, mode, non_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad1681-412e-466c-a302-84fb2f2d7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89027c13-0f84-4e7e-9103-131fb695c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(x[:, 0]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425adca-936d-47fe-b481-13a5ab08876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_non_nan_inds(x, safeMode=False):\n",
    "\n",
    "    # If we aren't in safe mode just read everything in.\n",
    "    if not safeMode:\n",
    "        \n",
    "        # If the type is memory mapped\n",
    "        if type(x)==MemoryMappedDF:\n",
    "    \n",
    "            # Get the values\n",
    "            x = x[:,:].values\n",
    "\n",
    "        return(~np.isnan(x).any(axis=1))\n",
    "\n",
    "    # Assume we can't load all data in at once\n",
    "    else:\n",
    "\n",
    "        # Create an empty boolean array\n",
    "        nan_array = np.zeros(x.shape[1], dtype=bool)\n",
    "\n",
    "        # Loop through columns one by one\n",
    "        for col in range(x.shape[1]):\n",
    "            nan_array[col] = np.isnan(x[:, col].values).any()\n",
    "\n",
    "        # Return result\n",
    "        return(nan_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e375a-845e-43a3-8234-c44f1bdfa0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from src.nets.nets_svd import nets_svd\n",
    "\n",
    "tmp = np.random.randn(60000,400)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "u,d,v = np.linalg.svd(tmp, full_matrices=False)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "t1 = time.time()\n",
    "u2,d2,v2 = nets_svd(tmp, reorder=False)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c3cfc-f4f8-4cb8-a564-644c437554a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(np.abs(tmp - (u @ np.diag(d) @ v))), np.amax(np.abs(tmp - (u2 @ np.diag(d2) @ v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05c106-aba1-4220-8370-20c90fcda833",
   "metadata": {},
   "outputs": [],
   "source": [
    "u2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835696b-c6a7-47d2-be9d-64243ae06bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdc83b-eb68-4715-91e2-ca6089e282bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d7c24d-aac8-469d-b9ab-34871f477e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735508c-f204-46de-b2ed-27ffd5c7fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dask.config.paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed1d1f-9415-42ee-a411-2259a3735413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf37ac-310a-426f-a7c1-19e2709aede8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd12d7-dd00-4bd8-9ded-514417d30339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07410473-3627-48aa-9a04-e6bb39cb715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "from dask.distributed import Client, as_completed\n",
    "cluster = LocalCluster()\n",
    "\n",
    "# Connect to cluster\n",
    "client = Client(cluster)   \n",
    "\n",
    "# Read in number of nodes we need\n",
    "num_nodes = 12\n",
    "\n",
    "# Scale the cluster\n",
    "cluster.scale(num_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31309196-41b2-4a00-af24-2b3267e24bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4590f4e-f623-483e-80b2-20de2ad32fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKER DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447197a8-3691-4633-a86c-68dcbd9ef4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = IDPs[:,:]\n",
    "conf = confounds[:,:]\n",
    "mode='nets_svd'\n",
    "demean=True, \n",
    "dtype='float64'\n",
    "conf_has_nans=None\n",
    "\n",
    "t1 = time.time()\n",
    "# Save original index\n",
    "original_index = y.index\n",
    "\n",
    "# Check if confounds have NaNs\n",
    "if conf_has_nans is None:\n",
    "    \n",
    "    # If the type is memory mapped\n",
    "    if type(conf)==MemoryMappedDF:\n",
    "\n",
    "        # Work out if the confounds have nans\n",
    "        conf_has_nans = conf[:,:].isna().sum().sum()\n",
    "\n",
    "    else:\n",
    "        # Work out if the confounds have nans\n",
    "        conf_has_nans = conf.isna().sum().sum()\n",
    "\n",
    "# If the confounds have nans\n",
    "if conf_has_nans:\n",
    "    \n",
    "    # If the type is memory mapped\n",
    "    if type(conf)==MemoryMappedDF:\n",
    "        \n",
    "        # We are trying to avoid reading everything in at once\n",
    "        conf_non_nan_inds = all_non_nan_inds(conf, safeMode=True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Otherwise, we can get the indices for non-nan rows in conf directly\n",
    "        conf_non_nan_inds = all_non_nan_inds(conf)\n",
    "\n",
    "    # Reduce conf and y down, ignoring the nan rows for conf\n",
    "    conf = conf[conf_non_nan_inds]\n",
    "    y = y[conf_non_nan_inds]\n",
    "    \n",
    "    # If we have subset the data we need to demean again\n",
    "    if demean:\n",
    "        \n",
    "        # Demean y and conf\n",
    "        y = nets_demean(y)\n",
    "        conf = nets_demean(conf)\n",
    "    \n",
    "# We now need to get the nan-patterns for y\n",
    "nan_patterns_y = create_nan_patterns(y)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print('init time: ', t2-t1)\n",
    "    \n",
    "# Empty futures list\n",
    "futures = []\n",
    "\n",
    "# Submit jobs\n",
    "for i in nan_patterns_y:\n",
    "\n",
    "    print('Deconfounding: ', i+1, '/', len(nan_patterns_y))\n",
    "\n",
    "    # Get the pattern\n",
    "    non_nan = ~np.array(nan_patterns_y[i]['pattern'],dtype=bool)\n",
    "    \n",
    "    # Check if we have at least 5 non-nan values\n",
    "    if np.sum(1*non_nan) > 5:\n",
    "        \n",
    "        # Run the i^{th} job.\n",
    "        future_i = client.submit(inside_loop, \n",
    "                                 y[nan_patterns_y[i]['columns']], \n",
    "                                 conf, non_nan, mode, pure=False)\n",
    "    \n",
    "        # Append to list \n",
    "        futures.append(future_i)\n",
    "\n",
    "# Completed jobs\n",
    "completed = as_completed(futures)\n",
    "\n",
    "    \n",
    "# # Get the list of columns in y that are also in y_deconf\n",
    "# common_columns = [col for col in y.columns if col in y_deconf.columns]\n",
    "\n",
    "# # Reorder y_deconf columns to match the order of common columns in y\n",
    "# y_deconf = y_deconf[common_columns]\n",
    "    \n",
    "# # Remove columns where all values are NaN\n",
    "# y_deconf = y_deconf.dropna(axis=1, how='all')\n",
    "\n",
    "# # Initialise output dataframe\n",
    "# deconf_out = pd.DataFrame(index=original_index,columns=y_deconf.columns,dtype=dtype)\n",
    "\n",
    "# # Restore the nan rows\n",
    "# if conf_has_nans:\n",
    "#     deconf_out[conf_non_nan_inds] = np.array(y_deconf.values,dtype=dtype)\n",
    "# else:\n",
    "#     deconf_out[:] = np.array(y_deconf.values,dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54969e99-1646-4c72-a0fa-d5075141ee75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7272358-6d57-4aa5-bdc2-ec63a128394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preproc.switch_type import switch_type\n",
    "\n",
    "#nonIDPs.save(os.path.join(os.getcwd(),'tmp_file2'))\n",
    "x=os.path.join(os.getcwd(),'tmp_file2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4c04b-7099-44b6-a1a6-66342a1b403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=switch_type(x,'pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061aa92-0e16-4d82-88e1-961c51a16403",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8347f-fe05-4069-8f39-23ae35493018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "y = read_memmap_df(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85e1d9-5bb0-4e0b-bcbb-4334853caadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179524e-2c44-4c9a-b8fb-77e62aedc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "b'\\x94\\x8c\\x0f_unpickle_block\\x94\\x93\\x94\\x8c\\x12numpy.core.numeric\\x94\\x8c\\x0b_frombuffer\\x94\\x93\\x94(\\x97h\\xbd\\x8c\\x05dtype\\x94\\x93\\x94\\x8c\\x02f8\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94bM\\x8f\\x0fJ\\x8e\\x07\\x01\\x00\\x86\\x94\\x8c\\x01F\\x94t\\x94R\\x94\\x8c\\x08builtins\\x94\\x8c\\x05slice\\x94\\x93\\x94K\\x00M\\x8f\\x0fK\\x01\\x87\\x94R\\x94K\\x02\\x87\\x94R\\x94\\x85\\x94]\\x94(\\x8c\\x18'.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4070d7b-67f9-4b0e-a9bf-4e258a9063ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here1', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951fa21e-bb70-442e-a042-876f8f588b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "future.visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880849e-bcc4-4673-852c-80e0bbd0cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scatter the data across the workers\n",
    "tmp_y = os.path.join(os.getcwd(),'temp_mmap','y.npz')\n",
    "tmp_conf = os.path.join(os.getcwd(),'temp_mmap','conf.npz')\n",
    "mode = 'nets_svd'\n",
    "columns = [y.columns[10]]\n",
    "non_nan = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbd084-da90-45e3-8601-52f9ed4604ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_loop(tmp_y, tmp_conf, columns, mode, non_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cb6f2-5787-4f11-89e8-b55ce82a8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change types to memory mapped dfs\n",
    "y2 = switch_type(tmp_y,out_type=\"MemoryMappedDF\")\n",
    "conf2 = switch_type(tmp_conf,out_type=\"MemoryMappedDF\")\n",
    "\n",
    "# Get the y's we're interested in\n",
    "y_current = y2[:,columns]\n",
    "\n",
    "# If we don't have nans work them out\n",
    "if non_nan is None:\n",
    "    non_nan = ~np.array(y_current.isna().astype(int).values,dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221c530-90fc-41e4-8152-88e395907818",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf2[:,:][non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffdfc4-f61a-4a45-89f6-6e56d0b3a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_current = y_current[non_nan]\n",
    "conf_current = conf2[np.where(non_nan),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42bdbb-5406-47e2-906d-53ca56ecaa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del y2, conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859f303-0b99-431f-896d-1d5af116b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(y_current.isna().astype(int).tolist())\n",
    "from src.preproc.switch_type import switch_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cdd977-9343-4428-8b44-cff445997475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here1', file=f)\n",
    "    \n",
    "# Change types to memory mapped dfs\n",
    "y2 = switch_type(tmp_y,out_type=\"MemoryMappedDF\")\n",
    "conf2 = switch_type(tmp_conf,out_type=\"MemoryMappedDF\")\n",
    "\n",
    "# Get dimensions we are ouputting to\n",
    "out_dim = y2.shape\n",
    "\n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here2', file=f)\n",
    "    \n",
    "# Save original index and columns for outputting later\n",
    "y_index_original = y2.index\n",
    "y_columns_original = y2.columns\n",
    "\n",
    "# Get the y's we're interested in\n",
    "y_current = y2[:,columns]\n",
    "\n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here3', file=f)\n",
    "# MARKER NEED TO CHECK HAVE AT LEAST 5 NON NAN HERE\n",
    "\n",
    "# If we don't have nans work them out\n",
    "if non_nan is None:\n",
    "    non_nan = ~np.array(y_current.isna().astype(int).values,dtype=bool)\n",
    "\n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here4', file=f)\n",
    "    \n",
    "# Subset y and conf to the appropriate rows\n",
    "y_current = y_current[non_nan]\n",
    "conf_current = conf2[:,:][non_nan] # Only time all data is read in\n",
    "\n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here5', file=f)\n",
    "    \n",
    "# Save y index and columns\n",
    "y_index = y_current.index\n",
    "y_columns = y_current.columns\n",
    "\n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here6', file=f)\n",
    "\n",
    "# If we are demeaning\n",
    "if demean:\n",
    "    \n",
    "    # Demean conf_current\n",
    "    conf_current = nets_demean(conf_current)\n",
    "    \n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here7', file=f)\n",
    "    \n",
    "# We don't want to work on views of the data as it will slow the computation\n",
    "conf_current = np.array(conf_current.values)\n",
    "y_current = np.array(y_current.values)\n",
    "\n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here8', file=f)\n",
    "    \n",
    "# Check if we are using psuedo inverse\n",
    "if mode.lower() == 'pinv':\n",
    "\n",
    "    # Regress conf out of y_current - we perform the pseudo inverse on\n",
    "    # conf^T @ conf as we expect the number of columns to be much(!) less\n",
    "    # than the number of rows and thus this ends up being more numerically\n",
    "    # stable than trying to invert, or approximately invert, conf itself.\n",
    "    betahat = np.linalg.pinv(conf_current.T @ conf_current) @ conf_current.T @ y_current\n",
    "\n",
    "    # Set computational zeros to actual zeros\n",
    "    betahat[np.abs(betahat) < 1e-10] = 0\n",
    "\n",
    "    # Get deconfounding variable predicted values to regress out\n",
    "    deconf_pred = pd.DataFrame(conf_current @ betahat)\n",
    "    deconf_pred.index = y_index\n",
    "    deconf_pred.columns = y_columns\n",
    "\n",
    "# Otherwise use svd\n",
    "elif mode.lower() == 'nets_svd':\n",
    "    \n",
    "    # Multiply the left-singular values which contribute to the rank of conf\n",
    "    # by the corresponding singular values to rank reduce conf\n",
    "    U, S, _ = nets_svd(conf_current, reorder=False)\n",
    "    \n",
    "    # Rank reduce U and reduce datatype as only need to multiply\n",
    "    # U = U[:, S < 1e-10]\n",
    "    \n",
    "    # Get deconfounding variable predicted values to regress out\n",
    "    deconf_pred = pd.DataFrame(U @ (U.T @ y_current))\n",
    "    deconf_pred.index = y_index\n",
    "    deconf_pred.columns = y_columns\n",
    "\n",
    "# Otherwise use svd\n",
    "elif mode.lower() == 'svd':\n",
    "    \n",
    "    # Multiply the left-singular values which contribute to the rank of conf\n",
    "    # by the corresponding singular values to rank reduce conf\n",
    "    U, S, _ = np.linalg.svd(conf_current, full_matrices=False)\n",
    "    \n",
    "    # Get the rank of the matrix\n",
    "    rank = np.sum(S > 1e-10)\n",
    "    \n",
    "    # Rank reduce U and reduce datatype as only need to multiply\n",
    "    U = U[:, :rank]\n",
    "    \n",
    "    # Get deconfounding variable predicted values to regress out\n",
    "    deconf_pred = pd.DataFrame(U @ (U.T @ y_current))\n",
    "    deconf_pred.index = y_index\n",
    "    deconf_pred.columns = y_columns\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Perform qr decomposition\n",
    "    Q, R = np.linalg.qr(conf_current)\n",
    "    betahat = np.linalg.pinv(R) @ (Q.T @ y_current)\n",
    "\n",
    "    # Set computational zeros to actual zeros\n",
    "    betahat[np.abs(betahat) < 1e-10] = 0\n",
    "\n",
    "    # Get deconfounding variable predicted values to regress out\n",
    "    deconf_pred = pd.DataFrame(conf_current @ betahat)\n",
    "    deconf_pred.index = y_index\n",
    "    deconf_pred.columns = y_columns\n",
    "    \n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here9', file=f)\n",
    "    \n",
    "# Get deconfounded y\n",
    "y_deconf_current = pd.DataFrame(y_current, index=y_index, columns=y_columns) - deconf_pred\n",
    "\n",
    "    \n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here10', file=f)\n",
    "    \n",
    "# If we are demeaning, demean y\n",
    "if demean:\n",
    "    y_deconf_current = nets_demean(y_deconf_current)\n",
    "\n",
    "    \n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here11', file=f)\n",
    "    \n",
    "# Update deconfounded y \n",
    "y_deconf_current_with_nans = np.ones((len(y_index_original), \n",
    "                                      len(y_deconf_current.columns)))*np.NaN\n",
    "\n",
    "with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "    print('here12', file=f)\n",
    "    \n",
    "# Update with current values\n",
    "y_deconf_current_with_nans[non_nan.flatten(),:] = y_deconf_current.values[:,:]\n",
    "\n",
    "# Make into a dataframe with correct index and rows\n",
    "y_deconf_current_with_nans = pd.DataFrame(y_deconf_current_with_nans,\n",
    "                                         index=y_index_original,\n",
    "                                         columns=y_deconf_current.columns)\n",
    "\n",
    "# Indices for where to add to memmap\n",
    "indices = np.ix_(np.arange(out_dim[0]),\n",
    "                 [list(y_columns_original).index(column) for column in columns])\n",
    "\n",
    "# Output filename\n",
    "out_fname = os.path.join(os.getcwd(),'temp_mmap','y_deconf.dat')\n",
    "addBlockToMmap(out_fname, y_deconf_current_with_nans.values, indices, out_dim, dtype=dtype)\n",
    "\n",
    "# with open('/well/nichols/users/inf852/confounds/tmp.txt','a') as f:\n",
    "#     print('here13', file=f)\n",
    "    \n",
    "# t2 = time.time()\n",
    "\n",
    "# print('iteration time: ', t2-t1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffdbc2-fdc2-4d8a-9a03-e2f9d1469489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp44=np.memmap(out_fname,shape=out_dim,dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdeca27-0ab3-4b6a-81b6-8fc091f1d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Indices for where to add to memmap\n",
    "indices2 = np.ix_([list(y_columns_original).index(column) for column in columns],np.arange(out_dim[0]))\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "addBlockToMmap(out_fname, y_deconf_current_with_nans.values, indices, out_dim, dtype=dtype)\n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "\n",
    "t1 = time.time()\n",
    "addBlockToMmap(out_fname + str(2), y_deconf_current_with_nans.values, indices2, (out_dim[1],out_dim[0]), dtype=dtype)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b7cdc-2c1b-42a5-bfaa-89e69db4f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp44=np.memmap(out_fname,shape=out_dim,dtype=dtype)\n",
    "tmp45=np.memmap(out_fname + str(2),shape=(out_dim[1],out_dim[0]),dtype=dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6368b6a-cae0-41da-b02c-d29848487df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp44.shape,tmp45.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929ae7b-479a-4008-8428-96f2506b11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(tmp44[:,10]==tmp45[10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098eca5-01d7-4b2a-a91f-6c8c2f3d53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tmp45[10,:]==tmp44[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52026e4c-afa8-4c86-83ab-708a9c3f13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(tmp44[:,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8fa78b-bf4b-4d9b-afed-e0940f40b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "199+67271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923c6ab-c77f-48dc-8c12-0b922fdff7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = np.memmap(os.path.join(os.getcwd(),'temp_mmap','y_deconf.dat'),shape=(y2.shape[1],y2.shape[0]),dtype=dtype)\n",
    "tmp1 ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23739531-c57e-4b59-aba6-9f650d8e7c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
