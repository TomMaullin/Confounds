{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb7b85f",
   "metadata": {},
   "source": [
    "## Sandbox notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import shutil \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 20)\n",
    "\n",
    "from lib.script_01_00 import generate_initial_variables\n",
    "from lib.script_01_01 import generate_raw_confounds\n",
    "from lib.script_01_02 import generate_nonlin_confounds\n",
    "\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.nets.nets_inverse_normal import nets_inverse_normal \n",
    "from src.nets.nets_normalise import nets_normalise \n",
    "from src.nets.nets_demean import nets_demean\n",
    "# from src.nets.nets_deconfound import nets_deconfound\n",
    "\n",
    "from src.duplicate.duplicate_categorical import duplicate_categorical\n",
    "from src.duplicate.duplicate_demedian_norm_by_site import duplicate_demedian_norm_by_site\n",
    "\n",
    "from src.preproc.datenum import datenum\n",
    "from src.preproc.switch_type import switch_type\n",
    "from src.preproc.days_in_year import days_in_year\n",
    "from src.preproc.filter_columns_by_site import filter_columns_by_site\n",
    "\n",
    "from src.memmap.MemoryMappedDF import MemoryMappedDF\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "from src.memmap.addBlockToMmap import addBlockToMmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82731372-cb80-4dca-aaec-ce4671396009",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/well/win/projects/ukbiobank/fbp/confounds/data/72k_data/'\n",
    "\n",
    "# Output directory (will eventually be equal to data_dir)\n",
    "out_dir = '/well/nichols/users/inf852/confounds/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e28b18-6668-4814-b992-8f08cd182a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in precomputed memmaps\n",
    "IDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs.npz'))\n",
    "IDPs_deconf = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','IDPs_deconf.npz'))\n",
    "nonIDPs = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonIDPs.npz'))\n",
    "misc = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','misc.npz'))\n",
    "confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','confounds.npz'))\n",
    "#nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds.npz'))\n",
    "p1 = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','p.npz'))\n",
    "nonlinear_confounds = read_memmap_df(os.path.join(os.getcwd(),'saved_memmaps','nonlinear_confounds_reduced.npz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0dc7d-7860-48f7-966e-f57f1ea58d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert input to memory mapped dataframes if it isn't already\n",
    "all_conf = switch_type(confounds, out_type='MemoryMappedDF')\n",
    "IDPs = switch_type(IDPs, out_type='MemoryMappedDF')\n",
    "\n",
    "# Confound groups we are interested in.\n",
    "conf_name = ['AGE', 'AGE_SEX', 'HEAD_SIZE',  'TE', 'STRUCT_MOTION', \n",
    "             'DVARS', 'HEAD_MOTION', 'HEAD_MOTION_ST', 'TABLE_POS', \n",
    "             'EDDY_QC']\n",
    "\n",
    "# Get all the confounds in the group\n",
    "conf_group = all_conf.get_groups(conf_name)\n",
    "\n",
    "# Get the subject ids\n",
    "sub_ids = IDPs.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225343c-41bd-428e-b88e-fc65257f4271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f029f-bee0-4389-a00f-576b4d7bdf82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe7cc9-44c9-4da6-8d42-0db9369b60a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8210fa6-0fcc-4ac7-822f-9a5d949531c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e7271-54e0-4576-9a9b-780ca0ff33cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a636bc-cc56-4309-8c11-4a2d5f3d6353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a102ddb-5a3f-4276-874f-1b9f7ef650f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660830c-28bc-4b12-a87f-3b69e4c34097",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_confounds[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0652a2-2b0e-43de-aabf-a4df8cc9aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import time\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from scipy.stats import f  \n",
    "from scipy.stats import t \n",
    "from scipy.linalg import pinv, lstsq  \n",
    "from src.nets.nets_demean import nets_demean\n",
    "from src.nets.nets_pearson import nets_pearson\n",
    "from src.preproc.switch_type import switch_type\n",
    "from src.memmap.read_memmap_df import read_memmap_df\n",
    "from src.nets.nets_load_match import nets_load_match\n",
    "from src.memmap.addBlockToMmap import addBlockToMmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb5f68-32df-4875-876e-7709a4ba3e31",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54808b11-37c1-49b1-ae53-c6d2c542b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = data_dir \n",
    "IDP_indices = np.arange(410,510)\n",
    "nonlinear_confounds = nonlinear_confounds#[:,:]#conf_ct\n",
    "IDPs_deconf = IDPs_deconf#IDPs                         \n",
    "method=1 \n",
    "dtype=np.float64\n",
    "p_fname=None\n",
    "ve_fname=None\n",
    "return_df=True\n",
    "# --------------------------------------------------------------------------------\n",
    "# Convert to appropriate datatype. If we have a filename for a memory mapped \n",
    "# dataframe we want to read it in as a memory mapped df (after all it is already \n",
    "# saved on disk so no extra memory usage there), otherwise if it is a pandas \n",
    "# dataframe we leave it as it is (after all it is already in memory so we are not\n",
    "# increasing usage).\n",
    "# --------------------------------------------------------------------------------\n",
    "t1_total = time.time()\n",
    "    \n",
    "# Initialise empty arrays for p values\n",
    "p_df2 = pd.DataFrame(np.zeros((len(IDP_indices),num_conf_nonlin))*np.NaN,columns=nonlinear_confounds.columns)\n",
    "\n",
    "# Initialise empty arrays for explained variances\n",
    "ve_df2 = pd.DataFrame(np.zeros((len(IDP_indices),num_conf_nonlin))*np.NaN,columns=nonlinear_confounds.columns)\n",
    "\n",
    "for i, IDP_index in enumerate(IDP_indices):\n",
    "    \n",
    "    # If we only have filename\n",
    "    if type(nonlinear_confounds) == str:\n",
    "        \n",
    "        # Convert input to memory mapped dataframes if it isn't already\n",
    "        nonlinear_confounds = switch_type(nonlinear_confounds, out_type='MemoryMappedDF')\n",
    "        \n",
    "    # If we only have filename\n",
    "    if type(IDPs_deconf) == str:\n",
    "        \n",
    "        # Convert input to memory mapped dataframes if it isn't already\n",
    "        IDPs_deconf = switch_type(IDPs_deconf, out_type='MemoryMappedDF')\n",
    "        \n",
    "    # Get the subject ids\n",
    "    sub_ids = IDPs_deconf.index\n",
    "    \n",
    "    # Read in the IDs for site\n",
    "    site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "    \n",
    "    # Get the unique site ids\n",
    "    unique_site_ids = np.unique(site_ids)\n",
    "    \n",
    "    # Initialize indSite as a list to hold the indices\n",
    "    inds_per_site = []\n",
    "    \n",
    "    # Loop over each value in site ids\n",
    "    for site_id in unique_site_ids:\n",
    "    \n",
    "        # If we have a pandas dataframe\n",
    "        if type(IDPs_deconf) == pd.core.frame.DataFrame:\n",
    "            IDPs_values = IDPs_deconf.iloc[:, IDP_index].values.flatten()\n",
    "        else:\n",
    "            IDPs_values = IDPs_deconf[:, IDP_index].values.flatten()\n",
    "    \n",
    "        # Find the non-nan indices for this site\n",
    "        indices = np.where(~np.isnan(IDPs_values) & (site_ids == site_id).all(axis=1))[0]\n",
    "    \n",
    "        # Append the found indices to the indSite list\n",
    "        inds_per_site.append(indices)\n",
    "    \n",
    "    # Delete the indices\n",
    "    del indices\n",
    "    \n",
    "    # Get the number of nonlinear confounds\n",
    "    num_conf_nonlin = nonlinear_confounds.shape[1]\n",
    "    \n",
    "    # Get the number of IDPs\n",
    "    num_IDPs = IDPs_deconf.shape[1]\n",
    "    \n",
    "    # Get IDP\n",
    "    if type(IDPs_deconf) == pd.core.frame.DataFrame:\n",
    "        IDP = IDPs_deconf.iloc[:, IDP_index].values\n",
    "    else:\n",
    "        IDP = IDPs_deconf[:, IDP_index].values\n",
    "    \n",
    "    # If coincident we can speed things up by considering multiple columns at once\n",
    "    for site_no in (unique_site_ids + 1):\n",
    "    \n",
    "        # Get the columns of nonlinear_confounds for this site\n",
    "        nonlinear_confounds_site = filter_columns_by_site(nonlinear_confounds, site_no)\n",
    "    \n",
    "        # Check if we have enough values to perform the comparison\n",
    "        if len(inds_per_site[site_no-1])!=0:\n",
    "            \n",
    "            # Subset to just this site (remembering zero indexing)\n",
    "            nonlinear_confounds_site = nonlinear_confounds_site.iloc[inds_per_site[site_no-1],:]\n",
    "    \n",
    "            # --------------------------------------------------------\n",
    "            # Get X,Y and predicted Y\n",
    "            # --------------------------------------------------------\n",
    "            # Demean the confound data for the current site and nonlinear confound\n",
    "            X = nets_demean(nonlinear_confounds_site).values\n",
    "        \n",
    "            # Get the IDP\n",
    "            Y = IDP[inds_per_site[site_no-1]]\n",
    "        \n",
    "            # Get predicted Y = Xbeta (note this is being done seperately for each\n",
    "            # column so we are not doing the usual inv(X.T @ X) @ X.T @ Y\n",
    "            pred_Y = ((np.sum(X*Y,axis=0)/np.sum(X*X,axis=0))*X)\n",
    "            \n",
    "            # Compute the residuals\n",
    "            resids = Y - pred_Y\n",
    "            \n",
    "            # First method\n",
    "            if method==1:\n",
    "                \n",
    "                # --------------------------------------------------------\n",
    "                # Variance explained version 1\n",
    "                # --------------------------------------------------------\n",
    "                # Get variance explained by pred_Y\n",
    "                ve = 100*((np.std(pred_Y,axis=0)/np.std(Y,axis=0))**2)\n",
    "            \n",
    "                # --------------------------------------------------------\n",
    "                # P Version 1\n",
    "                # --------------------------------------------------------\n",
    "            \n",
    "                # Compute the sum of squares for the effect\n",
    "                SSeffect = np.linalg.norm(pred_Y - np.mean(pred_Y),axis=0)**2  \n",
    "                \n",
    "                # Compute the sum of squares for the error\n",
    "                SSerror = np.linalg.norm(resids - np.mean(resids),axis=0)**2  \n",
    "                \n",
    "                # Degrees of freedom for the effect should be one as we are only \n",
    "                # regressing the one column, unless a column has no observations\n",
    "                # at all\n",
    "                df = 1*np.any(np.abs(X)>1e-8,axis=0)\n",
    "                \n",
    "                # Compute the degrees of freedom for the error\n",
    "                dferror = len(Y) - df  \n",
    "                \n",
    "                # Compute the F-statistic\n",
    "                F = (SSeffect / df) / (SSerror / dferror)  \n",
    "                \n",
    "                # Compute p[i] using the F-distribution\n",
    "                p = 1 - f.cdf(F, df, dferror)  \n",
    "    \n",
    "            # Second method\n",
    "            if method==2:\n",
    "                \n",
    "                # --------------------------------------------------------\n",
    "                # Variance explained version 2\n",
    "                # --------------------------------------------------------\n",
    "                \n",
    "                # Construct new design matrix\n",
    "                XplusIntercept = np.ones((X.shape[1], X.shape[0], 2))\n",
    "                XplusIntercept[:,:,1] = X.T[:]\n",
    "    \n",
    "                # Perform OLS regression\n",
    "                U, D, Vt = np.linalg.svd(XplusIntercept, full_matrices=False)\n",
    "            \n",
    "                # Get the rank of the matrix\n",
    "                rank = np.sum(D > 1e-10,axis=1)\n",
    "            \n",
    "                # Rank reduce U, D and Vt\n",
    "                for i, rank_current in enumerate(rank):\n",
    "                    U[i,:, rank_current:]=0 \n",
    "                    Vt[i,rank_current:,:]=0\n",
    "                    D[i,rank_current:]=0\n",
    "            \n",
    "                # Get betahat\n",
    "                beta = (Vt.transpose((0,2,1))/D.reshape(*D.shape,1)) @ (U.transpose((0,2,1)) @ Y)\n",
    "            \n",
    "                # Get residuals\n",
    "                resids = Y - XplusIntercept @ beta\n",
    "            \n",
    "                # Get sigma^2 estimator\n",
    "                sigma2 = np.sum(resids**2,axis=1)/Y.shape[0]\n",
    "            \n",
    "                # Contrast for beta2\n",
    "                L = np.array([0,1]).reshape((1,2,1))\n",
    "                \n",
    "                # Contrast variance\n",
    "                invDVtL = Vt/D.reshape(*D.shape,1) @ L \n",
    "                varLtBeta = np.sqrt(sigma2.reshape(*sigma2.shape,1)*(invDVtL.transpose((0,2,1)) @ invDVtL))\n",
    "            \n",
    "                # T statistic for contrast\n",
    "                T = L.transpose((0,2,1)) @ beta / varLtBeta\n",
    "                        \n",
    "                # Second version of variance explained\n",
    "                ve = 100*(1-(np.std(resids,axis=1)**2/np.std(Y,axis=0)**2)).flatten()\n",
    "            \n",
    "                # --------------------------------------------------------\n",
    "                # P-value version 2\n",
    "                # --------------------------------------------------------\n",
    "            \n",
    "                # P value\n",
    "                p = 2*t.sf(np.abs(T.flatten()), dferror)\n",
    "    \n",
    "            # Third method\n",
    "            if method==3:\n",
    "                \n",
    "                # --------------------------------------------------------\n",
    "                # P-value version 3\n",
    "                # --------------------------------------------------------\n",
    "            \n",
    "                # Number of elements\n",
    "                n = X.shape[0]\n",
    "                \n",
    "                # Compute numerator\n",
    "                numerator = np.sum(X*Y,axis=0) - n*np.mean(X,axis=0)*np.mean(Y,axis=0)\n",
    "                \n",
    "                # Compute denominator\n",
    "                denom_X = np.sqrt(np.linalg.norm(X,axis=0)**2 - n*np.mean(X,axis=0)**2)\n",
    "                denom_Y = np.sqrt(np.linalg.norm(Y,axis=0)**2 - n*np.mean(Y,axis=0)**2)\n",
    "                \n",
    "                # Compute coefficient\n",
    "                R = numerator/(denom_X*denom_Y)\n",
    "                 \n",
    "                # Get T statistic\n",
    "                T = R*np.sqrt((n-2)/(1-R**2))\n",
    "                \n",
    "                # Assuming 't' is your t-statistic and 'n' is sample size\n",
    "                p = 2*t.sf(np.abs(T), n-2)\n",
    "            \n",
    "                # --------------------------------------------------------\n",
    "                # Variance explained version 3\n",
    "                # --------------------------------------------------------\n",
    "            \n",
    "                # Compute version 3 of variance explained\n",
    "                ve = 100*R**2\n",
    "    \n",
    "            # Save p values and variance explained\n",
    "            ve_df2.loc[i,nonlinear_confounds_site.columns] = ve\n",
    "            p_df2.loc[i,nonlinear_confounds_site.columns] = p\n",
    "    \n",
    "    # Convert back to numpy\n",
    "    ve = ve_df2.values.flatten()\n",
    "    p = p_df2.values.flatten()\n",
    "            \n",
    "    # Check if we are returning the result\n",
    "    if not return_df:\n",
    "        \n",
    "        # Get the memmap filenames for p values\n",
    "        if p_fname is None:\n",
    "            p_fname = os.path.join(os.getcwd(),'temp_mmap', 'p.npy')\n",
    "            \n",
    "        # Get the memmap filenames for p values\n",
    "        if ve_fname is None:\n",
    "            ve_fname = os.path.join(os.getcwd(),'temp_mmap', 've.npy')\n",
    "        \n",
    "        # Indices for where to add to memmap\n",
    "        indices = np.ix_([IDP_index],np.arange(num_conf_nonlin))\n",
    "        \n",
    "        # Add p values to memory maps\n",
    "        addBlockToMmap(p_fname, p, indices,(num_IDPs, num_conf_nonlin),dtype=np.float64)\n",
    "        \n",
    "        # Add explained variance values to memory maps\n",
    "        addBlockToMmap(ve_fname, ve, indices,(num_IDPs, num_conf_nonlin),dtype=np.float64)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        # Return ve and p\n",
    "        pass\n",
    "    \n",
    "t2_total = time.time()\n",
    "print('Time to beat: ', t2_total-t1_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb284e-97df-421e-94a3-84fce75ca93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc15fc-7227-4ef6-8b52-5182f254b921",
   "metadata": {},
   "source": [
    "## V_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248011e-fbd2-4016-994d-8df8203ba01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_dir = data_dir \n",
    "IDP_indices = np.arange(410,510)\n",
    "nonlinear_confounds = nonlinear_confounds#[:,:]#conf_ct\n",
    "IDPs_deconf = IDPs_deconf#IDPs                         \n",
    "method=1 \n",
    "dtype=np.float64\n",
    "p_fname=None\n",
    "ve_fname=None\n",
    "return_df=True\n",
    "# --------------------------------------------------------------------------------\n",
    "# Convert to appropriate datatype. If we have a filename for a memory mapped \n",
    "# dataframe we want to read it in as a memory mapped df (after all it is already \n",
    "# saved on disk so no extra memory usage there), otherwise if it is a pandas \n",
    "# dataframe we leave it as it is (after all it is already in memory so we are not\n",
    "# increasing usage).\n",
    "# --------------------------------------------------------------------------------\n",
    "t1_total = time.time()\n",
    "\n",
    "# If we only have filename\n",
    "if type(nonlinear_confounds) == str:\n",
    "    \n",
    "    # Convert input to memory mapped dataframes if it isn't already\n",
    "    nonlinear_confounds = switch_type(nonlinear_confounds, out_type='MemoryMappedDF')\n",
    "    \n",
    "# If we only have filename\n",
    "if type(IDPs_deconf) == str:\n",
    "    \n",
    "    # Convert input to memory mapped dataframes if it isn't already\n",
    "    IDPs_deconf = switch_type(IDPs_deconf, out_type='MemoryMappedDF')\n",
    "    \n",
    "# Get the subject ids\n",
    "sub_ids = IDPs_deconf.index\n",
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site = []\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in unique_site_ids:\n",
    "\n",
    "    # Find the indices for this site\n",
    "    indices = np.where((site_ids == site_id).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site.append(indices)\n",
    "\n",
    "# Delete the indices\n",
    "del indices\n",
    "\n",
    "# Get the number of nonlinear confounds\n",
    "num_conf_nonlin = nonlinear_confounds.shape[1]\n",
    "\n",
    "# Get the number of IDPs\n",
    "num_IDPs = IDPs_deconf.shape[1]\n",
    "\n",
    "# Get the number of IDPs in the block\n",
    "num_IDPs_block = len(IDP_indices)\n",
    "\n",
    "# Initialise empty arrays for p values\n",
    "p_df = pd.DataFrame(np.zeros((num_IDPs_block,num_conf_nonlin))*np.NaN,columns=nonlinear_confounds.columns)\n",
    "\n",
    "# Initialise empty arrays for explained variances\n",
    "ve_df = pd.DataFrame(np.zeros((num_IDPs_block,num_conf_nonlin))*np.NaN,columns=nonlinear_confounds.columns)\n",
    "\n",
    "# Get IDP\n",
    "if type(IDPs_deconf) == pd.core.frame.DataFrame:\n",
    "    IDP_block = IDPs_deconf.iloc[:, IDP_indices].values\n",
    "else:\n",
    "    IDP_block = IDPs_deconf[:, IDP_indices].values\n",
    "\n",
    "# If coincident we can speed things up by considering multiple columns at once\n",
    "for site_no in (unique_site_ids + 1):\n",
    "\n",
    "    \n",
    "    # Get the columns of nonlinear_confounds for this site\n",
    "    nonlinear_confounds_site = filter_columns_by_site(nonlinear_confounds, site_no)\n",
    "\n",
    "    # Check if we have enough values to perform the comparison\n",
    "    if len(inds_per_site[site_no-1])!=0:\n",
    "        \n",
    "        # Subset to just this site (remembering zero indexing)\n",
    "        nonlinear_confounds_site = nonlinear_confounds_site.iloc[inds_per_site[site_no-1],:]\n",
    "\n",
    "        # Demean the confound data for the current site and nonlinear confound\n",
    "        X = nets_demean(nonlinear_confounds_site).values\n",
    "\n",
    "        # Number of confounds for site\n",
    "        num_conf_site = nonlinear_confounds_site.shape[1]\n",
    "        \n",
    "        # --------------------------------------------------------\n",
    "        # Get X,Y and predicted Y\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        # Get the IDP\n",
    "        Y = IDP_block[inds_per_site[site_no-1],:]\n",
    "\n",
    "        # Get zerod version\n",
    "        Y_with_zeros = np.array(Y)\n",
    "        Y_with_zeros[np.isnan(Y)]=0\n",
    "\n",
    "        # Compute Y'Y. Here each column is treated sepeately so Y is (n x 1)\n",
    "        # and Y'Y is a single value for each column\n",
    "        YtY = np.einsum('ij,ij->j',Y_with_zeros, Y_with_zeros).reshape(1,num_IDPs_block)\n",
    "\n",
    "        # Compute X'Y for each X and Y individually (i.e. for each column\n",
    "        # of X and each column of Y we do the (1 x n) by (n x 1) matrix\n",
    "        # multiplication to get a single value\n",
    "        XtY = np.zeros((num_conf_site, num_IDPs_block))\n",
    "\n",
    "        # Compute X'X. There is no easy way to do this in a broadcasted way\n",
    "        # as we have to construct a different X for each pattern of NaN values\n",
    "        # in Y.\n",
    "        XtX = np.zeros((num_conf_site, num_IDPs_block))\n",
    "\n",
    "        # Block storing number of observations for each column\n",
    "        n_per_col = np.zeros(IDP_block.shape[1])\n",
    "\n",
    "        # Loop through y columns (the nan removal we cannot broadcast)\n",
    "        for IDP_no in range(IDP_block.shape[1]):\n",
    "\n",
    "            # Get current Y\n",
    "            Y_current = Y[:,IDP_no:(IDP_no+1)]\n",
    "            \n",
    "            # Find the non-nan indices for this site\n",
    "            indices = np.where(~np.isnan(Y_current))[0]\n",
    "\n",
    "            # Subset X and Y\n",
    "            X_current = X[indices,:]\n",
    "            Y_current = Y_current[indices,:]\n",
    "\n",
    "            # Save n\n",
    "            n_per_col[IDP_no] = Y_current.shape[0]\n",
    "\n",
    "            # Demean\n",
    "            X_current = X_current - np.mean(X_current, axis=0)\n",
    "\n",
    "            # Compute XtX current\n",
    "            XtX[:,IDP_no] = np.einsum('ij,ij->j', X_current, X_current)\n",
    "\n",
    "            # Compute XtY current\n",
    "            XtY[:,IDP_no] = np.einsum('ij,ik->j', X_current, Y_current)\n",
    "\n",
    "        # Get betahat\n",
    "        betahat = XtY/XtX\n",
    "\n",
    "        # Get variance explained\n",
    "        ve = 100*(XtY**2)/YtY/XtX/n_per_col\n",
    "\n",
    "        # Get degrees of freedom\n",
    "        df = 1*np.any(np.abs(X)>1e-8,axis=0)\n",
    "\n",
    "        # Get error degrees of freedom\n",
    "        dferror = n_per_col.reshape((1, num_IDPs_block)) - df.reshape((num_conf_site,1))\n",
    "\n",
    "        # F stat\n",
    "        F = ((XtY**2)/YtY/XtX)/(df.reshape((num_conf_site,1))/dferror)\n",
    "    \n",
    "        # Compute p[i] using the F-distribution\n",
    "        p = 1 - f.cdf(F, df.reshape((num_conf_site,1)), dferror)\n",
    "\n",
    "        # Save p values and variance explained\n",
    "        ve_df[[*nonlinear_confounds_site.columns]] = ve.T\n",
    "        p_df[[*nonlinear_confounds_site.columns]] = p.T\n",
    "        \n",
    "# Check if we are returning the result\n",
    "if not return_df:\n",
    "    \n",
    "    # Get the memmap filenames for p values\n",
    "    if p_fname is None:\n",
    "        p_fname = os.path.join(os.getcwd(),'temp_mmap', 'p.npy')\n",
    "        \n",
    "    # Get the memmap filenames for p values\n",
    "    if ve_fname is None:\n",
    "        ve_fname = os.path.join(os.getcwd(),'temp_mmap', 've.npy')\n",
    "    \n",
    "    # Indices for where to add to memmap\n",
    "    indices = np.ix_([IDP_index],np.arange(num_conf_nonlin))\n",
    "    \n",
    "    # Add p values to memory maps\n",
    "    addBlockToMmap(p_fname, p, indices,(num_IDPs, num_conf_nonlin),dtype=np.float64)\n",
    "    \n",
    "    # Add explained variance values to memory maps\n",
    "    addBlockToMmap(ve_fname, ve, indices,(num_IDPs, num_conf_nonlin),dtype=np.float64)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Return ve and p\n",
    "    pass\n",
    "    \n",
    "t2_total = time.time()\n",
    "print('Time: ', t2_total-t1_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfb12e-6a15-4600-9e5a-638ad8f9954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(IDP_index) in (np.int64, np.int32, np.int16, 'int64', 'int32', 'int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af75a8-65bb-4d45-a433-97e189d61dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "(p_df2-p_df).abs().max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a357a-9ca1-4564-87dd-672cb2d99a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_current[:,0]*X_current[:,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797e269-6a6b-4bac-ae27-6acd76357442",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=np.einsum('ij,ij->j',X_current, X_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f595e6e-57dc-4fc9-946b-13057b5d623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XtY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa7956-4975-4176-a85c-9b9a6f56be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "XtX = np.zeros(XtY.shape)\n",
    "XtX[:,IDP_no]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ef704-03a8-413a-a8b2-0b5bc1e8d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 29\n",
    "k = 0\n",
    "\n",
    "result[j,k]-np.sum(X_current[:,j]*Y_current[:,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac2e80-fe49-4774-914f-c8e797d015cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883cb2d-8bad-4378-857f-c85c0517d332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0511e-6137-4b04-b63e-d553420841fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30623d5b-08d8-4270-8d86-bb047703f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d75be-e975-4f0f-aa9b-9484b5951d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get sigma^2 estimator\n",
    "sigma2 = np.sum(resids**2,axis=1)/Y.shape[0]\n",
    "\n",
    "# Contrast for beta2\n",
    "L = np.array([0,1]).reshape((1,2,1))\n",
    "\n",
    "# Contrast variance\n",
    "invDVtL = Vt/D.reshape(*D.shape,1) @ L \n",
    "varLtBeta = np.sqrt(sigma2.reshape(*sigma2.shape,1)*(invDVtL.transpose((0,2,1)) @ invDVtL))\n",
    "\n",
    "# T statistic for contrast\n",
    "T = L.transpose((0,2,1)) @ beta / varLtBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d627d3b-6958-43c0-921c-373cf9fc8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792980e-c44a-47df-91a3-9cea467323bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8f8e8-3364-497e-8b94-a175e3bd990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b6c70-e94b-45e5-8de7-5f73bd7928ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2*t.sf(np.abs(T.flatten()), dferror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b86a7d-b3e2-484e-a81f-32ea9a16a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae966f3-9e2e-4ed2-9d5f-76a5d57020d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Variance explained version 2\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Construct new design matrix\n",
    "XplusIntercept = np.ones((X.shape[1], X.shape[0], 2))\n",
    "XplusIntercept[:,:,1] = X.T[:]\n",
    "\n",
    "\n",
    "# Perform OLS regression\n",
    "U, D, Vt = np.linalg.svd(XplusIntercept, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60fbf3-ef84-417e-ac48-563e0368e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2 = np.sum(resids**2,axis=1)/Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2154a-0bdc-4835-b3ad-c9de87a78371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contrast for beta2\n",
    "L = np.array([0,1]).reshape((1,2,1))\n",
    "\n",
    "# Contrast variance\n",
    "invDVtL = Vt/D.reshape(*D.shape,1) @ L \n",
    "varLtBeta = np.sqrt(sigma2.reshape(*sigma2.shape,1))*(invDVtL.transpose((0,2,1)) @ invDVtL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e11d1-48ef-4c1b-86a7-d47e70d51ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = L.transpose((0,2,1)) @ beta / varLtBeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b06006-4545-4bfc-b7d4-f21b4ee39719",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dadc067-9a6f-49d5-a450-1e5072918d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of elements\n",
    "n = X.shape[0]\n",
    "\n",
    "# Compute numerator\n",
    "numerator = np.sum(X*Y,axis=0) - n*np.mean(X,axis=0)*np.mean(Y,axis=0)\n",
    "\n",
    "# Compute denominator\n",
    "denom_X = np.sqrt(np.linalg.norm(X,axis=0)**2 - n*np.mean(X,axis=0)**2)\n",
    "denom_Y = np.sqrt(np.linalg.norm(Y,axis=0)**2 - n*np.mean(Y,axis=0)**2)\n",
    "\n",
    "# Compute coefficient\n",
    "R = numerator/(denom_X*denom_Y)\n",
    " \n",
    "# Get T statistic\n",
    "T = R*np.sqrt((n-2)/(1-R**2))\n",
    "\n",
    "# Assuming 't' is your t-statistic and 'n' is sample size\n",
    "p = 2*t.sf(np.abs(T), n-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c89c5-d0d2-4d25-8b12-93be055f8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621ff44-59d7-4a37-b243-dec778c56e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.where(~np.isnan(crossed_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9381ca-e7a1-45b8-9ed8-dcca849264ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "IDP_index = 2000\n",
    "t1 = time.time()\n",
    "\n",
    "# Get the subject ids\n",
    "sub_ids = IDPs_deconf.index\n",
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site = []\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in unique_site_ids:\n",
    "\n",
    "    # If we have a pandas dataframe\n",
    "    if type(IDPs_deconf) == pd.core.frame.DataFrame:\n",
    "        IDPs_values = IDPs_deconf.iloc[:, IDP_index].values.flatten()\n",
    "    else:\n",
    "        IDPs_values = IDPs_deconf[:, IDP_index].values.flatten()\n",
    "\n",
    "    # Find the non-nan indices for this site\n",
    "    indices = np.where(~np.isnan(IDPs_values) & (site_ids == site_id).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site.append(indices)\n",
    "\n",
    "# Delete the indices\n",
    "del indices\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d12a1c-e403-466c-ad94-c4d488638fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDPs_deconf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39bcc51-c036-478e-ba1e-c8ca6796a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = IDPs_deconf[:,:].values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2acde-0298-4567-a4b4-c0e17139e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "IDPs_deconf[:, IDP_index]\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4adf1f-2f8b-4991-8986-14387a3ada43",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDPs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277beb9-03be-4746-a12b-c8948d75f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "# Get the subject ids\n",
    "sub_ids = nonlinear_confounds.index\n",
    "\n",
    "# Read in the IDs for site\n",
    "site_ids = nets_load_match(os.path.join(data_dir, 'ID_SITE.txt'), sub_ids)\n",
    "\n",
    "# Get the unique site ids\n",
    "unique_site_ids = np.unique(site_ids)\n",
    "\n",
    "# Initialize indSite as a list to hold the indices\n",
    "inds_per_site = {}\n",
    "\n",
    "# Loop over each value in site ids\n",
    "for site_id in (unique_site_ids + 1):\n",
    "\n",
    "    # Find the indices where all elements in a row of siteDATA match the current valueSite\n",
    "    # Note: This assumes siteDATA and siteValues have compatible shapes or values for comparison\n",
    "    indices = np.where((site_ids == site_id-1).all(axis=1))[0]\n",
    "\n",
    "    # Append the found indices to the indSite list\n",
    "    inds_per_site[site_id] = indices\n",
    "\n",
    "# Delete the indices\n",
    "del indices\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914bb24-c505-4086-b803-8e0f98826ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rough estimate of maximum memory (bytes)\n",
    "MAXMEM = 2**32\n",
    "\n",
    "# Get the number of subjects\n",
    "n_sub = len(sub_ids)\n",
    "\n",
    "# Block size computation\n",
    "blksize = int(MAXMEM/n_sub/8/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315077b4-1036-4357-beab-6a723ecff87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise empty dict to store headers\n",
    "columns_for_sites = {}\n",
    "\n",
    "# Number of crossed terms we will consider\n",
    "n_ct = 0\n",
    "n_ct_per_site = {}\n",
    "\n",
    "# Number of confound in each site\n",
    "n_conf_per_site = {}\n",
    "\n",
    "# Create a dict of site-specific column headers\n",
    "for site_index in (unique_site_ids + 1):\n",
    "\n",
    "    # Get the columns for this site\n",
    "    columns_for_sites[site_index] = filter_columns_by_site(confounds, \n",
    "                                                           site_index, return_df=False)\n",
    "\n",
    "    # Add nonlinear columns\n",
    "    columns_for_sites[site_index] = columns_for_sites[site_index] + \\\n",
    "                                    filter_columns_by_site(nonlinear_confounds, \n",
    "                                                           site_index, return_df=False)\n",
    "    \n",
    "    # Add the number of confounds for this site\n",
    "    n_conf_per_site[site_index] = int(len(columns_for_sites[site_index]))\n",
    "    \n",
    "    # Add the number of crossed terms for this site\n",
    "    n_ct_per_site[site_index] = int((len(columns_for_sites[site_index])-1)*(len(columns_for_sites[site_index]))/2)\n",
    "    n_ct = n_ct + n_ct_per_site[site_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153d5ad-9b68-4d9d-8cad-dd47e845873d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ae285-f90c-4c3d-b555-59163f474210",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ct_per_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0827808-7096-4e29-aad7-731a5b6bf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_inds = np.array(np.zeros((n_ct,3)),dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3fce3-3004-4ba6-85d9-029f895f2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This array gives the indices for the site-specific confounds in the crossed \n",
    "# term confound matrix. e.g. crossed_terms[:,site_idx[i]:site_idx[i+1]] are \n",
    "# crossed terms for site i.\n",
    "site_idx = np.cumsum([n_ct_per_site[site] for site in n_ct_per_site])\n",
    "site_idx = np.insert(site_idx,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1dc94-6060-417b-bb26-d04e76a348ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now construct the crossed_inds matrix. This is interpreted as follows: row\n",
    "# k represents the k-th confound - it is constructed from the product of the\n",
    "# crossed_inds[k,1]^th and crossed_inds[k,2]^th terms from site number\n",
    "# crossed_inds[k,0].\n",
    "for i in range(len(site_idx)-1):\n",
    "\n",
    "    # Set the site indices\n",
    "    crossed_inds[site_idx[i]:site_idx[i+1],0]=i\n",
    "\n",
    "    # Set the indices for the first crossed factor\n",
    "    crossed_inds[site_idx[i]:site_idx[i+1],1] = np.concatenate([np.repeat(i+1, i+1) for i in range(n_conf_per_site[i+1]-1)])\n",
    "    \n",
    "    # Set the indices for the second crossed factor\n",
    "    crossed_inds[site_idx[i]:site_idx[i+1],2] = np.concatenate([np.arange(i+1) for i in range(n_conf_per_site[i+1]-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af86a9-a6bd-40b1-a6d0-d5d2d9c69c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_inds[-10:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d65bcd-232a-4e28-adc8-6f5e950c865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the two\n",
    "confounds_full = pd.concat([confounds[:,:],nonlinear_confounds[:,:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b883b8-172c-480e-89ea-a4c02e0ae2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_full=MemoryMappedDF(confounds_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c233a-843f-4540-8991-b6e39951b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_confounds.list_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90a4da-fe54-4837-b575-dd8b4497977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=confounds.__dict__['groups']\n",
    "tmp2=nonlinear_confounds.__dict__['groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eecbd-a715-4fbe-b02b-2e8cf4ac6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add groupings\n",
    "groups = {**confounds.__dict__['groups'],**nonlinear_confounds.__dict__['groups']}\n",
    "\n",
    "# Loop through groups\n",
    "for group_name in groups:\n",
    "\n",
    "    # Read in the current variable group\n",
    "    current_group = groups[group_name]\n",
    "\n",
    "    # Initialise empty list for this group\n",
    "    updated_group = []\n",
    "    \n",
    "    # Loop through the variables\n",
    "    for variable in current_group:\n",
    "\n",
    "        # Check if its in the reduced confounds\n",
    "        if (variable in nonlinear_confounds.columns) or (variable in confounds.columns):\n",
    "\n",
    "            # Add to updated_group\n",
    "            updated_group = updated_group + [variable]\n",
    "\n",
    "    # If the new groups not empty save it as a group in the new memory mapped df\n",
    "    if len(updated_group) > 0:\n",
    "\n",
    "        # Add the updated group\n",
    "        confounds_full.set_group(group_name, updated_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fc0dc-f41a-4afc-aab8-292ec8a09d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_full.list_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c681c1c-a90d-4644-976d-f6879aeb2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_full.set_group('nonlin', list(nonlinear_confounds.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a055784-85c1-4d01-aabf-123a6fc4f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_full.get_groups(['nonlin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20071446-7454-4afa-a403-3b1e77eafd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(~np.isnan(crossed_inds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b2f12-5dd3-419a-ae5f-1c7b1b19ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "addBlockToMmap(os.path.join(os.getcwd(),'temp_mmap', 'crossed_inds.dat'), \n",
    "                   crossed_inds, np.where(~np.isnan(crossed_inds)),\n",
    "                   crossed_inds.shape, dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8c7ee-50ef-4f8c-9393-8a26fb46ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp4=np.memmap(os.path.join(os.getcwd(),'temp_mmap', 'crossed_inds.dat'), shape=(n_ct,3),dtype='int16') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef2bd4-0701-4c80-a86f-aff62b98640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f23c6-bda5-4011-9a26-9a7c5c941518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Rough estimate of maximum memory (bytes)\n",
    "    MAXMEM = 2**32\n",
    "\n",
    "    # Get the number of subjects\n",
    "    n_sub = len(sub_ids)\n",
    "\n",
    "    # Block size computation\n",
    "    blksize = int(MAXMEM/n_sub/8/64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a59a1-8e92-4faf-873f-449a3589fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(n_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb1cd5-7c8d-432d-9aba-dea2feb9ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # Get the number of blocks we are breaking computation into\n",
    "    num_blks = int(np.ceil(n_ct/blksize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965757dd-5b45-459c-a6bf-26b55dfb6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [idx[i*blksize:min((i+1)*blksize,n_ct)] for i in range(num_blks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317fc74-b6e1-4316-9ea4-9eea633bdaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1a9f6-6d79-4e83-9ece-b8e3d9011c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_inds = crossed_inds[block,:]\n",
    "n_ct_block = crossed_inds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39d6b1-fc84-40cb-931f-70610d216caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ct_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc62a83-3686-4361-8a93-acccaec2d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Set nonlinear confound group\n",
    "    confounds_full.set_group('nonlin', list(nonlinear_confounds.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e30fb-6fba-48bf-9437-a832b3de7e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_for_site=filter_columns_by_site(confounds_full, 4, return_df=False)\n",
    "# filter_columns_by_site(confounds, site_index, return_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b5252-77ac-40d6-a411-41e7abdf8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds[:,confounds_for_site[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8e46e-3dcd-43a9-a83d-15dba8be7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filter_columns_by_site(nonlinear_confounds, 1, return_df=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35be1a2-eab6-40e2-9d31-95b5a668a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(block[0]/blksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc44b51-fe48-4c0a-a39e-6127beff020c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
